---
title: "Take Home Exercise 2"
author: "LIANG YAO"
date: '7 Dec 2023'
date-modified: "`r Sys.Date()`"
execute: 
  warning: false
  echo: true
  eval: true
format: 
  html: 
    code-fold: true
    code-summary: "Show the code"
---

# **Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows**

## **1 Setting the Scene**

What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.

To provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!

As city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.

Unfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner's ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data.

## **2 Objectives**

This take-home exercise is motivated by two main reasons. Firstly, despite increasing amounts of open data available for public consumption, there has not been significant practice research carried out to show how these disparate data sources can be integrated, analysed, and modelled to support policy making decisions.

Secondly, there is a general lack of practical research to show how geospatial data science and analysis (GDSA) can be used to support decision-making.

Hence, your task for this take-home exercise is to conduct a case study to demonstrate the potential value of GDSA to integrate publicly available data from multiple sources for building a spatial interaction models to determine factors affecting urban mobility patterns of public bus transit.

## **3 The Data**

### **Open Government Data**

For the purpose of this assignment, data from several open government sources will be used:

-   *Passenger Volume by Origin Destination Bus Stops*, *Bus Stop Location*, *Train Station* and *Train Station Exit Point*, just to name a few of them, from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html).

-   *Master Plan 2019 Subzone Boundary*, *HDB Property Information*, *School Directory and Information* and other relevant data from [Data.gov.sg](https://beta.data.gov.sg/).

### **Specially collected data**

-   *Business*, *entertn*, *F&B*, *FinServ*, *Leisure&Recreation* and *Retails* are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.
-   HDB: This data set is the geocoded version of *HDB Property Information* data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest *HDB Property Information* provided on data.gov.sg, this [link](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/?panelset6=glimpse%28%29#geocoding-our-aspatial-data) provides a useful step-by-step guide.

::: callout-important
Those specially collected data aim to use within this excercise content only, if intend to put in other usage, approach course instructor [Dr.Â Kam Tin Seong](https://www.smu.edu.sg/faculty/profile/9618/KAM-Tin-Seong) and ask for permission first.
:::

For starting, load needing packages.

```{r}
pacman::p_load(sf, spdep, tmap, tidyverse, sfdep, stplanr, reshape2, ggpubr, DT)
```

## **4 The Task**

### **4.1 Geospatial Data Science**

#### 4.1.1 Generate Traffic analysis zone

Derive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).

First of all, we need to import *Bus Stop Location* from LTA DataMall.

```{r}
busstop = st_read(dsn = "./data/geospatial/BusStopLocation_Jul2023",
                   layer = "BusStop")  %>% st_transform(crs = 3414) %>% 
  distinct(BUS_STOP_N, .keep_all = TRUE)
```

Also import subzone geometry data as our background layer.

```{r}
sz = st_read(dsn = "./data/geospatial",
                   layer = "MP14_SUBZONE_WEB_PL")  %>% st_transform(crs = 3414) 
```

Then we can Derive an analytical hexagon data of 325m.

```{r}
hexagon <- st_sf(geometry = st_make_grid(busstop, cellsize = c(375,375), what = "polygons",square = FALSE)) %>%
  mutate(id = row_number()) %>% 
  st_transform(crs = 3414) 
```

Then we can join bus stop with hexagon, and join with subzone to exclude hexagons out of range.

```{r}
bus_hex <- st_join(
  st_join(hexagon, busstop%>%select(BUS_STOP_N,geometry), join = st_intersects),
  sz) %>%
  drop_na() %>%
  distinct(BUS_STOP_N, .keep_all = TRUE)
```

Check for duplicate geometry.

```{r}
#| eval: false
bus_hex %>%
  group_by(geometry)%>%
  filter(row_number()>1)
```

#### 4.1.2 Construct O-D Matrix of Commuter Flows.

With reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating *Passenger Volume by Origin Destination Bus Stops* and *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html). The O-D matrix must be aggregated at the analytics hexagon level

| Peak hour period             | Bus tap on time |
|------------------------------|-----------------|
| Weekday morning peak         | 6am to 9am      |
| Weekday afternoon peak       | 5pm to 8pm      |
| Weekend/holiday morning peak | 11am to 2pm     |
| Weekend/holiday evening peak | 4pm to 7pm      |

Import bus passenger trips data.

```{r}
odbus = read_csv("./data/aspatial/origin_destination_bus_202310.csv")  %>%
  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
         DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))
```

Extract passenger trips data during all peak time intervals.

```{r}
peak_trips <- bind_rows(
  odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
    mutate(interval = "weekdayam"),
  odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 &
           TIME_PER_HOUR <= 20) %>%
    mutate(interval = "weekdaypm"),
  odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 11 &
           TIME_PER_HOUR <= 14) %>%
    mutate(interval = "weekendam"),
  odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
    mutate(interval = "weekendpm")) %>%
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE, interval) %>%
  reframe(TRIPS = sum(TOTAL_TRIPS)) 

glimpse(peak_trips)
```

Check any bus stops not in our origin 'bus_hex' list.

```{r}
#| eval: false
peak_trips %>%
  filter(! ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %>%
  group_by(ORIGIN_PT_CODE) %>%
  reframe(TRIPS = sum(TRIPS))
```

Exclude any bus stops not included in 'bus_hex' data before continue.

```{r}
peak_trips <- peak_trips %>%
  filter(ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %>%
  filter(DESTINATION_PT_CODE %in% bus_hex$'BUS_STOP_N')
```

Duplication check before continue.

```{r}
#| eval: false
peak_trips %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

After that, we need to combine those passenger trip data with geospatial data by origin bus stops.

```{r}
peaktrip_hex <- left_join(peak_trips %>% 
                            group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
                            reframe(TRIPS = sum(TRIPS)),
                          bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("ORIGIN_PT_CODE" = "BUS_STOP_N"))  %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         DESTIN_BS = DESTINATION_PT_CODE) 
```

Duplication check before continue:

```{r}
#| eval: false
peaktrip_hex %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

Then we can continue to join again with geospatial data by destination bus stops.

```{r}
peaktrip_hex <- left_join(peaktrip_hex, bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("DESTIN_BS" = "BUS_STOP_N"),
                          suffix = c(".origin", ".destin")) 
```

Duplication check again.

```{r}
peaktrip_hex %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

We can save the output into a rds file.

```{r}
#| eval: false
write_rds(peaktrip_hex, "./data/rds/peaktrip_hex.rds")
```

#### 4.1.3 Visualize O-D Flows

Display the O-D flows of the passenger trips by using appropriate geovisualisation methods.

First let's ensure there aren't any observations with same origin and destination.

```{r}
#| eval: false
peaktrip_hex %>%
  filter(ORIGIN_BS==DESTIN_BS)
```

Then we can create flow lines and check summary of data in case there are any zero.

```{r}
peaktrip_flow <- od2line(flow = peaktrip_hex, 
                    zones = bus_hex,
                    zone_code = "BUS_STOP_N")

summary(peaktrip_flow)
```

Till now, we can plot out the bus trip flow during all 4 peak time intervals in total.

```{r}
tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peaktrip_flow %>% 
  filter(TRIPS >= 2000) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5)
```

And we can visualize 4 peak time intervals in facets style to check any difference within.

First need to wrangling the data to put trips data of different time intervals into different columns.

```{r}
peak_interval_trips <- peak_trips %>%
  pivot_wider(names_from = interval, 
              values_from = TRIPS, 
              values_fill = 0)

peak_interval_hex <- left_join(peak_interval_trips,
                          bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("ORIGIN_PT_CODE" = "BUS_STOP_N"))  %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         DESTIN_BS = DESTINATION_PT_CODE) 


peak_interval_hex <- left_join(peak_interval_hex, bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("DESTIN_BS" = "BUS_STOP_N"),
                          suffix = c(".origin", ".destin")) 
```

Before we continue, we can check duplication and save the result as a rds file.

```{r}
#| eval: false
peak_interval_hex %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
#| eval: false
write_rds(peak_interval_hex, "./data/rds/peak_interval_hex.rds")
```

Then we can create flow lines.

```{r}
peak_interval_flow <- od2line(flow = peak_interval_hex, 
                    zones = bus_hex,
                    zone_code = "BUS_STOP_N")

summary(peak_interval_flow)
```

Then we can plot out 4 peak intervals in facets.

```{r}
#| fig-width: 10
#| fig-height: 10
weekdayam_plot <- tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peak_interval_flow %>% 
  filter(weekdayam >= 5000) %>%
tm_shape() +
  tm_lines(lwd = "weekdayam",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5) +
  tm_layout(main.title = "Trips during Weekday 6am till 9am",
            main.title.size = 1.2)

weekdaypm_plot <- tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peak_interval_flow %>% 
  filter(weekdaypm >= 5000) %>%
tm_shape() +
  tm_lines(lwd = "weekdaypm",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5) +
  tm_layout(main.title = "Trips during Weekday 5pm till 8pm",
            main.title.size = 1.2)

weekendam_plot <- tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peak_interval_flow %>% 
  filter(weekendam >= 2000) %>%
tm_shape() +
  tm_lines(lwd = "weekendam",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5) +
  tm_layout(main.title = "Trips during Weekend/Holiday 11am till 2pm",
            main.title.size = 1.2)

weekendpm_plot <- tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peak_interval_flow %>% 
  filter(weekendpm >= 2000) %>%
tm_shape() +
  tm_lines(lwd = "weekendpm",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           alpha = 0.5) +
  tm_layout(main.title = "Trips during Weekend/Holiday 4pm till 7pm",
            main.title.size = 1.2)

tmap_arrange(weekdayam_plot, weekdaypm_plot, weekendam_plot, weekendpm_plot, asp=1, ncol=2,
             outer.margins = 0)
```

::: callout-note
**Observations:**

Describe the spatial patterns revealed by the geo-visualization (not more than 100 words per visual)

**try to add a facet graph for each time interval.**
:::

#### 4.1.4 Propulsive and Attractiveness variables

Firstly import all those propulsive and Attractiveness variables.

```{r}
business = st_read(dsn = "./data/geospatial",
                   layer = "Business")  %>% st_transform(crs = 3414) 

entertn = st_read(dsn = "./data/geospatial",
                   layer = "entertn")  %>% st_transform(crs = 3414) 

food = st_read(dsn = "./data/geospatial",
                   layer = "F&B")  %>% st_transform(crs = 3414) 

finance = st_read(dsn = "./data/geospatial",
                   layer = "FinServ")  %>% st_transform(crs = 3414) 

leisure = st_read(dsn = "./data/geospatial",
                   layer = "Leisure&Recreation")  %>% st_transform(crs = 3414) 

retail = st_read(dsn = "./data/geospatial",
                   layer = "Retails")  %>% st_transform(crs = 3414) 
```

Then we can assemble all those variables by using st_join.

```{r}
# use intersect of goemetry, count POI_NAME
factors <- st_join(retail %>% select(POI_NAME, geometry), 
                   business %>% select(POI_NAME, geometry), 
                   join = st_nearest_feature,
                   suffix = c("_retail", "_business")) %>% 
  st_join(finance %>% select(POI_NAME, geometry) %>% rename(finance = POI_NAME),
          join = st_nearest_feature) %>% 
  st_join(food %>% select(POI_NAME, geometry) %>% rename(food = POI_NAME),
          join = st_nearest_feature) %>% 
  st_join(leisure %>% select(POI_NAME, geometry) %>% rename(leisure = POI_NAME),
          join = st_nearest_feature) %>% 
  st_join(entertn %>% select(POI_NAME, geometry) %>% rename(entertn = POI_NAME),
          join = st_nearest_feature) %>% 
  group_by(geometry) %>%
  reframe(retail_poi = row_number(POI_NAME_retail),
          business_poi = row_number(POI_NAME_business),
          finance_poi = row_number(finance),
          food_poi = row_number(food),
          leisure_poi = row_number(leisure),
          entertn_poi = row_number(entertn)) 
```

Check summary of assembled factors before continue.

```{r}
summary(factors)
```

::: callout-important
-   Since left = TRUE is default in st_join, so here I will join those variables in the sequence of number of observations, which is, retail \<- business \<- finance \<- food \<- leisure \<- entertain.

-   And to avoid NA count in some variable, I will set the "join = st_nearest_feature" instead of default "st_intersect".
:::

#### 4.1.5 Distance Matrix

I will focus on weekend/holiday evening peak time interval for further analysis.

```{r}
weekendpm_trips <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
  reframe(TRIPS = sum(TOTAL_TRIPS))
 
weekendpm_trips <- weekendpm_trips %>%
  filter(ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %>%
  filter(DESTINATION_PT_CODE %in% bus_hex$'BUS_STOP_N')

weekendpm_hex <- left_join(weekendpm_trips,bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("ORIGIN_PT_CODE" = "BUS_STOP_N"))  %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         DESTIN_BS = DESTINATION_PT_CODE) 

weekendpm_hex <- left_join(weekendpm_hex, bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("DESTIN_BS" = "BUS_STOP_N"),
                          suffix = c(".origin", ".destin")) 
```

Create flow lines for those trips data.

```{r}
weekendpm_flow <- od2line(flow = weekendpm_hex, 
                    zones = bus_hex,
                    zone_code = "BUS_STOP_N")

summary(weekendpm_flow)
```

Then we need to convert it to **Spatial Polygons Data Frame.**

```{r}
#weekendpm_sp <- as(weekendpm_hex, "Spatial")
#weekendpm_sp
```

Then we can compute distance matrix.

```{r}

```

### **4.2 Spatial Interaction Modelling**

#### 4.2.1 Spatial Interactive model

Calibrate spatial interactive models to determine factors affecting urban commuting flows at the selected time interval.

#### 4.2.2 Visualizing Modelling results

Present the modelling results by using appropriate geovisualization and graphical visualization methods.

::: callout-important
**Observations:**

-   With reference to the Spatial Interaction Model output tables, maps and data visualization prepared, describe the modelling results
:::
