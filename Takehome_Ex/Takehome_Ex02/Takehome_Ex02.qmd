---
title: "Take Home Exercise 2"
author: "LIANG YAO"
date: '7 Dec 2023'
date-modified: "`r Sys.Date()`"
execute: 
  warning: false
  echo: true
  eval: true
format: 
  html: 
    code-fold: true
    code-summary: "Show the code"
---

# **Applied Spatial Interaction Models: A case study of Singapore public bus commuter flows**

## **1 Setting the Scene**

What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.

To provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!

As city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.

Unfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner's ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data.

## **2 Objectives**

This take-home exercise is motivated by two main reasons. Firstly, despite increasing amounts of open data available for public consumption, there has not been significant practice research carried out to show how these disparate data sources can be integrated, analysed, and modelled to support policy making decisions.

Secondly, there is a general lack of practical research to show how geospatial data science and analysis (GDSA) can be used to support decision-making.

Hence, your task for this take-home exercise is to conduct a case study to demonstrate the potential value of GDSA to integrate publicly available data from multiple sources for building a spatial interaction models to determine factors affecting urban mobility patterns of public bus transit.

## **3 The Data**

### **Open Government Data**

For the purpose of this assignment, data from several open government sources will be used:

-   *Passenger Volume by Origin Destination Bus Stops*, *Bus Stop Location*, *Train Station* and *Train Station Exit Point*, just to name a few of them, from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html).

-   *Master Plan 2019 Subzone Boundary*, *HDB Property Information*, *School Directory and Information* and other relevant data from [Data.gov.sg](https://beta.data.gov.sg/).

### **Specially collected data**

-   *Business*, *entertn*, *F&B*, *FinServ*, *Leisure&Recreation* and *Retails* are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.
-   HDB: This data set is the geocoded version of *HDB Property Information* data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest *HDB Property Information* provided on data.gov.sg, this [link](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/?panelset6=glimpse%28%29#geocoding-our-aspatial-data) provides a useful step-by-step guide.

::: callout-important
Those specially collected data aim to use within this excercise content only, if intend to put in other usage, approach course instructor [Dr.Â Kam Tin Seong](https://www.smu.edu.sg/faculty/profile/9618/KAM-Tin-Seong) and ask for permission first.
:::

For starting, load needing packages.

```{r}
pacman::p_load(sf, sp, spdep, tmap, tidyverse, sfdep, stplanr, performance, corrplot)
```

## **4 The Task**

### **4.1 Geospatial Data Science**

#### 4.1.1 Generate Traffic analysis zone

Derive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the [traffic analysis zone (TAZ)](https://tmg.utoronto.ca/files/Reports/Traffic-Zone-Guidance_March-2021_Final.pdf).

First of all, we need to import *Bus Stop Location* from LTA DataMall.

```{r}
#| code-fold: show
busstop = st_read(dsn = "./data/geospatial/BusStopLocation_Jul2023",
                   layer = "BusStop")  %>% st_transform(crs = 3414) %>% 
  distinct(BUS_STOP_N, .keep_all = TRUE)
```

Also import subzone geometry data as our background layer.

```{r}
#| code-fold: show
sz = st_read(dsn = "./data/geospatial",
                   layer = "MPSZ-2019")  %>% st_transform(crs = 3414) 
```

Then we can Derive an analytical hexagon data of 375m.

```{r}
#| code-fold: show
hexagon <- st_sf(geometry = st_make_grid(busstop, cellsize = 750, what = "polygons",square = FALSE)) %>%
  mutate(id = row_number()) %>%
  st_transform(crs = 3414) 

hexagon <- hexagon %>%
  mutate(N = lengths(st_intersects(hexagon, busstop))) %>% 
  filter(N>0)
```

Plot out our bus_hex to check.

```{r}
tm_shape(sz) +
  tm_polygons(alpha = 0.3) +
  tm_borders(alpha = 0.2) +
tm_shape(hexagon) +
  tm_fill("N", 
          style = "quantile", 
          palette = "Blues",
          title = "Bus stop counts",
          colorNA = NULL,
          showNA = FALSE) +
  tm_layout(main.title = "Count of Bus Stops at Hexagon Level",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="4star", size = 1.5) +
  tm_borders(alpha = 0.5) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

Then we can join bus stop with hexagon, and join with subzone to exclude hexagons out of range.

```{r}
#| code-fold: show
bus_hex <- st_join(
  st_join(hexagon, busstop%>%select(BUS_STOP_N,geometry), join = st_intersects),
  sz) %>%
  drop_na() %>%
  distinct(BUS_STOP_N, .keep_all = TRUE)

bus_hex
```

Notice there would be duplicate geometry, meaning multiple 'BUS_STOP_N's at same hexagon.

```{r}
#| code-fold: show
bus_hex %>%
  group_by(geometry)%>%
  filter(row_number()>1)
```

#### 4.1.2 Construct O-D Matrix of Commuter Flows.

With reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating *Passenger Volume by Origin Destination Bus Stops* and *Bus Stop Location* from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html). The O-D matrix must be aggregated at the analytics hexagon level

| Peak hour period             | Bus tap on time |
|------------------------------|-----------------|
| Weekday morning peak         | 6am to 9am      |
| Weekday afternoon peak       | 5pm to 8pm      |
| Weekend/holiday morning peak | 11am to 2pm     |
| Weekend/holiday evening peak | 4pm to 7pm      |

Import bus passenger trips data.

```{r}
#| code-fold: show
odbus = read_csv("./data/aspatial/origin_destination_bus_202310.csv")  %>%
  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
         DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))
```

Extract passenger trips data during all peak time intervals.

```{r}
peak_trips <- bind_rows(
  odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
    mutate(interval = "weekdayam"),
  odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 &
           TIME_PER_HOUR <= 20) %>%
    mutate(interval = "weekdaypm"),
  odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 11 &
           TIME_PER_HOUR <= 14) %>%
    mutate(interval = "weekendam"),
  odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
    mutate(interval = "weekendpm")) %>%
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE, interval) %>%
  reframe(TRIPS = sum(TOTAL_TRIPS)) 

glimpse(peak_trips)
```

Check any bus stops not in our origin 'bus_hex' list.

```{r}
#| code-fold: show
#| eval: false
peak_trips %>%
  filter(! ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %>%
  group_by(ORIGIN_PT_CODE) %>%
  reframe(TRIPS = sum(TRIPS))
```

Exclude any bus stops not included in 'bus_hex' data before continue.

```{r}
#| code-fold: show
peak_trips <- peak_trips %>%
  filter(ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %>%
  filter(DESTINATION_PT_CODE %in% bus_hex$'BUS_STOP_N')
```

Duplication check before continue.

```{r}
#| code-fold: show
#| eval: false
peak_trips %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

After that, we need to combine those passenger trip data with geospatial data by origin bus stops.

```{r}
#| code-fold: show
peaktrip_hex <- left_join(peak_trips %>% 
                            group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
                            reframe(TRIPS = sum(TRIPS)),
                          bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("ORIGIN_PT_CODE" = "BUS_STOP_N"))  %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         DESTIN_BS = DESTINATION_PT_CODE) 
```

Duplication check before continue:

```{r}
#| code-fold: show
#| eval: false
peaktrip_hex %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

Then we can continue to join again with geospatial data by destination bus stops.

```{r}
peaktrip_hex <- left_join(peaktrip_hex, bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("DESTIN_BS" = "BUS_STOP_N"),
                          suffix = c(".origin", ".destin")) 

glimpse(peaktrip_hex)
```

Duplication check again.

```{r}
#| code-fold: show
peaktrip_hex %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

We can save the output into a rds file.

```{r}
#| code-fold: show
#| eval: false
write_rds(peaktrip_hex, "./data/rds/peaktrip_hex.rds")
```

#### 4.1.3 Visualize O-D Flows

Display the O-D flows of the passenger trips by using appropriate geovisualisation methods.

First let's ensure there aren't any observations with same origin and destination.

```{r}
#| code-fold: show
#| eval: false
peaktrip_hex %>%
  filter(ORIGIN_BS==DESTIN_BS)
```

Then we can create flow lines and check summary of data in case there are any zero.

```{r}
#| code-fold: show
peaktrip_flow <- od2line(flow = peaktrip_hex, 
                    zones = bus_hex,
                    zone_code = "BUS_STOP_N")

summary(peaktrip_flow)
```

Till now, we can plot out the bus trip flow during all 4 peak time intervals in total.

```{r}
tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peaktrip_flow %>% 
  filter(TRIPS >= 2000) %>%
tm_shape() +
  tm_lines(lwd = "TRIPS",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           col = "darkgreen",
           alpha = 0.6)
```

And we can visualize 4 peak time intervals in facets style to check any difference within.

First need to wrangling the data to put trips data of different time intervals into different columns.

```{r}
peak_interval_trips <- peak_trips %>%
  pivot_wider(names_from = interval, 
              values_from = TRIPS, 
              values_fill = 0)

peak_interval_hex <- left_join(peak_interval_trips,
                          bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("ORIGIN_PT_CODE" = "BUS_STOP_N"))  %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         DESTIN_BS = DESTINATION_PT_CODE) 


peak_interval_hex <- left_join(peak_interval_hex, bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("DESTIN_BS" = "BUS_STOP_N"),
                          suffix = c(".origin", ".destin")) 

glimpse(peak_interval_hex)
```

Before we continue, we can check duplication and save the result as a rds file.

```{r}
#| code-fold: show
#| eval: false
peak_interval_hex %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

```{r}
#| code-fold: show
#| eval: false
write_rds(peak_interval_hex, "./data/rds/peak_interval_hex.rds")
```

Then we can create flow lines.

```{r}
peak_interval_flow <- od2line(flow = peak_interval_hex, 
                    zones = bus_hex,
                    zone_code = "BUS_STOP_N")

summary(peak_interval_flow)
```

Then we can plot out 4 peak intervals in facets.

```{r}
#| fig-width: 10
#| fig-height: 10
weekdayam_plot <- tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peak_interval_flow %>% 
  filter(weekdayam >= 5000) %>%
tm_shape() +
  tm_lines(lwd = "weekdayam",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           col = "darkgreen",
           alpha = 0.6) +
  tm_layout(main.title = "Trips during Weekday 6am till 9am",
            main.title.size = 1.2)

weekdaypm_plot <- tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peak_interval_flow %>% 
  filter(weekdaypm >= 5000) %>%
tm_shape() +
  tm_lines(lwd = "weekdaypm",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           col = "darkgreen",
           alpha = 0.6) +
  tm_layout(main.title = "Trips during Weekday 5pm till 8pm",
            main.title.size = 1.2)

weekendam_plot <- tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peak_interval_flow %>% 
  filter(weekendam >= 2000) %>%
tm_shape() +
  tm_lines(lwd = "weekendam",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           col = "darkgreen",
           alpha = 0.6) +
  tm_layout(main.title = "Trips during Weekend/Holiday 11am till 2pm",
            main.title.size = 1.2)

weekendpm_plot <- tm_shape(sz) +
  tmap_options(check.and.fix = TRUE) +
  tm_polygons() +
peak_interval_flow %>% 
  filter(weekendpm >= 2000) %>%
tm_shape() +
  tm_lines(lwd = "weekendpm",
           style = "quantile",
           scale = c(0.1, 1, 3, 5, 7, 10),
           n = 6,
           col = "darkgreen",
           alpha = 0.6) +
  tm_layout(main.title = "Trips during Weekend/Holiday 4pm till 7pm",
            main.title.size = 1.2)

tmap_arrange(weekdayam_plot, weekdaypm_plot, weekendam_plot, weekendpm_plot, asp=1, ncol=2,
             outer.margins = 0)
```

::: callout-note
**Observations:**

-   The heavy traffic lines are similar during weekdays no matter morning or evening peak time, and the locations are reconcile with those hexagons which are condensed with count of bus stops.

-   The flow lines between Woodlands and Tampines are the common busy lines during all peak hours no matter weekdays or weekends.

-   Trip flow during weekend/holiday afternoon peak hours is the most disperse one.
:::

#### 4.1.4 Propulsive and Attractiveness variables

Firstly import all those propulsive and Attractiveness variables.

```{r}
business = st_read(dsn = "./data/geospatial",
                   layer = "Business")  %>% st_transform(crs = 3414) 

entertn = st_read(dsn = "./data/geospatial",
                   layer = "entertn")  %>% st_transform(crs = 3414) 

food = st_read(dsn = "./data/geospatial",
                   layer = "F&B")  %>% st_transform(crs = 3414) 

finance = st_read(dsn = "./data/geospatial",
                   layer = "FinServ")  %>% st_transform(crs = 3414) 

leisure = st_read(dsn = "./data/geospatial",
                   layer = "Leisure&Recreation")  %>% st_transform(crs = 3414) 

retail = st_read(dsn = "./data/geospatial",
                   layer = "Retails")  %>% st_transform(crs = 3414) 
```

Then we can assemble all those variables with our hexagon.

```{r}
bus_var <- bus_hex %>% 
  select(BUS_STOP_N, N, geometry) %>%
  mutate(retail_N = lengths(st_intersects(bus_hex, retail)),
         business_N = lengths(st_intersects(bus_hex, business)),
         finance_N = lengths(st_intersects(bus_hex, finance)),
         food_N = lengths(st_intersects(bus_hex, food)),
         leisure_N = lengths(st_intersects(bus_hex, leisure)),
         entertn_N = lengths(st_intersects(bus_hex, entertn))) %>%
  mutate(retail_N = ifelse(retail_N>0, retail_N, 0.01),
         business_N = ifelse(business_N>0, business_N, 0.01),
         finance_N = ifelse(finance_N>0, finance_N, 0.01),
         food_N = ifelse(food_N>0, food_N, 0.01),
         leisure_N = ifelse(leisure_N>0, leisure_N, 0.01),
         entertn_N = ifelse(entertn_N>0, entertn_N, 0.01))

summary(bus_var)
```

::: callout-important
-   Here we use length of intersect between the geometry of variables and hexagons as count of variables.

-   We need to treat any case of 0, give it a constant 0.1 for further model fitting.
:::

#### 4.1.5 Distance Matrix

First we need to convert the bus_hex we have generated into **Spatial Polygons Data Frame.**

```{r}
bus_sp <- as(bus_hex, "Spatial")
bus_sp
```

Then we can compute distance matrix of our bus stops hexagons and check the head 5.

```{r}
dist <- spDists(bus_sp, 
                longlat = FALSE)
head(dist, n=c(5, 5))
```

And we need to give names to columns and rows of the matrix.

```{r}
busstop_N <- bus_sp$BUS_STOP_N

colnames(dist) <- paste0(busstop_N)
rownames(dist) <- paste0(busstop_N)

head(dist, n=c(5, 5))
```

Then we can label the bus stops and convert it to pair-distance.

```{r}
distPair <- as.data.frame(as.table(dist)) 
names(distPair) <- c("orig", "dest", "dist")
distPair <- distPair %>%
  filter(orig != dest)

glimpse(distPair)
```

Let's give a constant distance of 50m to intra-hexagon bus stops pairs before continue.

```{r}
distPair$dist <- ifelse(distPair$dist == 0,
                        50, distPair$dist)

summary(distPair)
```

### **4.2 Spatial Interaction Modelling**

#### 4.2.1 Data preparation

For this part, I will focus on weekend/holiday evening peak time interval for further analysis.

```{r}
weekendpm_trips <- odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %>%
  reframe(TRIPS = sum(TOTAL_TRIPS))
 
weekendpm_trips <- weekendpm_trips %>%
  filter(ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %>%
  filter(DESTINATION_PT_CODE %in% bus_hex$'BUS_STOP_N')

weekendpm_hex <- left_join(weekendpm_trips,bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("ORIGIN_PT_CODE" = "BUS_STOP_N"))  %>%
  rename(ORIGIN_BS = ORIGIN_PT_CODE,
         DESTIN_BS = DESTINATION_PT_CODE) 

weekendpm_hex <- left_join(weekendpm_hex, bus_hex %>% select(BUS_STOP_N, geometry), 
                          by = c("DESTIN_BS" = "BUS_STOP_N"),
                          suffix = c(".origin", ".destin")) 
```

Create flow lines for those trips data.

```{r}
weekendpm_flow <- od2line(flow = weekendpm_hex, 
                    zones = bus_hex,
                    zone_code = "BUS_STOP_N") 

summary(weekendpm_flow)
```

To ensure, let's check any case of origin == destination before continue.

```{r}
#| code-fold: show
#| eval: false
weekendpm_flow %>%
  filter(ORIGIN_BS == DESTIN_BS)
```

Then we can join the flow data with pairwise distance and use summary to check any case of 0.

```{r}
weekendpm_dist <- weekendpm_flow %>%
  left_join (distPair,
             by = c("ORIGIN_BS" = "orig",
                    "DESTIN_BS" = "dest"))

summary(weekendpm_dist)
```

Till now we can join the flow data with all 6 propulsive and Attractiveness variables by both origin and destination bus stops.

```{r}
weekendpm_SIM <- weekendpm_dist %>%
  left_join(as.data.frame(bus_var) %>% select(-geometry), 
            by = c("ORIGIN_BS"="BUS_STOP_N")) %>%
  left_join(as.data.frame(bus_var) %>% select(-geometry), 
            by = c("DESTIN_BS"="BUS_STOP_N"), 
            suffix = c("_orig", "_dest"))
#weekendpm_SIM[is.na(weekendpm_SIM)] <- 0.1

summary(weekendpm_SIM)
```

#### 4.2.2 Multi-collinearity of explanatory variables

Before we calibrate explanatory models, we should check the collinearity between our variables.

```{r}
#| fig-width: 10
#| fig-height: 10
var_cor <- cor(as.data.frame(weekendpm_SIM) %>% select(-c(1,2,19,20,21)))

corrplot(var_cor, method = "color")
```

::: callout-notice
Here we can find that:

-   distance is noticeable negative correlated with number of trips;

-   number of leisure points and number of food points are closely positive correlated;

-   number of retail points and number of finance points are closely positive correlated.
:::

Then we want to check the r-squared of each explanatory variables to decide which variable we should choose for further analysis.

```{r}
vars <- colnames(weekendpm_SIM)[-c(1,2,3,19,20,21)]

r_squared <- vector("numeric", length(vars))
#p_value <- vector("numeric", length(vars))
#coeffi <- vector("numeric", length(vars))

for (i in seq_along(vars)) {
#  formula <- as.formula(paste("TRIPS ~ log(", vars[i], ")", sep = ""))  
#  model <- glm(formula, family = poisson(link = "log"), data = weekendpm_SIM)
  r_squared[i] <- round(var_cor[1,i+1]^2,5)
#  p_value[i] <- round(summary(model)$coefficients[2, 4],5)
#  coeffi <- round(summary(model)$coefficients[2, 1],5)
}

data.frame(Variable = vars, R_squared = r_squared)
```

From above correlation analysis, I would drop number of retail, food and entertain point for further analysis.

```{r}
weekendpm_SIM_selected <- weekendpm_SIM %>%
  select(-c("retail_N_orig", "retail_N_dest", "food_N_orig", "food_N_dest", "entertn_N_orig", "entertn_N_dest"))
```

#### 4.2.3 Unconstrained/Origin-constrained/Destination-constrained SIMs.

Now we can calibrate an unconstrained spatial interaction model by using `glm().`

```{r}
uncSIM <- glm(formula = TRIPS ~ 
                log(finance_N_orig) + 
                log(finance_N_dest) +
                log(business_N_orig) + 
                log(business_N_dest) +
                log(leisure_N_orig) + 
                log(leisure_N_dest) +
                log(dist),
              family = poisson(link = "log"),
              data = weekendpm_SIM_selected,
              na.action = na.exclude)
uncSIM
```

Then let's fit a origin/destination/doubly constrained SIM .

```{r}
#| eval: false
origSIM <- glm(formula = TRIPS ~ 
                ORIGIN_BS + 
                log(finance_N_dest) +
                log(business_N_dest) +
                log(leisure_N_dest) +
                log(dist),
              family = poisson(link = "log"),
              data = weekendpm_SIM_selected,
              na.action = na.exclude)

destSIM <- glm(formula = TRIPS ~ 
                DESTIN_BS + 
                log(finance_N_orig) +
                log(business_N_orig) +
                log(leisure_N_orig) +
                log(dist),
              family = poisson(link = "log"),
              data = weekendpm_SIM_selected,
              na.action = na.exclude)
```

```{r}
#| eval: false
dbcSIM <- glm(formula = TRIPS ~ 
                ORIGIN_BS + 
                DESTIN_BS +
                log(dist),
              family = poisson(link = "log"),
              data = weekendpm_SIM_selected,
              na.action = na.exclude)

tail(summary(dbcSIM)$coefficients, 3)
```

#### 4.2.4 Model performance checking

Check the performance of 4 models.

#### 4.2.5 Visualizing Modelling results

Present the modelling results by using appropriate geovisualization and graphical visualization methods.

::: callout-note
**Observations:**

-   With reference to the Spatial Interaction Model output tables, maps and data visualization prepared, describe the modelling results
:::
