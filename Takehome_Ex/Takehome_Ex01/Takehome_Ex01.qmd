---
title: "Take Home Exercise 1"
author: "LIANG YAO"
date: '3 Dec 2023'
date-modified: "`r Sys.Date()`"
execute: 
  warning: false
  echo: true
  eval: true
format: 
  html: 
    code-fold: true
    code-summary: "Show the code"
---

# **Geospatial Analytics for Public Good**

## **1 Setting the Scene**

As city-wide urban infrastructures such as buses, taxis, mass rapid transit, public utilities and roads become digital, the datasets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS and RFID on the vehicles. For example, routes and ridership data were collected with the use of smart cards and Global Positioning System (GPS) devices available on the public buses. These massive movement data collected are likely to contain structure and patterns that provide useful information about characteristics of the measured phenomena. The identification, analysis and comparison of such patterns will provide greater insights on human movement and behaviours within a city. These understandings will potentially contribute to a better urban management and useful information for urban transport services providers both from the private and public sector to formulate informed decision to gain competitive advantage.

In real-world practices, the use of these massive locational aware data, however, tend to be confined to simple tracking and mapping with GIS applications. This is mainly due to a general lack of functions in conventional GIS which is capable of analysing and model spatial and spatio-temporal data effectively.

## **2 Objectives**

Exploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

## **3 The Data**

First of all, load needing packages.

```{r}
pacman::p_load(sf, spdep, tmap, tidyverse, sfdep, Kendall)
```

### **3.1 Aspatial data**

For the purpose of this take-home exercise, *Passenger Volume by Origin Destination Bus Stops* downloaded from [LTA DataMall](https://www.waterpointdata.org/access-data/) will be used.

Import the passenger volume by origin destination bus stops data.

```{r}
odbus = read_csv("./data/aspatial/origin_destination_bus_202308.csv")  %>%
  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),
         DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))
```

### **3.2 Geospatial data**

Two geospatial data will be used in this study, they are:

-   *Bus Stop Location* from LTA DataMall. It provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.

```{r}
busstop = st_read(dsn = "./data/geospatial/BusStopLocation_Jul2023",
                   layer = "BusStop")  %>% st_transform(crs = 3414) %>% 
  distinct(BUS_STOP_N, .keep_all = TRUE)
```

::: callout-important
Here I found that there are several rows in 'busstop' data have duplicate BUS_STOP_N but slightly different geometry, so I used distinct to keep only one of those for doing intersection with hexagon.
:::

-   Also import subzone geometry data as our background layer.

```{r}
sz = st_read(dsn = "./data/geospatial",
                   layer = "MPSZ-2019")  %>% st_transform(crs = 3414) 
```

-   *hexagon*, a [hexagon](https://desktop.arcgis.com/en/arcmap/latest/tools/spatial-statistics-toolbox/h-whyhexagons.htm) layer of 250m (this distance is the perpendicular distance between the centre of the hexagon and its edges.) should be used to replace the relative coarse and irregular Master Plan 2019 Planning Sub-zone GIS data set of URA.

```{r}
hexagon <- st_sf(geometry = st_make_grid(busstop, cellsize = c(250,250), what = "polygons",square = FALSE)) %>%
  mutate(id = row_number()) %>% 
  st_transform(crs = 3414) 
```

Then we need to join bus stop with hexagon, and join with subzone to exclude hexagons out of range.

```{r}
bus_hex <- st_join(
  st_join(hexagon,busstop%>%select(BUS_STOP_N,geometry), join = st_nearest_feature),
  sz) %>%
  drop_na() %>%
  distinct(BUS_STOP_N, .keep_all = TRUE)
```

Check for duplicate geometry.

```{r}
bus_hex %>%
  group_by(geometry)%>%
  filter(row_number()>1)
```

## **4 The Task**

The specific tasks of this take-home exercise are as follows:

### **4.1 Geovisualisation and Analysis**

-   With reference to the time intervals provided in the table below, compute the passenger trips generated by origin at the hexagon level,

    | Peak hour period             | Bus tap on time |
    |------------------------------|-----------------|
    | Weekday morning peak         | 6am to 9am      |
    | Weekday afternoon peak       | 5pm to 8pm      |
    | Weekend/holiday morning peak | 11am to 2pm     |
    | Weekend/holiday evening peak | 4pm to 7pm      |

-   Display the geographical distribution of the passenger trips by using appropriate geovisualisation methods,

-   Describe the spatial patterns revealed by the geovisualisation (not more than 200 words per visual).

#### 4.1.1 Extracting Data

Extract peak data, and combine 4 time intervals data for further use.

```{r}
peak_trips <- bind_rows(
  odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 6 &
           TIME_PER_HOUR <= 9) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  mutate(interval = "weekday_6_9"),
  odbus %>%
  filter(DAY_TYPE == "WEEKDAY") %>%
  filter(TIME_PER_HOUR >= 17 &
           TIME_PER_HOUR <= 20) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  mutate(interval = "weekday_17_20"),
  odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 11 &
           TIME_PER_HOUR <= 14) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  mutate(interval = "weekend_11_14"),
  odbus %>%
  filter(DAY_TYPE == "WEEKENDS/HOLIDAY") %>%
  filter(TIME_PER_HOUR >= 16 &
           TIME_PER_HOUR <= 19) %>%
  group_by(ORIGIN_PT_CODE) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  mutate(interval = "weekend_16_19"))

glimpse(peak_trips)
```

#### 4.1.2 Wrangling geospatial data

First combine passenger trip data with geospatial data.

```{r}
origin_trips <- left_join(peak_trips, busstop,
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  rename(BUS_STOP_N = ORIGIN_PT_CODE) %>%
  group_by(BUS_STOP_N) %>%
  summarise(TRIPS = sum(TRIPS))
```

Duplication check before continue:

```{r}
origin_trips %>%
  group_by_all() %>%
  filter(n()>1) %>%
  ungroup()
```

#### 4.1.3 Visualizing passenger trips

4.1.3.1 Firstly, let's check out the distribution of peak time trips of 4 time intervals in total.

Below code chunk aims at wrangling the peak time trips data for visualization.

```{r}
peaktrip_hex <- left_join(bus_hex %>% select(id, BUS_STOP_N, geometry), 
                          origin_trips, by = join_by(BUS_STOP_N)) 
```

Now we can visualize the distribution of total bus trips.

```{r}
tmap_mode("plot")
tm_shape(sz) +
  tm_polygons(alpha = 0.3) +
  tm_borders(alpha = 0.2) +
tm_shape(peaktrip_hex) +
  tm_fill("TRIPS", 
          style = "quantile", 
          palette = "Blues",
          title = "Passenger trips",
          colorNA = NULL,
          showNA = FALSE) +
  tm_layout(main.title = "Peak Time Passenger Trips",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45, 
            legend.width = 0.35,
            frame = TRUE) +
  tm_compass(type="4star", size = 1.5) +
  tm_borders(alpha = 0.5) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
```

4.1.3.2 Then, to display the geographical distribution of 4 time intervals separately for comparison:

-   Regenerate trip data with the "interval" column to indicate different time intervals.

    ```{r}
    peak_trips_interval <- left_join(peak_trips, busstop,
                by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
      rename(BUS_STOP_N = ORIGIN_PT_CODE) %>%
      group_by(BUS_STOP_N, interval) %>%
      summarise(TRIPS = sum(TRIPS)) %>% 
      mutate(daily_trips = 
               ifelse(grepl("weekday",interval), ceiling(TRIPS/22), ceiling(TRIPS/9)))

    glimpse(peak_trips_interval)
    ```

    ::: callout-important
    Here I create a column "daily_trips" to find number of trips per day, since it's hard to compare the absolute number when we are differentiating weekday and weekend peak times intervals. For Aug 2023, there are 31 days in which 8 days are weekends and 1 day is National holiday.
    :::

    ```{r}
    peak_dailytrips_interval <- peak_trips_interval %>%
      select(BUS_STOP_N,interval,daily_trips) %>%
      pivot_wider(names_from = interval, 
                  values_from = daily_trips, 
                  values_fill = NA)

    head(peak_dailytrips_interval, 5)
    ```

-   Join back with hexagon.

    ```{r}
    interval_dailytrip_hex <- left_join(bus_hex, peak_dailytrips_interval,
                               by = join_by(BUS_STOP_N)) 
    ```

-   Then we can visualize daily trip distribution for each time intervals.

    ```{r}
    tmap_mode("plot")
    tm_shape(sz) +
      tm_polygons(alpha = 0.3) +
      tm_borders(alpha = 0.2) +
    tm_shape(interval_dailytrip_hex)+ 
      tm_polygons(c("weekday_6_9","weekday_17_20","weekend_11_14","weekend_16_19"),
              style = "quantile", 
              palette = "Blues",
              title = "",
              colorNA = NULL,
              showNA = FALSE) + 
      tm_layout(legend.position = c("right", "bottom"),
        panel.show = TRUE,
        panel.labels = c("Weekday 6-9am", "Weekday 5-8pm", "Weekend 11am-2pm", "Weekend 4-7pm"))
    ```

::: callout-note
**Observation:**

1.  From the scale bar we can find that on daily bases, the number of bus trips at evening peak time intervals is larger than morning peak time intervals.

2.  The locations with heavy number of trips are very similar during all peak time intervals.
:::

### **4.2 Local Indicators of Spatial Association (LISA) Analysis**

-   Compute LISA of the passengers trips generate by origin at hexagon level.

-   Display the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e. p-value \< 0.05)

-   With reference to the analysis results, draw statistical conclusions (not more than 200 words per visual).

#### 4.2.1 Compute LISA of the passengers trips

First we need to derive contiguity weights use knn method.

```{r}
weight_q <-  peaktrip_hex%>%
  mutate(nb = st_knn(geometry,k=3),
         wt = st_inverse_distance(nb, geometry),
         .before = 1) %>%
  mutate(TRIPS = replace_na(TRIPS,0))

weight_q
```

::: callout-important
-   To treat those area with sparse location of bus stops (0 neighbor case), I used st_knn instead of st_contiguity, since our bus stops' geometry are not necessarily adjacent to each other.

-   Hence since there are lagged value to zone without neighbors, I set allow_zero to True.
:::

Before continue, we can perform a **Global Moran'I permutation test**.

```{r}
set.seed(1234)
global_moran_perm(weight_q$TRIPS,
                  weight_q$nb,
                  weight_q$wt,
                  adjust.n = TRUE,
                  nsim = 99)
```

::: callout-note
**Observation:**

The Moran'I statistic (0.10933) indicates a slightly positive spatial autocorrelation, suggesting that similar values tend to be clustered together in our map, so we can continue with LISA.
:::

Then we can compute LISA of bus trips during peak time hours.

```{r}
lisa <- weight_q %>% 
  mutate(local_moran = local_moran(
    TRIPS, nb, wt, nsim = 99),
         .before = 1) %>%
  unnest(local_moran)

glimpse(lisa)
```

#### 4.2.2 Display LISA maps of passenger trips

First of all, let's check out local Moran's I and p-value.

```{r}
tmap_mode("plot")

map1 <- 
  tm_shape(sz) +
  tm_polygons(alpha = 0.3) +
  tm_borders(alpha = 0.2) +
  tm_shape(lisa) +
  tm_fill("ii") + 
  tm_borders(alpha = 0.5) +
  tm_view(set.zoom.limits = c(6,8)) +
  tm_layout(main.title = "local Moran's I of Peak Time Trips",
            main.title.size = 0.8)

map2 <- 
  tm_shape(sz) +
  tm_polygons(alpha = 0.3) +
  tm_borders(alpha = 0.2) +
  tm_shape(lisa) +
  tm_fill("p_ii_sim",
          breaks = c(0, 0.001, 0.01, 0.05, 1),
              labels = c("0.001", "0.01", "0.05", "Not sig")) + 
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "p-value of local Moran's I",
            main.title.size = 0.8)

tmap_arrange(map1, map2, ncol = 2)
```

Then we can display only the significant (p-value \< 0.05) part on map.

```{r}
lisa_sig <- lisa  %>%
  filter(p_ii_sim < 0.05)
tmap_mode("plot")
tm_shape(sz) +
  tm_polygons(alpha = 0.3) +
  tm_borders(alpha = 0.2) +
tm_shape(lisa) +
  tm_polygons(alpha = 0.1) +
  tm_borders(alpha = 0.1) +
tm_shape(lisa_sig) +
  tm_fill("mean") + 
  tm_borders(alpha = 0.8)
```

::: callout-note
**Observation:**

Hot points mostly concentrate in those busy bus stops and cold points mostly in border area.
:::
