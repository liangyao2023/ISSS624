[
  {
    "objectID": "Takehome_Ex/Takehome_Ex01/data/geospatial/MPSZ-2019.html",
    "href": "Takehome_Ex/Takehome_Ex01/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html",
    "href": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "As city-wide urban infrastructures such as buses, taxis, mass rapid transit, public utilities and roads become digital, the datasets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS and RFID on the vehicles. For example, routes and ridership data were collected with the use of smart cards and Global Positioning System (GPS) devices available on the public buses. These massive movement data collected are likely to contain structure and patterns that provide useful information about characteristics of the measured phenomena. The identification, analysis and comparison of such patterns will provide greater insights on human movement and behaviours within a city. These understandings will potentially contribute to a better urban management and useful information for urban transport services providers both from the private and public sector to formulate informed decision to gain competitive advantage.\nIn real-world practices, the use of these massive locational aware data, however, tend to be confined to simple tracking and mapping with GIS applications. This is mainly due to a general lack of functions in conventional GIS which is capable of analysing and model spatial and spatio-temporal data effectively.\n\n\n\nExploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.\n\n\n\nFirst of all, load needing packages.\n\n\nCode\npacman::p_load(sf, spdep, tmap, tidyverse, sfdep, Kendall)\n\n\n\n\nFor the purpose of this take-home exercise, Passenger Volume by Origin Destination Bus Stops downloaded from LTA DataMall will be used.\nImport the passenger volume by origin destination bus stops data.\n\n\nCode\nodbus = read_csv(\"./data/aspatial/origin_destination_bus_202308.csv\")  %&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),\n         DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))\n\n\n\n\n\nTwo geospatial data will be used in this study, they are:\n\nBus Stop Location from LTA DataMall. It provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.\n\n\n\nCode\nbusstop = st_read(dsn = \"./data/geospatial/BusStopLocation_Jul2023\",\n                   layer = \"BusStop\")  %&gt;% st_transform(crs = 3414) %&gt;% \n  distinct(BUS_STOP_N, .keep_all = TRUE)\n\n\nReading layer `BusStop' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex01/data/geospatial/BusStopLocation_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHere I found that there are several rows in ‘busstop’ data have duplicate BUS_STOP_N but slightly different geometry, so I used distinct to keep only one of those for doing intersection with hexagon.\n\n\n\nAlso import subzone geometry data as our background layer.\n\n\n\nCode\nsz = st_read(dsn = \"./data/geospatial\",\n                   layer = \"MPSZ-2019\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nhexagon, a hexagon layer of 250m (this distance is the perpendicular distance between the centre of the hexagon and its edges.) should be used to replace the relative coarse and irregular Master Plan 2019 Planning Sub-zone GIS data set of URA.\n\n\n\nCode\nhexagon &lt;- st_sf(geometry = st_make_grid(busstop, cellsize = c(250,250), what = \"polygons\",square = FALSE)) %&gt;%\n  mutate(id = row_number()) %&gt;% \n  st_transform(crs = 3414) \n\n\nThen we need to join bus stop with hexagon, and join with subzone to exclude hexagons out of range.\n\n\nCode\nbus_hex &lt;- st_join(\n  st_join(hexagon,busstop%&gt;%select(BUS_STOP_N,geometry), join = st_nearest_feature),\n  sz) %&gt;%\n  drop_na() %&gt;%\n  distinct(BUS_STOP_N, .keep_all = TRUE)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo avoid same geometry for multiple bus stop, here I choose to use st_nearest_feature as join method instead of intersection.\n\n\nCheck for duplicate geometry.\n\n\nCode\nbus_hex %&gt;%\n  group_by(geometry)%&gt;%\n  filter(row_number()&gt;1)\n\n\nSimple feature collection with 0 features and 8 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 0 × 9\n# Groups:   geometry [0]\n# ℹ 9 variables: id &lt;int&gt;, BUS_STOP_N &lt;chr&gt;, SUBZONE_N &lt;chr&gt;, SUBZONE_C &lt;chr&gt;,\n#   PLN_AREA_N &lt;chr&gt;, PLN_AREA_C &lt;chr&gt;, REGION_N &lt;chr&gt;, REGION_C &lt;chr&gt;,\n#   geometry &lt;GEOMETRY [m]&gt;\n\n\n\n\n\n\nThe specific tasks of this take-home exercise are as follows:\n\n\n\nWith reference to the time intervals provided in the table below, compute the passenger trips generated by origin at the hexagon level,\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the geographical distribution of the passenger trips by using appropriate geovisualisation methods,\nDescribe the spatial patterns revealed by the geovisualisation (not more than 200 words per visual).\n\n\n\nExtract peak data, and combine 4 time intervals data for further use.\n\n\nCode\npeak_trips &lt;- bind_rows(\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(interval = \"weekday_6_9\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 &\n           TIME_PER_HOUR &lt;= 20) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(interval = \"weekday_17_20\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 &\n           TIME_PER_HOUR &lt;= 14) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(interval = \"weekend_11_14\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(interval = \"weekend_16_19\"))\n\nglimpse(peak_trips)\n\n\nRows: 20,044\nColumns: 3\n$ ORIGIN_PT_CODE &lt;fct&gt; 01012, 01013, 01019, 01029, 01039, 01059, 01109, 01112,…\n$ TRIPS          &lt;dbl&gt; 1973, 952, 1789, 2561, 2938, 1651, 161, 8492, 9015, 424…\n$ interval       &lt;chr&gt; \"weekday_6_9\", \"weekday_6_9\", \"weekday_6_9\", \"weekday_6…\n\n\n\n\n\nFirst combine passenger trip data with geospatial data.\n\n\nCode\norigin_trips &lt;- left_join(peak_trips, busstop,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(BUS_STOP_N = ORIGIN_PT_CODE) %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  summarise(TRIPS = sum(TRIPS))\n\n\nDuplication check before continue:\n\n\nCode\norigin_trips %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: BUS_STOP_N &lt;chr&gt;, TRIPS &lt;dbl&gt;\n\n\n\n\n\n4.1.3.1 Firstly, let’s check out the distribution of peak time trips of 4 time intervals in total.\nBelow code chunk aims at wrangling the peak time trips data for visualization.\n\n\nCode\npeaktrip_hex &lt;- left_join(bus_hex %&gt;% select(id, BUS_STOP_N, geometry), \n                          origin_trips, by = join_by(BUS_STOP_N)) \n\n\nNow we can visualize the distribution of total bus trips.\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\ntm_shape(peaktrip_hex) +\n  tm_fill(\"TRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Passenger trips\",\n          colorNA = NULL,\n          showNA = FALSE) +\n  tm_layout(main.title = \"Peak Time Passenger Trips\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"4star\", size = 1.5) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n4.1.3.2 Then, to display the geographical distribution of 4 time intervals separately for comparison:\nRegenerate trip data with the “interval” column to indicate different time intervals.\n\n\nCode\npeak_trips_interval &lt;- left_join(peak_trips, busstop,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(BUS_STOP_N = ORIGIN_PT_CODE) %&gt;%\n  group_by(BUS_STOP_N, interval) %&gt;%\n  summarise(TRIPS = sum(TRIPS)) %&gt;% \n  mutate(daily_trips = \n           ifelse(grepl(\"weekday\",interval), ceiling(TRIPS/22), ceiling(TRIPS/9)))\n\nglimpse(peak_trips_interval)\n\n\nRows: 20,044\nColumns: 4\nGroups: BUS_STOP_N [5,067]\n$ BUS_STOP_N  &lt;chr&gt; \"01012\", \"01012\", \"01012\", \"01012\", \"01013\", \"01013\", \"010…\n$ interval    &lt;chr&gt; \"weekday_17_20\", \"weekday_6_9\", \"weekend_11_14\", \"weekend_…\n$ TRIPS       &lt;dbl&gt; 8448, 1973, 2273, 3208, 7328, 952, 1697, 2796, 3608, 1789,…\n$ daily_trips &lt;dbl&gt; 384, 90, 253, 357, 334, 44, 189, 311, 164, 82, 168, 181, 4…\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHere I create a column “daily_trips” to find number of trips per day, since it’s hard to compare the absolute number when we are differentiating weekday and weekend peak times intervals. For Aug 2023, there are 31 days in which 8 days are weekends and 1 day is National holiday.\n\n\nTo draw polygons, I will use pivot_wider to pivot interval into columns.\n\n\nCode\npeak_dailytrips_interval &lt;- peak_trips_interval %&gt;%\n  select(BUS_STOP_N,interval,daily_trips) %&gt;%\n  pivot_wider(names_from = interval, \n              values_from = daily_trips, \n              values_fill = NA)\n\nhead(peak_dailytrips_interval, 5)\n\n\n# A tibble: 5 × 5\n# Groups:   BUS_STOP_N [5]\n  BUS_STOP_N weekday_17_20 weekday_6_9 weekend_11_14 weekend_16_19\n  &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 01012                384          90           253           357\n2 01013                334          44           189           311\n3 01019                164          82           168           181\n4 01029                424         117           364           472\n5 01039                589         134           603           823\n\n\nJoin back with hexagon before visualization.\n\n\nCode\ninterval_dailytrip_hex &lt;- left_join(bus_hex, peak_dailytrips_interval,\n                           by = join_by(BUS_STOP_N)) \n\n\nThen we can visualize daily trip distribution for each time intervals.\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\ntm_shape(interval_dailytrip_hex)+ \n  tm_polygons(c(\"weekday_6_9\",\"weekday_17_20\",\"weekend_11_14\",\"weekend_16_19\"),\n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"\",\n          colorNA = NULL,\n          showNA = FALSE) + \n  tm_layout(legend.position = c(\"right\", \"bottom\"),\n    panel.show = TRUE,\n    panel.labels = c(\"Weekday 6-9am\", \"Weekday 5-8pm\", \"Weekend 11am-2pm\", \"Weekend 4-7pm\"))\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation:\n\nFrom the scale bar we can find that on daily bases, the number of bus trips at evening peak time intervals is larger than morning peak time intervals.\nThe locations with heavy number of trips are very similar during all peak time intervals.\n\n\n\n\n\n\n\n\nCompute LISA of the passengers trips generate by origin at hexagon level.\nDisplay the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e. p-value &lt; 0.05)\nWith reference to the analysis results, draw statistical conclusions (not more than 200 words per visual).\n\n\n\nFirst we need to derive contiguity weights use knn method.\n\n\nCode\nweight_q &lt;-  peaktrip_hex%&gt;%\n  mutate(nb = st_knn(geometry,k=3),\n         wt = st_inverse_distance(nb, geometry),\n         .before = 1) %&gt;%\n  mutate(TRIPS = replace_na(TRIPS,0))\n\nweight_q\n\n\nSimple feature collection with 4044 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 3720.122 ymin: 26337.76 xmax: 48220.12 ymax: 50225.63\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n           nb                                 wt  id BUS_STOP_N TRIPS\n1   2, 11, 20 0.04618802, 0.06099943, 0.06575959   1      25059   549\n2  11, 12, 14 0.11094004, 0.11094004, 0.08728716   6      25751   210\n3    5, 7, 18    0.2000000, 0.1333333, 0.1109400  73      26379   374\n4   7, 12, 14    0.4000000, 0.2000000, 0.2309401 195      25719  4555\n5    3, 8, 18    0.2000000, 0.1511858, 0.1511858 198      26369   403\n6   9, 10, 15    0.4000000, 0.1511858, 0.1333333 202      26279   722\n7   4, 14, 17    0.4000000, 0.2000000, 0.1511858 257      26389   780\n8   5, 15, 23    0.1511858, 0.2000000, 0.1109400 261      26299   499\n9   6, 10, 13    0.4000000, 0.2309401, 0.1511858 264      26261 15682\n10   6, 9, 13    0.1511858, 0.2309401, 0.4000000 265      26251  2419\n                         geometry\n1  POLYGON ((3845.122 26554.27...\n2  POLYGON ((3845.122 28719.33...\n3  POLYGON ((3970.122 30667.89...\n4  POLYGON ((4220.122 29801.86...\n5  POLYGON ((4220.122 31100.9,...\n6  POLYGON ((4220.122 32832.95...\n7  POLYGON ((4345.122 30018.37...\n8  POLYGON ((4345.122 31750.42...\n9  POLYGON ((4345.122 33049.46...\n10 POLYGON ((4345.122 33482.47...\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nTo treat those area with sparse location of bus stops (0 neighbor case), I used st_knn instead of st_contiguity, since our bus stops’ geometry are not necessarily adjacent to each other.\nHence since there are lagged value to zone without neighbors, I set allow_zero to True.\n\n\n\nBefore continue, we can perform a Global Moran’I permutation test.\n\n\nCode\nset.seed(1234)\nglobal_moran_perm(weight_q$TRIPS,\n                  weight_q$nb,\n                  weight_q$wt,\n                  adjust.n = TRUE,\n                  nsim = 99)\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.10933, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation:\nThe Moran’I statistic (0.10933) indicates a slightly positive spatial autocorrelation, suggesting that similar values tend to be clustered together in our map, so we can continue with LISA.\n\n\nThen we can compute LISA of passenger trips during peak time hours.\n\n\nCode\nlisa &lt;- weight_q %&gt;% \n  mutate(local_moran = local_moran(\n    TRIPS, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\nglimpse(lisa)\n\n\nRows: 4,044\nColumns: 18\n$ ii           &lt;dbl&gt; 0.01683533, 0.03658211, 0.05060816, 0.06595975, 0.0565997…\n$ eii          &lt;dbl&gt; 0.0023890846, 0.0011069140, -0.0020652686, -0.0074375318,…\n$ var_ii       &lt;dbl&gt; 0.0004110818, 0.0018452585, 0.0075471964, 0.0087109926, 0…\n$ z_ii         &lt;dbl&gt; 0.7125097, 0.8258406, 0.6063156, 0.7864048, 0.7386234, 0.…\n$ p_ii         &lt;dbl&gt; 0.4761492, 0.4088945, 0.5443052, 0.4316304, 0.4601357, 0.…\n$ p_ii_sim     &lt;dbl&gt; 0.21, 0.07, 0.13, 0.04, 0.09, 0.82, 0.28, 0.03, 0.11, 0.5…\n$ p_folded_sim &lt;dbl&gt; 0.105, 0.035, 0.065, 0.020, 0.045, 0.410, 0.140, 0.015, 0…\n$ skewness     &lt;dbl&gt; -3.920333, -4.000057, -6.940666, -4.857223, -4.134682, -5…\n$ kurtosis     &lt;dbl&gt; 22.623553, 22.981888, 63.967522, 32.243401, 22.447469, 49…\n$ mean         &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ median       &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ pysal        &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ nb           &lt;nb&gt; &lt;2, 11, 20&gt;, &lt;11, 12, 14&gt;, &lt;5, 7, 18&gt;, &lt;7, 12, 14&gt;, &lt;3, 8,…\n$ wt           &lt;list&gt; &lt;0.04618802, 0.06099943, 0.06575959&gt;, &lt;0.11094004, 0.110…\n$ id           &lt;int&gt; 1, 6, 73, 195, 198, 202, 257, 261, 264, 265, 315, 318, 32…\n$ BUS_STOP_N   &lt;chr&gt; \"25059\", \"25751\", \"26379\", \"25719\", \"26369\", \"26279\", \"26…\n$ TRIPS        &lt;dbl&gt; 549, 210, 374, 4555, 403, 722, 780, 499, 15682, 2419, 241…\n$ geometry     &lt;POLYGON [m]&gt; POLYGON ((3845.122 26554.27..., POLYGON ((3845.12…\n\n\n\n\n\nFirst of all, let’s check out local Moran’s I and p-value.\n\n\nCode\ntmap_mode(\"plot\")\n\nmap1 &lt;- \n  tm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\n  tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Peak Time Trips\",\n            main.title.size = 0.8)\n\nmap2 &lt;- \n  tm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\n  tm_shape(lisa) +\n  tm_fill(\"p_ii_sim\",\n          palette = \"-Reds\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHere I reverse the color palette of p-value, to put more emphasize on significant spots (which with lower p-value).\n\n\nThen we can display only the significant (p-value &lt; 0.05) part on map.\n\n\nCode\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\ntm_shape(lisa) +\n  tm_polygons(alpha = 0.1) +\n  tm_borders(alpha = 0.1) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation:\n\nHot spots mostly concentrate in those busy bus stops and cold spots mostly in border area.\nThe location of those hot/cold spots go along with the density of passenger trip numbers.\nThere’s seldom High-Low or Low-High spots which would be spatial outliers."
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html#setting-the-scene",
    "href": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html#setting-the-scene",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "As city-wide urban infrastructures such as buses, taxis, mass rapid transit, public utilities and roads become digital, the datasets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS and RFID on the vehicles. For example, routes and ridership data were collected with the use of smart cards and Global Positioning System (GPS) devices available on the public buses. These massive movement data collected are likely to contain structure and patterns that provide useful information about characteristics of the measured phenomena. The identification, analysis and comparison of such patterns will provide greater insights on human movement and behaviours within a city. These understandings will potentially contribute to a better urban management and useful information for urban transport services providers both from the private and public sector to formulate informed decision to gain competitive advantage.\nIn real-world practices, the use of these massive locational aware data, however, tend to be confined to simple tracking and mapping with GIS applications. This is mainly due to a general lack of functions in conventional GIS which is capable of analysing and model spatial and spatio-temporal data effectively."
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html#objectives",
    "href": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html#objectives",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "Exploratory Spatial Data Analysis (ESDA) hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate Local Indicators of Spatial Association (GLISA) and Emerging Hot Spot Analysis (EHSA) to undercover the spatial and spatio-temporal mobility patterns of public bus passengers in Singapore."
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html#the-data",
    "href": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html#the-data",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "First of all, load needing packages.\n\n\nCode\npacman::p_load(sf, spdep, tmap, tidyverse, sfdep, Kendall)\n\n\n\n\nFor the purpose of this take-home exercise, Passenger Volume by Origin Destination Bus Stops downloaded from LTA DataMall will be used.\nImport the passenger volume by origin destination bus stops data.\n\n\nCode\nodbus = read_csv(\"./data/aspatial/origin_destination_bus_202308.csv\")  %&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),\n         DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))\n\n\n\n\n\nTwo geospatial data will be used in this study, they are:\n\nBus Stop Location from LTA DataMall. It provides information about all the bus stops currently being serviced by buses, including the bus stop code (identifier) and location coordinates.\n\n\n\nCode\nbusstop = st_read(dsn = \"./data/geospatial/BusStopLocation_Jul2023\",\n                   layer = \"BusStop\")  %&gt;% st_transform(crs = 3414) %&gt;% \n  distinct(BUS_STOP_N, .keep_all = TRUE)\n\n\nReading layer `BusStop' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex01/data/geospatial/BusStopLocation_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHere I found that there are several rows in ‘busstop’ data have duplicate BUS_STOP_N but slightly different geometry, so I used distinct to keep only one of those for doing intersection with hexagon.\n\n\n\nAlso import subzone geometry data as our background layer.\n\n\n\nCode\nsz = st_read(dsn = \"./data/geospatial\",\n                   layer = \"MPSZ-2019\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nhexagon, a hexagon layer of 250m (this distance is the perpendicular distance between the centre of the hexagon and its edges.) should be used to replace the relative coarse and irregular Master Plan 2019 Planning Sub-zone GIS data set of URA.\n\n\n\nCode\nhexagon &lt;- st_sf(geometry = st_make_grid(busstop, cellsize = c(250,250), what = \"polygons\",square = FALSE)) %&gt;%\n  mutate(id = row_number()) %&gt;% \n  st_transform(crs = 3414) \n\n\nThen we need to join bus stop with hexagon, and join with subzone to exclude hexagons out of range.\n\n\nCode\nbus_hex &lt;- st_join(\n  st_join(hexagon,busstop%&gt;%select(BUS_STOP_N,geometry), join = st_nearest_feature),\n  sz) %&gt;%\n  drop_na() %&gt;%\n  distinct(BUS_STOP_N, .keep_all = TRUE)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo avoid same geometry for multiple bus stop, here I choose to use st_nearest_feature as join method instead of intersection.\n\n\nCheck for duplicate geometry.\n\n\nCode\nbus_hex %&gt;%\n  group_by(geometry)%&gt;%\n  filter(row_number()&gt;1)\n\n\nSimple feature collection with 0 features and 8 fields\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 0 × 9\n# Groups:   geometry [0]\n# ℹ 9 variables: id &lt;int&gt;, BUS_STOP_N &lt;chr&gt;, SUBZONE_N &lt;chr&gt;, SUBZONE_C &lt;chr&gt;,\n#   PLN_AREA_N &lt;chr&gt;, PLN_AREA_C &lt;chr&gt;, REGION_N &lt;chr&gt;, REGION_C &lt;chr&gt;,\n#   geometry &lt;GEOMETRY [m]&gt;"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html#the-task",
    "href": "Takehome_Ex/Takehome_Ex01/Takehome_Ex01.html#the-task",
    "title": "Take Home Exercise 1",
    "section": "",
    "text": "The specific tasks of this take-home exercise are as follows:\n\n\n\nWith reference to the time intervals provided in the table below, compute the passenger trips generated by origin at the hexagon level,\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nDisplay the geographical distribution of the passenger trips by using appropriate geovisualisation methods,\nDescribe the spatial patterns revealed by the geovisualisation (not more than 200 words per visual).\n\n\n\nExtract peak data, and combine 4 time intervals data for further use.\n\n\nCode\npeak_trips &lt;- bind_rows(\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(interval = \"weekday_6_9\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 &\n           TIME_PER_HOUR &lt;= 20) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(interval = \"weekday_17_20\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 &\n           TIME_PER_HOUR &lt;= 14) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(interval = \"weekend_11_14\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS)) %&gt;%\n  mutate(interval = \"weekend_16_19\"))\n\nglimpse(peak_trips)\n\n\nRows: 20,044\nColumns: 3\n$ ORIGIN_PT_CODE &lt;fct&gt; 01012, 01013, 01019, 01029, 01039, 01059, 01109, 01112,…\n$ TRIPS          &lt;dbl&gt; 1973, 952, 1789, 2561, 2938, 1651, 161, 8492, 9015, 424…\n$ interval       &lt;chr&gt; \"weekday_6_9\", \"weekday_6_9\", \"weekday_6_9\", \"weekday_6…\n\n\n\n\n\nFirst combine passenger trip data with geospatial data.\n\n\nCode\norigin_trips &lt;- left_join(peak_trips, busstop,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(BUS_STOP_N = ORIGIN_PT_CODE) %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  summarise(TRIPS = sum(TRIPS))\n\n\nDuplication check before continue:\n\n\nCode\norigin_trips %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\n# A tibble: 0 × 2\n# ℹ 2 variables: BUS_STOP_N &lt;chr&gt;, TRIPS &lt;dbl&gt;\n\n\n\n\n\n4.1.3.1 Firstly, let’s check out the distribution of peak time trips of 4 time intervals in total.\nBelow code chunk aims at wrangling the peak time trips data for visualization.\n\n\nCode\npeaktrip_hex &lt;- left_join(bus_hex %&gt;% select(id, BUS_STOP_N, geometry), \n                          origin_trips, by = join_by(BUS_STOP_N)) \n\n\nNow we can visualize the distribution of total bus trips.\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\ntm_shape(peaktrip_hex) +\n  tm_fill(\"TRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Passenger trips\",\n          colorNA = NULL,\n          showNA = FALSE) +\n  tm_layout(main.title = \"Peak Time Passenger Trips\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_compass(type=\"4star\", size = 1.5) +\n  tm_borders(alpha = 0.5) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2)\n\n\n\n\n\n4.1.3.2 Then, to display the geographical distribution of 4 time intervals separately for comparison:\nRegenerate trip data with the “interval” column to indicate different time intervals.\n\n\nCode\npeak_trips_interval &lt;- left_join(peak_trips, busstop,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(BUS_STOP_N = ORIGIN_PT_CODE) %&gt;%\n  group_by(BUS_STOP_N, interval) %&gt;%\n  summarise(TRIPS = sum(TRIPS)) %&gt;% \n  mutate(daily_trips = \n           ifelse(grepl(\"weekday\",interval), ceiling(TRIPS/22), ceiling(TRIPS/9)))\n\nglimpse(peak_trips_interval)\n\n\nRows: 20,044\nColumns: 4\nGroups: BUS_STOP_N [5,067]\n$ BUS_STOP_N  &lt;chr&gt; \"01012\", \"01012\", \"01012\", \"01012\", \"01013\", \"01013\", \"010…\n$ interval    &lt;chr&gt; \"weekday_17_20\", \"weekday_6_9\", \"weekend_11_14\", \"weekend_…\n$ TRIPS       &lt;dbl&gt; 8448, 1973, 2273, 3208, 7328, 952, 1697, 2796, 3608, 1789,…\n$ daily_trips &lt;dbl&gt; 384, 90, 253, 357, 334, 44, 189, 311, 164, 82, 168, 181, 4…\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHere I create a column “daily_trips” to find number of trips per day, since it’s hard to compare the absolute number when we are differentiating weekday and weekend peak times intervals. For Aug 2023, there are 31 days in which 8 days are weekends and 1 day is National holiday.\n\n\nTo draw polygons, I will use pivot_wider to pivot interval into columns.\n\n\nCode\npeak_dailytrips_interval &lt;- peak_trips_interval %&gt;%\n  select(BUS_STOP_N,interval,daily_trips) %&gt;%\n  pivot_wider(names_from = interval, \n              values_from = daily_trips, \n              values_fill = NA)\n\nhead(peak_dailytrips_interval, 5)\n\n\n# A tibble: 5 × 5\n# Groups:   BUS_STOP_N [5]\n  BUS_STOP_N weekday_17_20 weekday_6_9 weekend_11_14 weekend_16_19\n  &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;\n1 01012                384          90           253           357\n2 01013                334          44           189           311\n3 01019                164          82           168           181\n4 01029                424         117           364           472\n5 01039                589         134           603           823\n\n\nJoin back with hexagon before visualization.\n\n\nCode\ninterval_dailytrip_hex &lt;- left_join(bus_hex, peak_dailytrips_interval,\n                           by = join_by(BUS_STOP_N)) \n\n\nThen we can visualize daily trip distribution for each time intervals.\n\n\nCode\ntmap_mode(\"plot\")\ntm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\ntm_shape(interval_dailytrip_hex)+ \n  tm_polygons(c(\"weekday_6_9\",\"weekday_17_20\",\"weekend_11_14\",\"weekend_16_19\"),\n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"\",\n          colorNA = NULL,\n          showNA = FALSE) + \n  tm_layout(legend.position = c(\"right\", \"bottom\"),\n    panel.show = TRUE,\n    panel.labels = c(\"Weekday 6-9am\", \"Weekday 5-8pm\", \"Weekend 11am-2pm\", \"Weekend 4-7pm\"))\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation:\n\nFrom the scale bar we can find that on daily bases, the number of bus trips at evening peak time intervals is larger than morning peak time intervals.\nThe locations with heavy number of trips are very similar during all peak time intervals.\n\n\n\n\n\n\n\n\nCompute LISA of the passengers trips generate by origin at hexagon level.\nDisplay the LISA maps of the passengers trips generate by origin at hexagon level. The maps should only display the significant (i.e. p-value &lt; 0.05)\nWith reference to the analysis results, draw statistical conclusions (not more than 200 words per visual).\n\n\n\nFirst we need to derive contiguity weights use knn method.\n\n\nCode\nweight_q &lt;-  peaktrip_hex%&gt;%\n  mutate(nb = st_knn(geometry,k=3),\n         wt = st_inverse_distance(nb, geometry),\n         .before = 1) %&gt;%\n  mutate(TRIPS = replace_na(TRIPS,0))\n\nweight_q\n\n\nSimple feature collection with 4044 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 3720.122 ymin: 26337.76 xmax: 48220.12 ymax: 50225.63\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n           nb                                 wt  id BUS_STOP_N TRIPS\n1   2, 11, 20 0.04618802, 0.06099943, 0.06575959   1      25059   549\n2  11, 12, 14 0.11094004, 0.11094004, 0.08728716   6      25751   210\n3    5, 7, 18    0.2000000, 0.1333333, 0.1109400  73      26379   374\n4   7, 12, 14    0.4000000, 0.2000000, 0.2309401 195      25719  4555\n5    3, 8, 18    0.2000000, 0.1511858, 0.1511858 198      26369   403\n6   9, 10, 15    0.4000000, 0.1511858, 0.1333333 202      26279   722\n7   4, 14, 17    0.4000000, 0.2000000, 0.1511858 257      26389   780\n8   5, 15, 23    0.1511858, 0.2000000, 0.1109400 261      26299   499\n9   6, 10, 13    0.4000000, 0.2309401, 0.1511858 264      26261 15682\n10   6, 9, 13    0.1511858, 0.2309401, 0.4000000 265      26251  2419\n                         geometry\n1  POLYGON ((3845.122 26554.27...\n2  POLYGON ((3845.122 28719.33...\n3  POLYGON ((3970.122 30667.89...\n4  POLYGON ((4220.122 29801.86...\n5  POLYGON ((4220.122 31100.9,...\n6  POLYGON ((4220.122 32832.95...\n7  POLYGON ((4345.122 30018.37...\n8  POLYGON ((4345.122 31750.42...\n9  POLYGON ((4345.122 33049.46...\n10 POLYGON ((4345.122 33482.47...\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nTo treat those area with sparse location of bus stops (0 neighbor case), I used st_knn instead of st_contiguity, since our bus stops’ geometry are not necessarily adjacent to each other.\nHence since there are lagged value to zone without neighbors, I set allow_zero to True.\n\n\n\nBefore continue, we can perform a Global Moran’I permutation test.\n\n\nCode\nset.seed(1234)\nglobal_moran_perm(weight_q$TRIPS,\n                  weight_q$nb,\n                  weight_q$wt,\n                  adjust.n = TRUE,\n                  nsim = 99)\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.10933, observed rank = 100, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation:\nThe Moran’I statistic (0.10933) indicates a slightly positive spatial autocorrelation, suggesting that similar values tend to be clustered together in our map, so we can continue with LISA.\n\n\nThen we can compute LISA of passenger trips during peak time hours.\n\n\nCode\nlisa &lt;- weight_q %&gt;% \n  mutate(local_moran = local_moran(\n    TRIPS, nb, wt, nsim = 199),\n         .before = 1) %&gt;%\n  unnest(local_moran)\n\nglimpse(lisa)\n\n\nRows: 4,044\nColumns: 18\n$ ii           &lt;dbl&gt; 0.01683533, 0.03658211, 0.05060816, 0.06595975, 0.0565997…\n$ eii          &lt;dbl&gt; 0.0023890846, 0.0011069140, -0.0020652686, -0.0074375318,…\n$ var_ii       &lt;dbl&gt; 0.0004110818, 0.0018452585, 0.0075471964, 0.0087109926, 0…\n$ z_ii         &lt;dbl&gt; 0.7125097, 0.8258406, 0.6063156, 0.7864048, 0.7386234, 0.…\n$ p_ii         &lt;dbl&gt; 0.4761492, 0.4088945, 0.5443052, 0.4316304, 0.4601357, 0.…\n$ p_ii_sim     &lt;dbl&gt; 0.21, 0.07, 0.13, 0.04, 0.09, 0.82, 0.28, 0.03, 0.11, 0.5…\n$ p_folded_sim &lt;dbl&gt; 0.105, 0.035, 0.065, 0.020, 0.045, 0.410, 0.140, 0.015, 0…\n$ skewness     &lt;dbl&gt; -3.920333, -4.000057, -6.940666, -4.857223, -4.134682, -5…\n$ kurtosis     &lt;dbl&gt; 22.623553, 22.981888, 63.967522, 32.243401, 22.447469, 49…\n$ mean         &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ median       &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ pysal        &lt;fct&gt; Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low-Low, Low…\n$ nb           &lt;nb&gt; &lt;2, 11, 20&gt;, &lt;11, 12, 14&gt;, &lt;5, 7, 18&gt;, &lt;7, 12, 14&gt;, &lt;3, 8,…\n$ wt           &lt;list&gt; &lt;0.04618802, 0.06099943, 0.06575959&gt;, &lt;0.11094004, 0.110…\n$ id           &lt;int&gt; 1, 6, 73, 195, 198, 202, 257, 261, 264, 265, 315, 318, 32…\n$ BUS_STOP_N   &lt;chr&gt; \"25059\", \"25751\", \"26379\", \"25719\", \"26369\", \"26279\", \"26…\n$ TRIPS        &lt;dbl&gt; 549, 210, 374, 4555, 403, 722, 780, 499, 15682, 2419, 241…\n$ geometry     &lt;POLYGON [m]&gt; POLYGON ((3845.122 26554.27..., POLYGON ((3845.12…\n\n\n\n\n\nFirst of all, let’s check out local Moran’s I and p-value.\n\n\nCode\ntmap_mode(\"plot\")\n\nmap1 &lt;- \n  tm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\n  tm_shape(lisa) +\n  tm_fill(\"ii\") + \n  tm_borders(alpha = 0.5) +\n  tm_view(set.zoom.limits = c(6,8)) +\n  tm_layout(main.title = \"local Moran's I of Peak Time Trips\",\n            main.title.size = 0.8)\n\nmap2 &lt;- \n  tm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\n  tm_shape(lisa) +\n  tm_fill(\"p_ii_sim\",\n          palette = \"-Reds\",\n          breaks = c(0, 0.001, 0.01, 0.05, 1),\n              labels = c(\"0.001\", \"0.01\", \"0.05\", \"Not sig\")) + \n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 0.8)\n\ntmap_arrange(map1, map2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHere I reverse the color palette of p-value, to put more emphasize on significant spots (which with lower p-value).\n\n\nThen we can display only the significant (p-value &lt; 0.05) part on map.\n\n\nCode\nlisa_sig &lt;- lisa  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\ntmap_mode(\"plot\")\ntm_shape(sz) +\n  tm_polygons(alpha = 0.3) +\n  tm_borders(alpha = 0.2) +\ntm_shape(lisa) +\n  tm_polygons(alpha = 0.1) +\n  tm_borders(alpha = 0.1) +\ntm_shape(lisa_sig) +\n  tm_fill(\"mean\") + \n  tm_borders(alpha = 0.8)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservation:\n\nHot spots mostly concentrate in those busy bus stops and cold spots mostly in border area.\nThe location of those hot/cold spots go along with the density of passenger trip numbers.\nThere’s seldom High-Low or Low-High spots which would be spatial outliers."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html",
    "title": "Hands on Exercise 1",
    "section": "",
    "text": "Data are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. In this hands-on exercise, you are required to extract the necessary data sets from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe purpose of this section is not merely extracting the necessary data sets. It also aims to introduce you to public available data sets. Students are encouraged to explore the rest of the available data sets in these three data sources.\n\n\n\n\n\nNext, at the Hands-on_Ex01 folder, create a sub-folder called data. Then, inside the data sub-folder, create two sub-folders and name them geospatial and aspatial respectively.\nPlace Master Plan 2014 Subzone Boundary (Web), Pre-Schools Location and Cycling Path zipped files into geospatial sub-folder and unzipped them. Copy the unzipped files from their respective sub-folders and place them inside geospatial sub-folder.\n\n\n\nNow, you will extract the downloaded listing data file. At Downloads folder, cut and paste listing.csv into aspatial sub-folder.\n\n\n\n\nIn this hands-on exercise, two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nType the following code chunk.\n\n\nCode\npacman::p_load(sf, tidyverse)\n\n\nThen, import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\n\nCode\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe message above reveals that the geospatial objects are multipolygon features. There are a total of 323 multipolygon features and 15 fields in mpsz simple feature data frame. mpsz is in svy21 projected coordinates systems. The bounding box provides the x extend and y extend of the data.\n\n\n\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\n\nCode\ncyclingpath = st_read(dsn = \"data/geospatial/CyclingPath_Jul2023\", \n                         layer = \"CyclingPathGazette\")\n\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex01/data/geospatial/CyclingPath_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe message above reveals that there are a total of 2558 features and 2 fields in cyclingpath linestring feature data frame and it is in svy21 projected coordinates system too.\n\n\n\nThe pre-schools-location-kml is in kml format. The code chunk below will be used to import the kml into R. Notice that in the code chunk below, the complete path and the kml file extension were provided.\n\n\nCode\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex01/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that preschool is a point feature data frame. There are a total of 1359 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system.\n\n\n\n\n\n\nThe column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\n\nCode\nst_geometry(mpsz)\n\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\n\n\n\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\n\nCode\nglimpse(mpsz)\n\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\n\nCode\nhead(mpsz, n=5)\n\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n\n\n\nIn geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. This is the time you will find plot() of R Graphic comes in very handy as shown in the code chunk below.\n\n\nCode\nplot(mpsz)\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\n\nCode\nplot(st_geometry(mpsz))\n\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\n\nCode\nplot(mpsz[\"PLN_AREA_N\"])           \n\n\n\n\n\n\n\n\nMap projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, you will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\n\nCode\nst_crs(mpsz)       \n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\n\nCode\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\n\nNow, let us check the CSR again by using the code chunk below.\n\n\nCode\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\n\nCode\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems.\n\n\n\n\nIn practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points.\nIn this section, you will learn how to import an aspatial data into R environment and save it as a tibble data frame. Next, you will convert it into a simple feature data frame.\nFor the purpose of this exercise, the listings.csv data downloaded from AirBnb will be used.\n\n\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\n\nCode\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\n\nCode\nlist(listings) \n\n\n[[1]]\n# A tibble: 3,483 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,473 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\n\nCode\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nLet us examine the content of this newly created simple feature data frame.\n\n\nCode\nglimpse(listings_sf)\n\n\nRows: 3,483\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 55, 69, 220, 85, 75, 45, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 20, 24, 47, 22, 17, 12, 133, 18, 6, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.14, 0.16, 0.31, 0.17, 0.12, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 52, 52, 5, 7, 52, 52, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 89, 89, 89, 275, 274, 89, 365, 365, 365…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 3, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame.\n\n\n\n\nBesides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths.\n\n\nCode\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\n\nCode\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\n\nLastly, sum() of Base R will be used to derive the total land involved\n\n\nCode\nsum(buffer_cycling$AREA)\n\n\n1774367 [m^2]\n\n\n\n\n\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\n\nCode\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou should not confuse with st_intersection().\n\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\n\nCode\nsummary(mpsz3414$`PreSch Count`)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\n\nCode\ntop_n(mpsz3414, 1, `PreSch Count`)\n\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nThe solution:\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\n\nCode\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\n\nCode\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\n\n\n\n\n\nIn practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\n\nCode\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\n\nCode\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nUsing ggplot2 method, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\n\nCode\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#data-acquisition",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#data-acquisition",
    "title": "Hands on Exercise 1",
    "section": "",
    "text": "Data are key to data analytics including geospatial analytics. Hence, before analysing, we need to assemble the necessary data. In this hands-on exercise, you are required to extract the necessary data sets from the following sources:\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe purpose of this section is not merely extracting the necessary data sets. It also aims to introduce you to public available data sets. Students are encouraged to explore the rest of the available data sets in these three data sources.\n\n\n\n\n\nNext, at the Hands-on_Ex01 folder, create a sub-folder called data. Then, inside the data sub-folder, create two sub-folders and name them geospatial and aspatial respectively.\nPlace Master Plan 2014 Subzone Boundary (Web), Pre-Schools Location and Cycling Path zipped files into geospatial sub-folder and unzipped them. Copy the unzipped files from their respective sub-folders and place them inside geospatial sub-folder.\n\n\n\nNow, you will extract the downloaded listing data file. At Downloads folder, cut and paste listing.csv into aspatial sub-folder."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#importing-the-data",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#importing-the-data",
    "title": "Hands on Exercise 1",
    "section": "",
    "text": "In this hands-on exercise, two R packages will be used. They are:\n\nsf for importing, managing, and processing geospatial data, and\ntidyverse for performing data science tasks such as importing, wrangling and visualising data.\n\nTidyverse consists of a family of R packages. In this hands-on exercise, the following packages will be used:\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data, and\nggplot2 for visualising data\n\nType the following code chunk.\n\n\nCode\npacman::p_load(sf, tidyverse)\n\n\nThen, import the following geospatial data into R by using st_read() of sf package:\n\nMP14_SUBZONE_WEB_PL, a polygon feature layer in ESRI shapefile format,\nCyclingPath, a line feature layer in ESRI shapefile format, and\nPreSchool, a point feature layer in kml file format.\n\n\n\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame. Note that when the input geospatial data is in shapefile format, two arguments will be used, namely: dsn to define the data path and layer to provide the shapefile name. Also note that no extension such as .shp, .dbf, .prj and .shx are needed.\n\n\nCode\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThe message above reveals that the geospatial objects are multipolygon features. There are a total of 323 multipolygon features and 15 fields in mpsz simple feature data frame. mpsz is in svy21 projected coordinates systems. The bounding box provides the x extend and y extend of the data.\n\n\n\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\n\nCode\ncyclingpath = st_read(dsn = \"data/geospatial/CyclingPath_Jul2023\", \n                         layer = \"CyclingPathGazette\")\n\n\nReading layer `CyclingPathGazette' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex01/data/geospatial/CyclingPath_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 2558 features and 2 fields\nGeometry type: MULTILINESTRING\nDimension:     XY\nBounding box:  xmin: 11854.32 ymin: 28347.98 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe message above reveals that there are a total of 2558 features and 2 fields in cyclingpath linestring feature data frame and it is in svy21 projected coordinates system too.\n\n\n\nThe pre-schools-location-kml is in kml format. The code chunk below will be used to import the kml into R. Notice that in the code chunk below, the complete path and the kml file extension were provided.\n\n\nCode\npreschool = st_read(\"data/geospatial/PreSchoolsLocation.kml\")\n\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex01/data/geospatial/PreSchoolsLocation.kml' \n  using driver `KML'\nSimple feature collection with 2290 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6878 ymin: 1.247759 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that preschool is a point feature data frame. There are a total of 1359 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#checking-the-content-of-a-simple-feature-data-frame",
    "title": "Hands on Exercise 1",
    "section": "",
    "text": "The column in the sf data.frame that contains the geometries is a list, of class sfc. We can retrieve the geometry list-column in this case by mpsz$geom or mpsz[[1]], but the more general way uses st_geometry() as shown in the code chunk below.\n\n\nCode\nst_geometry(mpsz)\n\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\n\n\n\nBeside the basic feature information, we also would like to learn more about the associated attribute information in the data frame. This is the time you will find glimpse() of dplyr. very handy as shown in the code chunk below.\n\n\nCode\nglimpse(mpsz)\n\n\nRows: 323\nColumns: 16\n$ OBJECTID   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO &lt;int&gt; 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  &lt;chr&gt; \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  &lt;chr&gt; \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N &lt;chr&gt; \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C &lt;chr&gt; \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   &lt;chr&gt; \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   &lt;chr&gt; \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    &lt;chr&gt; \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D &lt;date&gt; 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     &lt;dbl&gt; 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     &lt;dbl&gt; 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng &lt;dbl&gt; 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area &lt;dbl&gt; 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nglimpse() report reveals the data type of each fields. For example FMEL-UPD_D field is in date data type and X_ADDR, Y_ADDR, SHAPE_L and SHAPE_AREA fields are all in double-precision values.\n\n\n\nSometimes we would like to reveal complete information of a feature object, this is the job of head() of Base R\n\n\nCode\nhead(mpsz, n=5)\n\n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#plotting-the-geospatial-data",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#plotting-the-geospatial-data",
    "title": "Hands on Exercise 1",
    "section": "",
    "text": "In geospatial data science, by looking at the feature information is not enough. We are also interested to visualise the geospatial features. This is the time you will find plot() of R Graphic comes in very handy as shown in the code chunk below.\n\n\nCode\nplot(mpsz)\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above. We can, however, choose to plot only the geometry by using the code chunk below.\n\n\nCode\nplot(st_geometry(mpsz))\n\n\n\n\n\nAlternatively, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\n\nCode\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#working-with-projection",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#working-with-projection",
    "title": "Hands on Exercise 1",
    "section": "",
    "text": "Map projection is an important property of a geospatial data. In order to perform geoprocessing using two geospatial data, we need to ensure that both geospatial data are projected using similar coordinate system.\nIn this section, you will learn how to project a simple feature data frame from one coordinate system to another coordinate system. The technical term of this process is called projection transformation.\n\n\nOne of the common issue that can happen during importing geospatial data into R is that the coordinate system of the source data was either missing (such as due to missing .proj for ESRI shapefile) or wrongly assigned during the importing process.\nThis is an example the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below.\n\n\nCode\nst_crs(mpsz)       \n\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\n\nAlthough mpsz data frame is projected in svy21 but when we read until the end of the print, it indicates that the EPSG is 9001. This is a wrong EPSG code because the correct EPSG code for svy21 should be 3414.\nIn order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\n\nCode\nmpsz3414 &lt;- st_set_crs(mpsz, 3414)\n\n\nNow, let us check the CSR again by using the code chunk below.\n\n\nCode\nst_crs(mpsz3414)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n\nIn geospatial analytics, it is very common for us to transform the original data from geographic coordinate system to projected coordinate system. This is because geographic coordinate system is not appropriate if the analysis need to use distance or/and area measurements.\nLet us take preschool simple feature data frame as an example. The print below reveals that it is in wgs84 coordinate system.\nThis is a scenario that st_set_crs() is not appropriate and st_transform() of sf package should be used. This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\nLet us perform the projection transformation by using the code chunk below.\n\n\nCode\npreschool3414 &lt;- st_transform(preschool, \n                              crs = 3414)\n\n\nNext, let us display the content of preschool3414 sf data frame as shown below.\nNotice that it is in svy21 projected coordinate system now. Furthermore, if you refer to Bounding box:, the values are greater than 0-360 range of decimal degree commonly used by most of the geographic coordinate systems."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#importing-and-converting-an-aspatial-data",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#importing-and-converting-an-aspatial-data",
    "title": "Hands on Exercise 1",
    "section": "",
    "text": "In practice, it is not unusual that we will come across data such as listing of Inside Airbnb. We call this kind of data aspatial data. This is because it is not a geospatial data but among the data fields, there are two fields that capture the x- and y-coordinates of the data points.\nIn this section, you will learn how to import an aspatial data into R environment and save it as a tibble data frame. Next, you will convert it into a simple feature data frame.\nFor the purpose of this exercise, the listings.csv data downloaded from AirBnb will be used.\n\n\nSince listings data set is in csv file format, we will use read_csv() of readr package to import listing.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\n\nCode\nlistings &lt;- read_csv(\"data/aspatial/listings.csv\")\n\n\nAfter importing the data file into R, it is important for us to examine if the data file has been imported correctly.\nThe code chunk below shows list() of Base R instead of glimpse() is used to do the job.\n\n\nCode\nlist(listings) \n\n\n[[1]]\n# A tibble: 3,483 × 18\n       id name      host_id host_name neighbourhood_group neighbourhood latitude\n    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n 1  71609 Villa in…  367042 Belinda   East Region         Tampines          1.35\n 2  71896 Home in …  367042 Belinda   East Region         Tampines          1.35\n 3  71903 Home in …  367042 Belinda   East Region         Tampines          1.35\n 4 275343 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 5 275344 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 6 289234 Home in …  367042 Belinda   East Region         Tampines          1.34\n 7 294281 Rental u… 1521514 Elizabeth Central Region      Newton            1.31\n 8 324945 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n 9 330095 Rental u… 1439258 Kay       Central Region      Bukit Merah       1.29\n10 369141 Place to… 1521514 Elizabeth Central Region      Newton            1.31\n# ℹ 3,473 more rows\n# ℹ 11 more variables: longitude &lt;dbl&gt;, room_type &lt;chr&gt;, price &lt;dbl&gt;,\n#   minimum_nights &lt;dbl&gt;, number_of_reviews &lt;dbl&gt;, last_review &lt;date&gt;,\n#   reviews_per_month &lt;dbl&gt;, calculated_host_listings_count &lt;dbl&gt;,\n#   availability_365 &lt;dbl&gt;, number_of_reviews_ltm &lt;dbl&gt;, license &lt;chr&gt;\n\n\nThe output reveals that listing tibble data frame consists of 4252 rows and 16 columns. Two useful fields we are going to use in the next phase are latitude and longitude. Note that they are in decimal degree format. As a best guess, we will assume that the data is in wgs84 Geographic Coordinate System.\n\n\n\nThe code chunk below converts listing data frame into a simple feature data frame by using st_as_sf() of sf packages\n\n\nCode\nlistings_sf &lt;- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %&gt;%\n  st_transform(crs = 3414)\n\n\nThings to learn from the arguments above:\n\ncoords argument requires you to provide the column name of the x-coordinates first then followed by the column name of the y-coordinates.\ncrs argument requires you to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System and EPSG: 3414 is Singapore SVY21 Projected Coordinate System. You can search for other country’s epsg code by referring to epsg.io.\n%&gt;% is used to nest st_transform() to transform the newly created simple feature data frame into svy21 projected coordinates system.\n\nLet us examine the content of this newly created simple feature data frame.\n\n\nCode\nglimpse(listings_sf)\n\n\nRows: 3,483\nColumns: 17\n$ id                             &lt;dbl&gt; 71609, 71896, 71903, 275343, 275344, 28…\n$ name                           &lt;chr&gt; \"Villa in Singapore · ★4.44 · 2 bedroom…\n$ host_id                        &lt;dbl&gt; 367042, 367042, 367042, 1439258, 143925…\n$ host_name                      &lt;chr&gt; \"Belinda\", \"Belinda\", \"Belinda\", \"Kay\",…\n$ neighbourhood_group            &lt;chr&gt; \"East Region\", \"East Region\", \"East Reg…\n$ neighbourhood                  &lt;chr&gt; \"Tampines\", \"Tampines\", \"Tampines\", \"Bu…\n$ room_type                      &lt;chr&gt; \"Private room\", \"Private room\", \"Privat…\n$ price                          &lt;dbl&gt; 150, 80, 80, 55, 69, 220, 85, 75, 45, 7…\n$ minimum_nights                 &lt;dbl&gt; 92, 92, 92, 60, 60, 92, 92, 60, 60, 92,…\n$ number_of_reviews              &lt;dbl&gt; 20, 24, 47, 22, 17, 12, 133, 18, 6, 81,…\n$ last_review                    &lt;date&gt; 2020-01-17, 2019-10-13, 2020-01-09, 20…\n$ reviews_per_month              &lt;dbl&gt; 0.14, 0.16, 0.31, 0.17, 0.12, 0.09, 0.9…\n$ calculated_host_listings_count &lt;dbl&gt; 5, 5, 5, 52, 52, 5, 7, 52, 52, 7, 7, 1,…\n$ availability_365               &lt;dbl&gt; 89, 89, 89, 275, 274, 89, 365, 365, 365…\n$ number_of_reviews_ltm          &lt;dbl&gt; 0, 0, 0, 0, 3, 0, 0, 1, 3, 0, 0, 0, 0, …\n$ license                        &lt;chr&gt; NA, NA, NA, \"S0399\", \"S0399\", NA, NA, \"…\n$ geometry                       &lt;POINT [m]&gt; POINT (41972.5 36390.05), POINT (…\n\n\nTable above shows the content of listing_sf. Notice that a new column called geometry has been added into the data frame. On the other hand, the longitude and latitude columns have been dropped from the data frame."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#geoprocessing-with-sf-package",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#geoprocessing-with-sf-package",
    "title": "Hands on Exercise 1",
    "section": "",
    "text": "Besides providing functions to handling (i.e. importing, exporting, assigning projection, transforming projection etc) geospatial data, sf package also offers a wide range of geoprocessing (also known as GIS analysis) functions.\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n\nThe scenario:\nThe authority is planning to upgrade the exiting cycling path. To do so, they need to acquire 5 metres of reserved land on the both sides of the current cycling path. You are tasked to determine the extend of the land need to be acquired and their total area.\nThe solution:\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths.\n\n\nCode\nbuffer_cycling &lt;- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\n\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\n\n\nCode\nbuffer_cycling$AREA &lt;- st_area(buffer_cycling)\n\n\nLastly, sum() of Base R will be used to derive the total land involved\n\n\nCode\nsum(buffer_cycling$AREA)\n\n\n1774367 [m^2]\n\n\n\n\n\nThe scenario:\nA pre-school service group want to find out the numbers of pre-schools in each Planning Subzone.\nThe solution:\nThe code chunk below performs two operations at one go. Firstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() of Base R is used to calculate numbers of pre-schools that fall inside each planning subzone.\n\n\nCode\nmpsz3414$`PreSch Count`&lt;- lengths(st_intersects(mpsz3414, preschool3414))\n\n\n\n\n\n\n\n\nWarning\n\n\n\nYou should not confuse with st_intersection().\n\n\nYou can check the summary statistics of the newly derived PreSch Count field by using summary() as shown in the code chunk below.\n\n\nCode\nsummary(mpsz3414$`PreSch Count`)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    4.00    7.09   10.00   72.00 \n\n\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\n\nCode\ntop_n(mpsz3414, 1, `PreSch Count`)\n\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 39655.33 ymin: 35966 xmax: 42940.57 ymax: 38622.37\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO     SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      189          2 TAMPINES EAST    TMSZ02      N   TAMPINES         TM\n     REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR SHAPE_Leng\n1 EAST REGION       ER 21658EAAF84F4D8D 2014-12-05 41122.55 37392.39   10180.62\n  SHAPE_Area                       geometry PreSch Count\n1    4339824 MULTIPOLYGON (((42196.76 38...           72\n\n\nThe solution:\nFirstly, the code chunk below uses st_area() of sf package to derive the area of each planning subzone.\n\n\nCode\nmpsz3414$Area &lt;- mpsz3414 %&gt;%\n  st_area()\n\n\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\n\nCode\nmpsz3414 &lt;- mpsz3414 %&gt;%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#exploratory-data-analysis-eda",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#exploratory-data-analysis-eda",
    "title": "Hands on Exercise 1",
    "section": "",
    "text": "In practice, many geospatial analytics start with Exploratory Data Analysis. In this section, you will learn how to use appropriate ggplot2 functions to create functional and yet truthful statistical graphs for EDA purposes.\nFirstly, we will plot a histogram to reveal the distribution of PreSch Density. Conventionally, hist() of R Graphics will be used as shown in the code chunk below.\n\n\nCode\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\n\nAlthough the syntax is very easy to use however the output is far from meeting publication quality. Furthermore, the function has limited room for further customisation.\nIn the code chunk below, appropriate ggplot2 functions will be used.\n\n\nCode\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nUsing ggplot2 method, plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\n\nCode\nggplot(data=mpsz3414, \n       aes(y = `PreSch Count`, \n           x= as.numeric(`PreSch Density`)))+\n  geom_point(color=\"black\", \n             fill=\"light blue\") +\n  xlim(0, 40) +\n  ylim(0, 40) +\n  labs(title = \"\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school count\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#importing-data-into-r",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#importing-data-into-r",
    "title": "Hands on Exercise 1",
    "section": "1 Importing Data into R",
    "text": "1 Importing Data into R\nIn this hands-on exercise, the key R package use is tmap package in R. Beside tmap package, four other R packages will be used. They are:\n\nreadr for importing delimited text file,\ntidyr for tidying data,\ndplyr for wrangling data and\nsf for handling geospatial data.\n\nAmong the four packages, readr, tidyr and dplyr are part of tidyverse package.\nThe code chunk below will be used to install and load these packages in RStudio.\n\n\nCode\npacman::p_load(sf, tmap, tidyverse)\n\n\n\n1.1 The Data\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\n\n\n1.2 Importing Geospatial Data into R\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz.\n\n\nCode\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nYou can examine the content of mpsz by using the code chunk below.\n\n\nCode\nmpsz\n\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\n\n\n1.3 Importing Attribute Data into R\nNext, we will import respopagsextod2011to2022.csv file into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\n\nCode\npopdata &lt;- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\n\n\n1.4 Data Preparation\nBefore a thematic map can be prepared, you are required to prepare a data table with year 2020 values. The data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\n\n\n1.4.1 Data wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\n\nCode\npopdata2020 &lt;- popdata %&gt;%\n  filter(Time == 2020) %&gt;%\n  group_by(PA, SZ, AG) %&gt;%\n  summarise(`POP` = sum(`Pop`)) %&gt;%\n  ungroup()%&gt;%\n  pivot_wider(names_from=AG, \n              values_from=POP) %&gt;%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %&gt;%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%&gt;%\nmutate(`AGED`=rowSums(.[16:21])) %&gt;%\nmutate(`TOTAL`=rowSums(.[3:21])) %&gt;%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %&gt;%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\n\n\n1.4.2 Joining the attribute data and geospatial data\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\n\n\nCode\npopdata2020 &lt;- popdata2020 %&gt;%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %&gt;%\n  filter(`ECONOMY ACTIVE` &gt; 0)\n\n\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\n\nCode\nmpsz_pop2020 &lt;- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\n\n\n\nCode\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#choropleth-mapping-geospatial-data-using-tmap",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#choropleth-mapping-geospatial-data-using-tmap",
    "title": "Hands on Exercise 1",
    "section": "2 Choropleth Mapping Geospatial Data Using tmap",
    "text": "2 Choropleth Mapping Geospatial Data Using tmap\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n2.1 Plotting a choropleth map quickly by using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It is concise and provides a good default visualisation in many cases.\nThe code chunk below will draw a cartographic standard choropleth map as shown below.\n\n\nCode\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\nThings to learn from the code chunk above:\n\ntmap_mode() with “plot” option is used to produce a static map. For interactive mode, “view” option should be used.\nfill argument is used to map the attribute (i.e. DEPENDENCY)\n\n\n\n2.2 Creating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n2.2.1 Drawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\n\nCode\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n\n2.2.2 Drawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\nThings to learn from tm_polygons():\n\nThe default interval binning used to draw the choropleth map is called “pretty”. A detailed discussion of the data classification methods supported by tmap will be provided in sub-section 4.3.\nThe default colour scheme used is YlOrRd of ColorBrewer. You will learn more about the color scheme in sub-section 4.4.\nBy default, Missing value will be shaded in grey.\n\n\n\n2.2.3 Drawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nNotice that the planning subzones are shared according to the respective dependecy values\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\nNotice that light-gray border lines have been added on the choropleth map.\nThe alpha argument is used to define transparency number between 0 (totally transparent) and 1 (not transparent). By default, the alpha value of the col is used (normally 1).\nBeside alpha argument, there are three other arguments for tm_borders(), they are:\n\ncol = border colour,\nlwd = border line width. The default is 1, and\nlty = border line type. The default is “solid”.\n\n\n\n\n2.3 Data classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n2.3.1 Plotting choropleth maps with built-in classification methods\nThe code chunk below shows a quantile data classification that used 5 classes.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nIn the code chunk below, equal data classification method is used.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nNotice that the distribution of quantile data classification method are more evenly distributed then equal data classification method.\nDIY: Using what you had learned, prepare choropleth maps by using different classification methods supported by tmap and compare their differences.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"DEPENDENCY\",\"DEPENDENCY\"),\n          style = c(\"kmeans\",\"sd\"), \n          n = 5) +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\nDIY: Preparing choropleth maps by using similar classification method but with different numbers of classes (i.e. 2, 6, 10, 20). Compare the output maps, what observation can you draw?\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"DEPENDENCY\",\"DEPENDENCY\"),\n          style = \"kmeans\", \n          n = c(2,10),\n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\n\n\n2.3.2 Plotting choropleth map with custome break\nFor all the built-in styles, the category breaks are computed internally. In order to override these defaults, the breakpoints can be set explicitly by means of the breaks argument to the tm_fill(). It is important to note that, in tmap the breaks include a minimum and maximum. As a result, in order to end up with n categories, n+1 elements must be specified in the breaks option (the values must be in increasing order).\nBefore we get started, it is always a good practice to get some descriptive statistics on the variable before setting the break points. Code chunk below will be used to compute and display the descriptive statistics of DEPENDENCY field.\n\n\nCode\nsummary(mpsz_pop2020$DEPENDENCY)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1111  0.7147  0.7866  0.8585  0.8763 19.0000      92 \n\n\nWith reference to the results above, we set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100. Our breaks vector is thus c(0, 0.60, 0.70, 0.80, 0.90, 1.00)\nNow, we will plot the choropleth map by using the code chunk below.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n2.4 Colour Scheme\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n2.4.1 Using ColourBrewer palette\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nTo reverse the colour shading, add a “-” prefix.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n\n2.5 Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios. Colour settings and data classification methods covered in the previous section relate to the palette and break-points are used to affect how the map looks.\n\n1.2.5.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n2.5.2 Map style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\n\n\n2.5.3 Cartographic Furniture\nBeside map style, tmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nTo reset the default style, refer to the code chunk below.\n\n\nCode\ntmap_style(\"white\")\n\n\n\n\n\n2.6 Drawing Small Multiple Choropleth Maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically. Small multiple maps enable the visualisation of how spatial relationships change with respect to another variable, such as time.\nIn tmap, small multiple maps can be plotted in three ways:\n\nby assigning multiple values to at least one of the asthetic arguments,\nby defining a group-by variable in tm_facets(), and\nby creating multiple stand-alone maps with tmap_arrange().\n\n\n2.6.1 By assigning multiple values to at least one of the aesthetic arguments\nIn this example, small multiple choropleth maps are created by defining ncols in tm_fill().\n\n\nCode\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\n\n\nIn this example, small multiple choropleth maps are created by assigning multiple values to at least one of the aesthetic arguments.\n\n\nCode\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n2.6.2 By defining a group-by variable in tm_facets()\nIn this example, multiple small choropleth maps are created by using tm_facets().\n\n\nCode\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n2.6.3 By creating multiple stand-alone maps with tmap_arrange()\nIn this example, multiple small choropleth maps are created by creating multiple stand-alone maps with tmap_arrange().\n\n\nCode\nyoungmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap &lt;- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n2.7 Mappping Spatial Object Meeting a Selection Criterion\nInstead of creating small multiple choropleth map, you can also use selection funtion to map spatial objects meeting the selection criterion.\n\n\nCode\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#reference",
    "href": "Handson_Ex/Handson_Ex01/Handson_Ex01.html#reference",
    "title": "Hands on Exercise 1",
    "section": "3 Reference",
    "text": "3 Reference\n\n3.1 All about tmap package\n\ntmap: Thematic Maps in R\ntmap\ntmap: get started!\ntmap: changes in version 2.0\ntmap: creating thematic maps in a flexible way (useR!2015)\nExploring and presenting maps with tmap (useR!2017)\n\n\n\n3.2 Geospatial data wrangling\n\nsf: Simple Features for R\nSimple Features for R: StandardizedSupport for Spatial Vector Data\nReading, Writing and Converting Simple Features\n\n\n\n3.3 Data wrangling\n\ndplyr\nTidy data\ntidyr: Easily Tidy Data with ‘spread()’ and ‘gather()’ Functions"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/data/geospatial/MPSZ-2019.html",
    "href": "Handson_Ex/Handson_Ex04/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex03/Handson_Ex03.html",
    "href": "Handson_Ex/Handson_Ex03/Handson_Ex03.html",
    "title": "Hands on Excercise 3",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or a spatial interaction matrix.\nIn this hands-on exercise, you will learn how to build an OD matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall. By the end of this hands-on exercise, you will be able:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#overview",
    "href": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#overview",
    "title": "Hands on Excercise 3",
    "section": "",
    "text": "Spatial interaction represent the flow of people, material, or information between locations in geographical space. It encompasses everything from freight shipments, energy flows, and the global trade in rare antiquities, to flight schedules, rush hour woes, and pedestrian foot traffic.\nEach spatial interaction, as an analogy for a set of movements, is composed of a discrete origin/destination pair. Each pair can be represented as a cell in a matrix where rows are related to the locations (centroids) of origin, while columns are related to locations (centroids) of destination. Such a matrix is commonly known as an origin/destination matrix, or a spatial interaction matrix.\nIn this hands-on exercise, you will learn how to build an OD matrix by using Passenger Volume by Origin Destination Bus Stops data set downloaded from LTA DataMall. By the end of this hands-on exercise, you will be able:\n\nto import and extract OD data for a selected time interval,\nto import and save geospatial data (i.e. bus stops and mpsz) into sf tibble data frame objects,\nto populate planning subzone code into bus stops sf tibble data frame,\nto construct desire lines geospatial data from the OD data, and\nto visualise passenger volume by origin and destination bus stops by using the desire lines data."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#preparing-flow-data",
    "href": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#preparing-flow-data",
    "title": "Hands on Excercise 3",
    "section": "2 Preparing Flow Data",
    "text": "2 Preparing Flow Data\n\n2.1 Get ready\n\n\nCode\npacman::p_load(tmap, sf, DT, stplanr,\n               performance,\n               ggpubr, tidyverse)\n\n\n\n\n2.2 Importing the OD Data\nImport the passenger volume by origin destination bus stops data.\n\n\nCode\nodbus &lt;- read_csv(\"./data/aspatial/origin_destination_bus_202310.csv\") %&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),\n         DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))\n\n\nCheck the data.\n\n\nCode\nglimpse(odbus)\n\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 20281, 20281, 1…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;dbl&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\n\n\n2.3 Extracting Data\nFor the purpose of this exercise, we will extract commuting flows on weekday and between 6 and 9 o’clock.\n\n\nCode\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nSave the output in rds format\n\n\nCode\nwrite_rds(odbus6_9, \"./data/rds/odbus6_9.rds\")\n\n\nCan extract data from saved file again.\n\n\nCode\n# read from saved file.\n# odbus6_9 &lt;- read_rds(\"chap15/data/rds/odbus6_9.rds\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#working-with-geospatial-data",
    "href": "Handson_Ex/Handson_Ex03/Handson_Ex03.html#working-with-geospatial-data",
    "title": "Hands on Excercise 3",
    "section": "3 Working with GeoSpatial Data",
    "text": "3 Working with GeoSpatial Data\n\n3.1 Importing Data\nUse sf package to read master plan subzone data and bus stop location data.\n\n\nCode\nmpsz &lt;- st_read(dsn = \"./data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;% st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex03/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\nbusstop &lt;- st_read(dsn = \"./data/geospatial\",\n                   layer = \"BusStop\")  %&gt;% st_transform(crs = 3414)\n\n\nReading layer `BusStop' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex03/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nTip\n\n\n\nHere we use “st_transform(crs = 3414)” to change the coordinate from decimal degree to meters.\n\n\n\n\n3.2 Wrangling Data\nCombine the bus stop location with the Singapore subzone map.\n\n\nCode\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore boundary.\n\n\n\nCode\nglimpse(busstop_mpsz)\n\n\nRows: 5,156\nColumns: 2\n$ BUS_STOP_N &lt;chr&gt; \"13099\", \"13089\", \"06151\", \"13211\", \"13139\", \"13109\", \"1311…\n$ SUBZONE_C  &lt;chr&gt; \"RVSZ05\", \"RVSZ05\", \"SRSZ01\", \"SRSZ01\", \"SRSZ01\", \"SRSZ01\",…\n\n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus7_9 data frame.\n\n\nCode\nod_data &lt;- left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\n\nBefore continue, it is a good practice for us to check for duplicating records.\n\n\nCode\nod_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\n# A tibble: 1,186 × 4\n   ORIGIN_BS DESTIN_BS TRIPS ORIGIN_SZ\n   &lt;chr&gt;     &lt;fct&gt;     &lt;dbl&gt; &lt;chr&gt;    \n 1 11009     01341         1 QTSZ01   \n 2 11009     01341         1 QTSZ01   \n 3 11009     01411         4 QTSZ01   \n 4 11009     01411         4 QTSZ01   \n 5 11009     01421        17 QTSZ01   \n 6 11009     01421        17 QTSZ01   \n 7 11009     01511        19 QTSZ01   \n 8 11009     01511        19 QTSZ01   \n 9 11009     01521         2 QTSZ01   \n10 11009     01521         2 QTSZ01   \n# ℹ 1,176 more rows\n\n\nIf duplicated records are found, the code chunk below will be used to retain the unique records.\n\n\nCode\nod_data &lt;- unique(od_data)\n\n\nNext, we will update od_data data frame cwith the planning subzone codes.\n\n\nCode\nod_data &lt;- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) \n\n\n\n\nCode\nod_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\n# A tibble: 1,350 × 5\n   ORIGIN_BS DESTIN_BS TRIPS ORIGIN_SZ SUBZONE_C\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;    \n 1 01013     51071         2 RCSZ10    CCSZ01   \n 2 01013     51071         2 RCSZ10    CCSZ01   \n 3 01112     51071        66 RCSZ10    CCSZ01   \n 4 01112     51071        66 RCSZ10    CCSZ01   \n 5 01112     53041         4 RCSZ10    BSSZ01   \n 6 01112     53041         4 RCSZ10    BSSZ01   \n 7 01121     51071         8 RCSZ04    CCSZ01   \n 8 01121     51071         8 RCSZ04    CCSZ01   \n 9 01121     82221         1 RCSZ04    GLSZ05   \n10 01121     82221         1 RCSZ04    GLSZ05   \n# ℹ 1,340 more rows\n\n\n\n\nCode\nod_data &lt;- unique(od_data)\n\n\n\n\nCode\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\n\nIt is time to save the output into an rds file format.\n\n\nCode\nwrite_rds(od_data, \"./data/rds/od_data.rds\")\n\n\n\n\nCode\nod_data &lt;- read_rds(\"./data/rds/od_data.rds\")\n\n\n\n\n3.3 Visualising Spatial Interaction\nIn this section, you will learn how to prepare a desire line by using stplanr package.\n\n3.3.1 Removing intra-zonal flows\nWe will not plot the intra-zonal flows. The code chunk below will be used to remove intra-zonal flows.\n\n\nCode\nod_data1 &lt;- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]\n\n\n\n\n3.3.2 Creating desire lines\nIn this code chunk below, od2line() of stplanr package is used to create the desire lines.\n\n\nCode\nflowLine &lt;- od2line(flow = od_data1, \n                    zones = mpsz,\n                    zone_code = \"SUBZONE_C\")\n\n\n\n\n3.3.3 Visualising the desire lines\nTo visualise the resulting desire lines, the code chunk below is used.\n\n\nCode\ntm_shape(mpsz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\nflowLine %&gt;%  \ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5)\n\n\n\n\n\nWhen the flow data are very messy and highly skewed like the one shown above, it is wiser to focus on selected flows, for example flow greater than or equal to 5000 as shown below.\n\n\nCode\ntm_shape(mpsz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\nflowLine %&gt;%  \n  filter(MORNING_PEAK &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5)"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex02/Inclass_Ex02.html",
    "href": "Inclass_Ex/Inclass_Ex02/Inclass_Ex02.html",
    "title": "In Class Exercise 2",
    "section": "",
    "text": "In this exercise, a new sfdep package will be sused.\n\nThe Basics of sfdep\nSpacetime and Spacetime cubes\nEmerging Hot Spot Analysis\nConditional Permutations with sfdep\n\n\n\nCode\npacman::p_load(tmap, sf, sfdep, tidyverse, knitr, plotly)\n\n\n\n\n\nImport the Hunan GDPPC data.\n\n\nCode\nhunan_2012 = read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nUse sf package to read geospatial data.\n\n\nCode\nhunan = st_read(dsn = \"data/geospatial\",\n                   layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Inclass_Ex/Inclass_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\nCode\nhunan_data = left_join(hunan, hunan_2012, by = join_by(County)) %&gt;%\n  select(1:4, 7, 15)\n\n\n\nSince both tables got 88 observations, in order to ensure retaining all geospatial properties, left join to geo file.\nHere the ‘County’ column is the common column in both table, actually no need to explicitly identify the join_by, but here I would like to leave it there for clarification.\n\n\n\n\n\nCode\nwm_q &lt;- hunan_data %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, style = 'W'),  \n         .before = 1)  # to put the weight column at the most left."
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex02/Inclass_Ex02.html#data-preparation",
    "href": "Inclass_Ex/Inclass_Ex02/Inclass_Ex02.html#data-preparation",
    "title": "In Class Exercise 2",
    "section": "",
    "text": "In this exercise, a new sfdep package will be sused.\n\nThe Basics of sfdep\nSpacetime and Spacetime cubes\nEmerging Hot Spot Analysis\nConditional Permutations with sfdep\n\n\n\nCode\npacman::p_load(tmap, sf, sfdep, tidyverse, knitr, plotly)\n\n\n\n\n\nImport the Hunan GDPPC data.\n\n\nCode\nhunan_2012 = read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nUse sf package to read geospatial data.\n\n\nCode\nhunan = st_read(dsn = \"data/geospatial\",\n                   layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Inclass_Ex/Inclass_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\n\n\nCode\nhunan_data = left_join(hunan, hunan_2012, by = join_by(County)) %&gt;%\n  select(1:4, 7, 15)\n\n\n\nSince both tables got 88 observations, in order to ensure retaining all geospatial properties, left join to geo file.\nHere the ‘County’ column is the common column in both table, actually no need to explicitly identify the join_by, but here I would like to leave it there for clarification.\n\n\n\n\n\nCode\nwm_q &lt;- hunan_data %&gt;%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb, style = 'W'),  \n         .before = 1)  # to put the weight column at the most left."
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex02/Inclass_Ex02.html#computing-local-morons-i",
    "href": "Inclass_Ex/Inclass_Ex02/Inclass_Ex02.html#computing-local-morons-i",
    "title": "In Class Exercise 2",
    "section": "2 Computing local Moron’s I",
    "text": "2 Computing local Moron’s I\n\n\nCode\nlisa &lt;- wm_q %&gt;%\n  mutate(local_moran = local_moran(GDPPC, nb, wt, nsim = 99),\n         .before = 1) %&gt;%\n  unnest(local_moran)"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex02/Inclass_Ex02.html#time-series-analysis",
    "href": "Inclass_Ex/Inclass_Ex02/Inclass_Ex02.html#time-series-analysis",
    "title": "In Class Exercise 2",
    "section": "3 Time series analysis",
    "text": "3 Time series analysis\n\n\nCode\nGDPPC = read_csv(\"data/aspatial/Hunan_GDPPC.csv\")\n\n\n\n3.1 Data Wranggling\n\n3.1.1 Creating a time series cube\n\n\nCode\nGDPPC_st &lt;- spacetime(GDPPC, hunan, \n                      .loc_col = \"County\", .time_col = \"Year\")\n\n\nVerifying space-time cube object.\n\n\nCode\nis_spacetime_cube(GDPPC_st)\n\n\n[1] TRUE\n\n\n\n\nCode\nGDPPC_nb &lt;- GDPPC_st %&gt;%\n  activate(\"geometry\") %&gt;%\n  mutate(nb = include_self(st_contiguity(geometry)),\n         wt = st_inverse_distance(nb, geometry, scale = 1, alpha = 1),\n         .before = 1) %&gt;%\n  set_nbs(\"nb\") %&gt;%\n  set_wts(\"wt\")\n\n\n\n\n\n3.2 Computing Gi*\n\n\nCode\ngi_stars &lt;- GDPPC_nb %&gt;%\n  group_by(Year) %&gt;%\n  mutate(gi_star = local_gstar_perm(GDPPC, nb, wt)) %&gt;%\n  unnest(gi_star)"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html",
    "href": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html",
    "title": "In Class Exercise 3",
    "section": "",
    "text": "Code\npacman::p_load(tmap, sf, DT, sp, reshape2,\n               ggpubr, units, tidyverse)\n\n\n\n\n\nThis exercise is a continuation of Chapter 15: Processing and Visualising Flow Data and the following data will be used:\n\nod_data.rds, weekday morning peak passenger flows at planning subzone level.\nmpsz.rds, URA Master Plan 2019 Planning Subzone boundary in simple feature tibble data frame format.\n\nBeside these two data sets, an additional attribute data file called pop.csv will be provided.\nImport the population data.\n\n\nCode\npop &lt;- read_csv(\"./data/aspatial/pop.csv\")\n\n\nCheck the population data per age range.\n\n\nCode\nglimpse(pop)\n\n\nRows: 332\nColumns: 5\n$ PA       &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG …\n$ SZ       &lt;chr&gt; \"ANG MO KIO TOWN CENTRE\", \"CHENG SAN\", \"CHONG BOON\", \"KEBUN B…\n$ AGE7_12  &lt;dbl&gt; 310, 1140, 1010, 1050, 420, 810, 390, 980, 0, 260, 0, 1190, 6…\n$ AGE13_24 &lt;dbl&gt; 710, 2770, 2650, 2390, 1120, 1920, 1150, 2000, 0, 650, 0, 326…\n$ AGE25_64 &lt;dbl&gt; 2780, 15700, 14240, 12460, 3620, 9650, 4350, 11320, 0, 2500, …"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#preparing-flow-data",
    "href": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#preparing-flow-data",
    "title": "In Class Exercise 3",
    "section": "",
    "text": "Code\npacman::p_load(tmap, sf, DT, sp, reshape2,\n               ggpubr, units, tidyverse)\n\n\n\n\n\nThis exercise is a continuation of Chapter 15: Processing and Visualising Flow Data and the following data will be used:\n\nod_data.rds, weekday morning peak passenger flows at planning subzone level.\nmpsz.rds, URA Master Plan 2019 Planning Subzone boundary in simple feature tibble data frame format.\n\nBeside these two data sets, an additional attribute data file called pop.csv will be provided.\nImport the population data.\n\n\nCode\npop &lt;- read_csv(\"./data/aspatial/pop.csv\")\n\n\nCheck the population data per age range.\n\n\nCode\nglimpse(pop)\n\n\nRows: 332\nColumns: 5\n$ PA       &lt;chr&gt; \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG MO KIO\", \"ANG …\n$ SZ       &lt;chr&gt; \"ANG MO KIO TOWN CENTRE\", \"CHENG SAN\", \"CHONG BOON\", \"KEBUN B…\n$ AGE7_12  &lt;dbl&gt; 310, 1140, 1010, 1050, 420, 810, 390, 980, 0, 260, 0, 1190, 6…\n$ AGE13_24 &lt;dbl&gt; 710, 2770, 2650, 2390, 1120, 1920, 1150, 2000, 0, 650, 0, 326…\n$ AGE25_64 &lt;dbl&gt; 2780, 15700, 14240, 12460, 3620, 9650, 4350, 11320, 0, 2500, …"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#computing-distance-matrix",
    "href": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#computing-distance-matrix",
    "title": "In Class Exercise 3",
    "section": "2 Computing Distance Matrix",
    "text": "2 Computing Distance Matrix\n\n2.1 Importing Data\nUse sf package to read master plan subzone data and bus stop location data.\n\n\nCode\nmpsz &lt;- st_read(dsn = \"./data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;% st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Inclass_Ex/Inclass_Ex03/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\n2.2 Converting from sf data.table to SpatialPolygonsDataFrame\nThere are at least two ways to compute the required distance matrix. One is based on sf and the other is based on sp. Past experience shown that computing distance matrix by using sf function took relatively longer time that sp method especially the data set is large. In view of this, sp method is used in the code chunks below.\nFirst as.Spatial() will be used to convert mpsz from sf tibble data frame to SpatialPolygonsDataFrame of sp object as shown in the code chunk below.\n\n\nCode\nmpsz_sp &lt;- as(mpsz, \"Spatial\")\nmpsz_sp\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 332 \nextent      : 2667.538, 56396.44, 15748.72, 50256.33  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 6\nnames       : SUBZONE_N, SUBZONE_C, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C \nmin values  : ADMIRALTY,    AMSZ01, ANG MO KIO,         AM, CENTRAL REGION,       CR \nmax values  :    YUNNAN,    YSSZ09,     YISHUN,         YS,    WEST REGION,       WR \n\n\n\n\n2.3 Computing the distance matrix\nNext, spDists() of sp package will be used to compute the Euclidean distance between the centroids of the planning subzones.\n\n\nCode\ndist &lt;- spDists(mpsz_sp, \n                longlat = FALSE)\nhead(dist, n=c(10, 10))\n\n\n           [,1]       [,2]      [,3]      [,4]       [,5]      [,6]      [,7]\n [1,]     0.000  3926.0025  3939.108 20252.964  2989.9839  1431.330 19211.836\n [2,]  3926.003     0.0000   305.737 16513.865   951.8314  5254.066 16242.523\n [3,]  3939.108   305.7370     0.000 16412.062  1045.9088  5299.849 16026.146\n [4,] 20252.964 16513.8648 16412.062     0.000 17450.3044 21665.795  7229.017\n [5,]  2989.984   951.8314  1045.909 17450.304     0.0000  4303.232 17020.916\n [6,]  1431.330  5254.0664  5299.849 21665.795  4303.2323     0.000 20617.082\n [7,] 19211.836 16242.5230 16026.146  7229.017 17020.9161 20617.082     0.000\n [8,] 14960.942 12749.4101 12477.871 11284.279 13336.0421 16281.453  5606.082\n [9,]  7515.256  7934.8082  7649.776 18427.503  7801.6163  8403.896 14810.930\n[10,]  6391.342  4975.0021  4669.295 15469.566  5226.8731  7707.091 13111.391\n           [,8]      [,9]     [,10]\n [1,] 14960.942  7515.256  6391.342\n [2,] 12749.410  7934.808  4975.002\n [3,] 12477.871  7649.776  4669.295\n [4,] 11284.279 18427.503 15469.566\n [5,] 13336.042  7801.616  5226.873\n [6,] 16281.453  8403.896  7707.091\n [7,]  5606.082 14810.930 13111.391\n [8,]     0.000  9472.024  8575.490\n [9,]  9472.024     0.000  3780.800\n[10,]  8575.490  3780.800     0.000\n\n\n\n\n2.4 Labelling column and row heanders of a distance matrix\nFirst, we will create a list sorted according to the the distance matrix by planning sub-zone code.\n\n\nCode\nsz_names &lt;- mpsz$SUBZONE_C\n\n\nNext we will attach SUBZONE_C to row and column for distance matrix matching ahead\n\n\nCode\ncolnames(dist) &lt;- paste0(sz_names)\nrownames(dist) &lt;- paste0(sz_names)\n\n\n\n\n2.5 Pivoting distance value by SUBZONE_C\nNext, we will pivot the distance matrix into a long table by using the row and column subzone codes as show in the code chunk below.\n\n\nCode\ndistPair &lt;- melt(dist) %&gt;%\n  rename(dist = value)\nhead(distPair, 10)\n\n\n     Var1   Var2      dist\n1  MESZ01 MESZ01     0.000\n2  RVSZ05 MESZ01  3926.003\n3  SRSZ01 MESZ01  3939.108\n4  WISZ01 MESZ01 20252.964\n5  MUSZ02 MESZ01  2989.984\n6  MPSZ05 MESZ01  1431.330\n7  WISZ03 MESZ01 19211.836\n8  WISZ02 MESZ01 14960.942\n9  SISZ02 MESZ01  7515.256\n10 SISZ01 MESZ01  6391.342\n\n\n\nNotice that the within zone distance is 0.\n\n\n\n2.6 Updating intra-zonal distances\nIn this section, we are going to append a constant value to replace the intra-zonal distance of 0.\nFirst, we will select and find out the minimum value of the distance by using summary().\n\n\nCode\ndistPair %&gt;%\n  filter(dist &gt; 0) %&gt;%\n  summary()\n\n\n      Var1             Var2             dist        \n MESZ01 :   331   MESZ01 :   331   Min.   :  173.8  \n RVSZ05 :   331   RVSZ05 :   331   1st Qu.: 7149.5  \n SRSZ01 :   331   SRSZ01 :   331   Median :11890.0  \n WISZ01 :   331   WISZ01 :   331   Mean   :12229.4  \n MUSZ02 :   331   MUSZ02 :   331   3rd Qu.:16401.7  \n MPSZ05 :   331   MPSZ05 :   331   Max.   :49894.4  \n (Other):107906   (Other):107906                    \n\n\nNext, a constant distance value of 50m is added into intra-zones distance.\n\n\nCode\ndistPair$dist &lt;- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\n\n\nThe code chunk below will be used to check the result data.frame.\n\n\nCode\nsummary(distPair)\n\n\n      Var1             Var2             dist      \n MESZ01 :   332   MESZ01 :   332   Min.   :   50  \n RVSZ05 :   332   RVSZ05 :   332   1st Qu.: 7097  \n SRSZ01 :   332   SRSZ01 :   332   Median :11864  \n WISZ01 :   332   WISZ01 :   332   Mean   :12193  \n MUSZ02 :   332   MUSZ02 :   332   3rd Qu.:16388  \n MPSZ05 :   332   MPSZ05 :   332   Max.   :49894  \n (Other):108232   (Other):108232                  \n\n\nThe code chunk below is used to rename the origin and destination fields.\n\n\nCode\ndistPair &lt;- distPair %&gt;%\n  rename(orig = Var1,\n         dest = Var2)\n\n\nThe code chunk below is used to rename the origin and destination fields.\n\n\nCode\nwrite_rds(distPair, \"./data/rds/distPair.rds\")"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#preparing-flow-data-1",
    "href": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#preparing-flow-data-1",
    "title": "In Class Exercise 3",
    "section": "3. Preparing flow data",
    "text": "3. Preparing flow data\nIimport od_data save in hands on exercise 3 into R environment.\n\n\nCode\nod_data &lt;- read_rds(\"./data/rds/od_data.rds\")\n\n\nNext, we will compute the total passenger trip between and within planning subzones by using the code chunk below. The output is all flow_data.\n\n\nCode\nflow_data &lt;- od_data %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;% \n  summarize(TRIPS = sum(MORNING_PEAK)) \n\nhead(flow_data, 10)\n\n\n# A tibble: 10 × 3\n# Groups:   ORIGIN_SZ [1]\n   ORIGIN_SZ DESTIN_SZ TRIPS\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 AMSZ01    AMSZ01     2694\n 2 AMSZ01    AMSZ02    10591\n 3 AMSZ01    AMSZ03    14980\n 4 AMSZ01    AMSZ04     3106\n 5 AMSZ01    AMSZ05     7734\n 6 AMSZ01    AMSZ06     2306\n 7 AMSZ01    AMSZ07     1824\n 8 AMSZ01    AMSZ08     2734\n 9 AMSZ01    AMSZ09     2300\n10 AMSZ01    AMSZ10      164\n\n\n\n3.1 Separating intra-flow from passenger volume df\nCode chunk below is used to add three new fields in flow_data dataframe.\n\n\nCode\nflow_data$FlowNoIntra &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0, flow_data$TRIPS)\nflow_data$offset &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, \n  0.000001, 1)\n\n\n\n\n3.2 Combining passenger volume data with distance value\nBefore we can join flow_data and distPair, we need to convert data value type of ORIGIN_SZ and DESTIN_SZ fields of flow_data dataframe into factor data type.\n\n\nCode\nflow_data$ORIGIN_SZ &lt;- as.factor(flow_data$ORIGIN_SZ)\nflow_data$DESTIN_SZ &lt;- as.factor(flow_data$DESTIN_SZ)\n\n\nNow, left_join() of dplyr will be used to flow_data dataframe and distPair dataframe. The output is called flow_data1.\n\n\nCode\nflow_data1 &lt;- flow_data %&gt;%\n  left_join (distPair,\n             by = c(\"ORIGIN_SZ\" = \"orig\",\n                    \"DESTIN_SZ\" = \"dest\"))"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#preparing-origin-and-destination-attributes",
    "href": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#preparing-origin-and-destination-attributes",
    "title": "In Class Exercise 3",
    "section": "4. Preparing Origin and Destination Attributes",
    "text": "4. Preparing Origin and Destination Attributes\n\n4.1 Geospatial data wrangling\n\n\nCode\npop &lt;- pop %&gt;%\n  left_join(mpsz,\n            by = c(\"PA\" = \"PLN_AREA_N\",\n                   \"SZ\" = \"SUBZONE_N\")) %&gt;%\n  select(1:6) %&gt;%\n  rename(SZ_NAME = SZ,\n         SZ = SUBZONE_C)\n\n\n\n\n4.2 Preparing origin attribute\n\n\nCode\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(ORIGIN_SZ = \"SZ\")) %&gt;%\n  rename(ORIGIN_AGE7_12 = AGE7_12,\n         ORIGIN_AGE13_24 = AGE13_24,\n         ORIGIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\n\n\n\n4.3 Preparing destination attribute\n\n\nCode\nflow_data1 &lt;- flow_data1 %&gt;%\n  left_join(pop,\n            by = c(DESTIN_SZ = \"SZ\")) %&gt;%\n  rename(DESTIN_AGE7_12 = AGE7_12,\n         DESTIN_AGE13_24 = AGE13_24,\n         DESTIN_AGE25_64 = AGE25_64) %&gt;%\n  select(-c(PA, SZ_NAME))\n\n\nWe will called the output data file SIM_data. it is in rds data file format.\n\n\nCode\nwrite_rds(flow_data1, \"./data/rds/SIM_data.rds\")"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#calibrating-spatial-interaction-models",
    "href": "Inclass_Ex/Inclass_Ex03/Inclass_Ex03.html#calibrating-spatial-interaction-models",
    "title": "In Class Exercise 3",
    "section": "5. Calibrating Spatial Interaction Models",
    "text": "5. Calibrating Spatial Interaction Models\nIn this section, you will learn how to calibrate Spatial Interaction Models by using Poisson Regression method.\n\n5.1 Visualising the dependent variable\n\n\nCode\nSIM_data &lt;- read_rds(\"./data/rds/SIM_data.rds\")\n\n\nFirstly, let us plot the distribution of the dependent variable (i.e. TRIPS) by using histogram method by using the code chunk below.\n\n\nCode\nggplot(data = SIM_data,\n       aes(x = TRIPS)) +\n  geom_histogram() +\n  theme_light()\n\n\n\n\n\n\nNotice that the distribution is highly skewed and not resemble bell shape or also known as normal distribution.\n\nNext, let us visualise the relation between the dependent variable and one of the key independent variable in Spatial Interaction Model, namely distance.\n\n\nCode\nggplot(data = SIM_data,\n       aes(x = dist,\n           y = TRIPS)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  theme_light()\n\n\n\n\n\n\nNotice that their relationship hardly resemble linear relationship.\n\nOn the other hand, if we plot the scatter plot by using the log transformed version of both variables, we can see that their relationship is more resemble linear relationship.\n\n\nCode\nggplot(data = SIM_data,\n       aes(x = log(dist),\n           y = log(TRIPS))) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  theme_light()"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex01/data/geospatial/MPSZ-2019.html",
    "href": "Inclass_Ex/Inclass_Ex01/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex01/Inclass_Ex01.html",
    "href": "Inclass_Ex/Inclass_Ex01/Inclass_Ex01.html",
    "title": "In Class Exercise 1",
    "section": "",
    "text": "SSS624 Applied Geospatial Analytics will be conducted using case study approach. This run the use case is Urban Mobility analysis by using passenger volume by origin-destination bus stops. To get ready for the excercise, you are required to do the following as soon as possible:\n\nApply an API access from LTA by visiting LTA DataMall,\nComplete the API Access formand submit.  Please note that it will take at least one working day to reply you.\nNext, return to Dynamic Datasets page and click on API Documentation. The pdf document appears.  Click on 2.6 and read the content carefully.\nOnce you received the API access code, read Section 1 of API Document and follow the instruction provided to download the data sets. You are required to download last three months data (August, September and October)."
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex01/Inclass_Ex01.html#data-acquisition",
    "href": "Inclass_Ex/Inclass_Ex01/Inclass_Ex01.html#data-acquisition",
    "title": "In Class Exercise 1",
    "section": "",
    "text": "SSS624 Applied Geospatial Analytics will be conducted using case study approach. This run the use case is Urban Mobility analysis by using passenger volume by origin-destination bus stops. To get ready for the excercise, you are required to do the following as soon as possible:\n\nApply an API access from LTA by visiting LTA DataMall,\nComplete the API Access formand submit.  Please note that it will take at least one working day to reply you.\nNext, return to Dynamic Datasets page and click on API Documentation. The pdf document appears.  Click on 2.6 and read the content carefully.\nOnce you received the API access code, read Section 1 of API Document and follow the instruction provided to download the data sets. You are required to download last three months data (August, September and October)."
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex01/Inclass_Ex01.html#data-preparation",
    "href": "Inclass_Ex/Inclass_Ex01/Inclass_Ex01.html#data-preparation",
    "title": "In Class Exercise 1",
    "section": "2 Data Preparation",
    "text": "2 Data Preparation\n\n2.1 Get ready\n\n\nCode\npacman::p_load(tmap, sf, tidyverse, knitr, h3jsr)\n\n\n\n\n2.2 Importing the OD Data\nImport the passenger volume by origin destination bus stops data.\n\n\nCode\nodbus &lt;- read_csv(\"./data/aspatial/origin_destination_bus_202308.csv\") %&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),\n         DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))\n\n\nCheck the data.\n\n\nCode\nglimpse(odbus)\n\n\nRows: 5,709,512\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 44069, 20281, 2…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 17229, 20141, 2…\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\n\n\n2.3 Extracting Data\nExtract passenger volume data between 7-9 o’clock during weekdays.\n\n\nCode\norigin7_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 7 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\nSave the output in rds format\n\n\nCode\nwrite_rds(origin7_9, \"./data/rds/origin7_9.rds\")\n\n\nCan extract data from saved file again.\n\n\nCode\n# read from saved file.\n# origin7_9 &lt;- read_rds(\"./data/rds/origin7_9.rds\")"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex01/Inclass_Ex01.html#working-with-geospatial-data",
    "href": "Inclass_Ex/Inclass_Ex01/Inclass_Ex01.html#working-with-geospatial-data",
    "title": "In Class Exercise 1",
    "section": "3 Working with GeoSpatial Data",
    "text": "3 Working with GeoSpatial Data\n\n3.1 Importing Data\nUse sf package to read master plan subzone data and bus stop location data.\n\n\nCode\nmpsz &lt;- st_read(dsn = \"./data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;% st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Inclass_Ex/Inclass_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\n\nCode\nbusstop &lt;- st_read(dsn = \"./data/geospatial\",\n                   layer = \"BusStop\")  %&gt;% st_transform(crs = 3414)\n\n\nReading layer `BusStop' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Inclass_Ex/Inclass_Ex01/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\n\n\n\n\n\n\nTip\n\n\n\nHere we use “st_transform(crs = 3414)” to change the coordinate from decimal degree to meters.\n\n\n\n\n3.2 Wrangling Data\nCombine the bus stop location with the Singapore subzone map.\n\n\nCode\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\n\nst_intersection() is used to perform point and polygon overly and the output will be in point sf object.\nselect() of dplyr package is then use to retain only BUS_STOP_N and SUBZONE_C in the busstop_mpsz sf data frame.\nfive bus stops are excluded in the resultant data frame because they are outside of Singapore boundary.\n\n\n\nCode\nglimpse(busstop_mpsz)\n\n\nRows: 5,156\nColumns: 2\n$ BUS_STOP_N &lt;chr&gt; \"13099\", \"13089\", \"06151\", \"13211\", \"13139\", \"13109\", \"1311…\n$ SUBZONE_C  &lt;chr&gt; \"RVSZ05\", \"RVSZ05\", \"SRSZ01\", \"SRSZ01\", \"SRSZ01\", \"SRSZ01\",…\n\n\nNext, we are going to append the planning subzone code from busstop_mpsz data frame onto odbus7_9 data frame.\n\n\nCode\norigin_data &lt;- left_join(origin7_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C)\n\n\nBefore continue, it is a good practice for us to check for duplicating records.\n\n\nCode\nduplicate &lt;- origin_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nIt will be a good practice to confirm if the duplicating records issue has been addressed fully.\n\n\nCode\nmpsz_origtrip &lt;- left_join(mpsz, \n                           origin_data,\n                           by = c(\"SUBZONE_C\" = \"ORIGIN_SZ\"))\n\n\n\n\n3.3 Choropleth Visualization\nUsing the steps you had learned, prepare a choropleth map showing the distribution of passenger trips at planning sub-zone level.\n\n\nCode\ntm_shape(mpsz_origtrip)+\n  tm_fill(\"TRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Passenger trips\") +\n  tm_layout(main.title = \"Passenger trips generated at planning sub-zone level\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from URA\\n and Passenger trips data from LTA\", \n             position = c(\"left\", \"bottom\"))"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html",
    "href": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html",
    "title": "In Class Exercise 4",
    "section": "",
    "text": "Geographically weighted poisson regression."
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#overview",
    "href": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#overview",
    "title": "In Class Exercise 4",
    "section": "",
    "text": "Geographically weighted poisson regression."
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#preparing-data",
    "href": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#preparing-data",
    "title": "In Class Exercise 4",
    "section": "2 Preparing Data",
    "text": "2 Preparing Data\nTwo data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\n2.1 Get ready\n\n\nCode\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)\n\n\n\n\n2.2 Geospatial Data\nImport the subzone data.\nCheck the data.\n\n\n2.3 Aspatial Data\nImporting condo resale data.\nWrangling data."
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex03/data/geospatial/MPSZ-2019.html",
    "href": "Inclass_Ex/Inclass_Ex03/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex03/data/geospatial/MPSZ-2019.html",
    "href": "Handson_Ex/Handson_Ex03/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html",
    "title": "Hands on Excercise 4",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#overview",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#overview",
    "title": "Hands on Excercise 4",
    "section": "",
    "text": "Geographically weighted regression (GWR) is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable). In this hands-on exercise, you will learn how to build hedonic pricing models by using GWR methods. The dependent variable is the resale prices of condominium in 2015. The independent variables are divided into either structural and locational."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#preparing-data",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#preparing-data",
    "title": "Hands on Excercise 4",
    "section": "2 Preparing Data",
    "text": "2 Preparing Data\nTwo data sets will be used in this model building exercise, they are:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\n2.1 Get ready\n\n\nCode\npacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)\n\n\n\n\n2.2 Geospatial Data\nImport the subzone data.\n\n\nCode\nmpsz &lt;- st_read(dsn = \"./data/geospatial\",\n                   layer = \"MP14_SUBZONE_WEB_PL\") %&gt;% st_transform(crs = 3414)\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex04/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nCheck the data.\n\n\nCode\nst_crs(mpsz)\n\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\nReveal the extent of mpsz.\n\n\nCode\nst_bbox(mpsz) #view extent\n\n\n     xmin      ymin      xmax      ymax \n 2667.538 15748.721 56396.440 50256.334 \n\n\n\n\n2.3 Aspatial Data\nImporting condo resale data.\n\n\nCode\ncondo_resale = read_csv(\"./data/aspatial/Condo_resale_2015.csv\")\nglimpse(condo_resale)\n\n\nRows: 1,436\nColumns: 23\n$ LATITUDE             &lt;dbl&gt; 1.287145, 1.328698, 1.313727, 1.308563, 1.321437,…\n$ LONGITUDE            &lt;dbl&gt; 103.7802, 103.8123, 103.7971, 103.8247, 103.9505,…\n$ POSTCODE             &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472, 3…\n$ SELLING_PRICE        &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1320…\n$ AREA_SQM             &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 168,…\n$ AGE                  &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22, 6,…\n$ PROX_CBD             &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783402…\n$ PROX_CHILDCARE       &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543, 0…\n$ PROX_ELDERLYCARE     &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.121…\n$ PROX_URA_GROWTH_AREA &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.410632,…\n$ PROX_HAWKER_MARKET   &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969, 0…\n$ PROX_KINDERGARTEN    &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076, 0…\n$ PROX_MRT             &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.528…\n$ PROX_PARK            &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.116…\n$ PROX_PRIMARY_SCH     &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.709…\n$ PROX_TOP_PRIMARY_SCH &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.709…\n$ PROX_SHOPPING_MALL   &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.307…\n$ PROX_SUPERMARKET     &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.581…\n$ PROX_BUS_STOP        &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340, 0…\n$ NO_Of_UNITS          &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34, 3…\n$ FAMILY_FRIENDLY      &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nCheck some detail of the data.\n\n\nCode\nhead(condo_resale$LONGITUDE) #see the data in XCOORD column\n\n\n[1] 103.7802 103.8123 103.7971 103.8247 103.9505 103.9386\n\n\n\n\nCode\nhead(condo_resale$LATITUDE) #see the data in YCOORD column\n\n\n[1] 1.287145 1.328698 1.313727 1.308563 1.321437 1.314198\n\n\n\n\nCode\nsummary(condo_resale)\n\n\n    LATITUDE       LONGITUDE        POSTCODE      SELLING_PRICE     \n Min.   :1.240   Min.   :103.7   Min.   : 18965   Min.   :  540000  \n 1st Qu.:1.309   1st Qu.:103.8   1st Qu.:259849   1st Qu.: 1100000  \n Median :1.328   Median :103.8   Median :469298   Median : 1383222  \n Mean   :1.334   Mean   :103.8   Mean   :440439   Mean   : 1751211  \n 3rd Qu.:1.357   3rd Qu.:103.9   3rd Qu.:589486   3rd Qu.: 1950000  \n Max.   :1.454   Max.   :104.0   Max.   :828833   Max.   :18000000  \n    AREA_SQM          AGE           PROX_CBD       PROX_CHILDCARE    \n Min.   : 34.0   Min.   : 0.00   Min.   : 0.3869   Min.   :0.004927  \n 1st Qu.:103.0   1st Qu.: 5.00   1st Qu.: 5.5574   1st Qu.:0.174481  \n Median :121.0   Median :11.00   Median : 9.3567   Median :0.258135  \n Mean   :136.5   Mean   :12.14   Mean   : 9.3254   Mean   :0.326313  \n 3rd Qu.:156.0   3rd Qu.:18.00   3rd Qu.:12.6661   3rd Qu.:0.368293  \n Max.   :619.0   Max.   :37.00   Max.   :19.1804   Max.   :3.465726  \n PROX_ELDERLYCARE  PROX_URA_GROWTH_AREA PROX_HAWKER_MARKET PROX_KINDERGARTEN \n Min.   :0.05451   Min.   :0.2145       Min.   :0.05182    Min.   :0.004927  \n 1st Qu.:0.61254   1st Qu.:3.1643       1st Qu.:0.55245    1st Qu.:0.276345  \n Median :0.94179   Median :4.6186       Median :0.90842    Median :0.413385  \n Mean   :1.05351   Mean   :4.5981       Mean   :1.27987    Mean   :0.458903  \n 3rd Qu.:1.35122   3rd Qu.:5.7550       3rd Qu.:1.68578    3rd Qu.:0.578474  \n Max.   :3.94916   Max.   :9.1554       Max.   :5.37435    Max.   :2.229045  \n    PROX_MRT         PROX_PARK       PROX_PRIMARY_SCH  PROX_TOP_PRIMARY_SCH\n Min.   :0.05278   Min.   :0.02906   Min.   :0.07711   Min.   :0.07711     \n 1st Qu.:0.34646   1st Qu.:0.26211   1st Qu.:0.44024   1st Qu.:1.34451     \n Median :0.57430   Median :0.39926   Median :0.63505   Median :1.88213     \n Mean   :0.67316   Mean   :0.49802   Mean   :0.75471   Mean   :2.27347     \n 3rd Qu.:0.84844   3rd Qu.:0.65592   3rd Qu.:0.95104   3rd Qu.:2.90954     \n Max.   :3.48037   Max.   :2.16105   Max.   :3.92899   Max.   :6.74819     \n PROX_SHOPPING_MALL PROX_SUPERMARKET PROX_BUS_STOP       NO_Of_UNITS    \n Min.   :0.0000     Min.   :0.0000   Min.   :0.001595   Min.   :  18.0  \n 1st Qu.:0.5258     1st Qu.:0.3695   1st Qu.:0.098356   1st Qu.: 188.8  \n Median :0.9357     Median :0.5687   Median :0.151710   Median : 360.0  \n Mean   :1.0455     Mean   :0.6141   Mean   :0.193974   Mean   : 409.2  \n 3rd Qu.:1.3994     3rd Qu.:0.7862   3rd Qu.:0.220466   3rd Qu.: 590.0  \n Max.   :3.4774     Max.   :2.2441   Max.   :2.476639   Max.   :1703.0  \n FAMILY_FRIENDLY     FREEHOLD      LEASEHOLD_99YR  \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.0000   Median :0.0000  \n Mean   :0.4868   Mean   :0.4227   Mean   :0.4882  \n 3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n\n\n\n2.3.1 Converting aspatial data frame into a sf object\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\n\nCode\ncondo_resale.sf &lt;- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %&gt;%\n  st_transform(crs=3414)\n\n\nNotice that st_transform() of sf package is used to convert the coordinates from wgs84 (i.e. crs:4326) to svy21 (i.e. crs=3414).\nNext, head() is used to list the content of condo_resale.sf object.\n\n\nCode\nhead(condo_resale.sf)\n\n\nSimple feature collection with 6 features and 21 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 22085.12 ymin: 29951.54 xmax: 41042.56 ymax: 34546.2\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 6 × 22\n  POSTCODE SELLING_PRICE AREA_SQM   AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE\n     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1   118635       3000000      309    30     7.94          0.166            2.52 \n2   288420       3880000      290    32     6.61          0.280            1.93 \n3   267833       3325000      248    33     6.90          0.429            0.502\n4   258380       4250000      127     7     4.04          0.395            1.99 \n5   467169       1400000      145    28    11.8           0.119            1.12 \n6   466472       1320000      139    22    10.3           0.125            0.789\n# ℹ 15 more variables: PROX_URA_GROWTH_AREA &lt;dbl&gt;, PROX_HAWKER_MARKET &lt;dbl&gt;,\n#   PROX_KINDERGARTEN &lt;dbl&gt;, PROX_MRT &lt;dbl&gt;, PROX_PARK &lt;dbl&gt;,\n#   PROX_PRIMARY_SCH &lt;dbl&gt;, PROX_TOP_PRIMARY_SCH &lt;dbl&gt;,\n#   PROX_SHOPPING_MALL &lt;dbl&gt;, PROX_SUPERMARKET &lt;dbl&gt;, PROX_BUS_STOP &lt;dbl&gt;,\n#   NO_Of_UNITS &lt;dbl&gt;, FAMILY_FRIENDLY &lt;dbl&gt;, FREEHOLD &lt;dbl&gt;,\n#   LEASEHOLD_99YR &lt;dbl&gt;, geometry &lt;POINT [m]&gt;"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html",
    "title": "Hands on Exercise 2",
    "section": "",
    "text": "Two data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nCode\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nCode\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\n\nCode\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\n\nCode\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nIn this section, you will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\n\nCode\nwm_q[[1]]\n\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\n\nCode\nhunan$County[1]\n\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\n\nCode\nhunan$NAME_3[c(2,3,4,57,85)]\n\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\n\nCode\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\n\nCode\nstr(wm_q) %&gt;% head(10)\n\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nNULL\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output might cut across several pages. Save the trees if you are going to print out the report.\n\n\n\n\n\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\n\nCode\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\n\nCode\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\n\nCode\ncoords &lt;- cbind(longitude, latitude)\n\n\nWe check the first few observations to see if things are formatted correctly.\n\n\nCode\nhead(coords)\n\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"blue\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"blue\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)         \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\n\nCode\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nTip\n\n\n\nQuiz: What is the meaning of “Average number of links: 3.681818” shown above?\nAs I understand, this should be number of nonzero links over number of regions.\n\n\nNext, we will use str() to display the structure of the weight matrix is to combine table() and card() of spdep.\n\n\nCode\ntable(hunan$County, card(wm_d62))\n\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\nTo extract number of regions from the table:\n\n\nCode\nn_comp &lt;- n.comp.nb(wm_d62)\ntable(n_comp$comp.id)\n\n\n\n 1 \n88 \n\n\n\n\nNext, we will plot the distance weight matrix by using the code chunk below.\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"blue\", length=0.08)\n\n\n\n\n\nThe blue lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"blue\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\n\nNotice that each county has six neighbours, no less no more!\n\n\n\nWe can plot the weight matrix using the code chunk below.\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"blue\")\n\n\n\n\n\n\n\n\n\n\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\n\nCode\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\n\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q    \n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors type:\n\n\nCode\nrswm_q$weights[10]\n\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.2 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\n\nCode\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\n\nCode\nrswm_ids$weights[1]\n\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\n\nCode\nsummary(unlist(rswm_ids$weights))\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338 \n\n\n\n\n\n\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\n\nCode\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\n\nCode\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: Can you see the meaning of Spatial lag with row-standardized weights now?\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\n\nCode\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\n\nCode\nhead(hunan)\n\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\n\nCode\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\n\nCode\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: Can you understand the meaning of Spatial lag as a sum of neighboring values now?\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\n\nCode\nhunan &lt;- left_join(hunan, lag.res)\n\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode\nwm_qs &lt;- include.self(wm_q)\n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\n\nCode\nwm_qs[[1]]\n\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five. Now we obtain weights with nb2listw()\n\n\nCode\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\n\nCode\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below. Let’s check the first 10 rows.\n\n\nCode\nhunan %&gt;%\n  select(\"County\", \"lag GDPPC\", \"lag_window_avg GDPPC\") %&gt;%\n  head(10) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\n\nCode\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\nFor more effective comparison, it is advicible to use the core tmap mapping functions.\n\n\n\n\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\n\nCode\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five. Again, we use nb2listw() and glist() to explicitly assign weight values.\n\n\nCode\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\n\nCode\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  head(10) %&gt;%\n  kable()\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\n\nCode\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#data-preparation",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#data-preparation",
    "title": "Hands on Exercise 2",
    "section": "",
    "text": "Two data sets will be used in this hands-on exercise, they are:\n\nHunan county boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\nCode\npacman::p_load(sf, spdep, tmap, tidyverse, knitr)\n\n\n\n\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nCode\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\n\nCode\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\n\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#visualising-regional-development-indicator",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#visualising-regional-development-indicator",
    "title": "Hands on Exercise 2",
    "section": "",
    "text": "Now, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\n\nCode\nbasemap &lt;- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc &lt;- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#computing-contiguity-spatial-weights",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#computing-contiguity-spatial-weights",
    "title": "Hands on Exercise 2",
    "section": "",
    "text": "In this section, you will learn how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\n\n\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode\nwm_q &lt;- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\n\nCode\nwm_q[[1]]\n\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nWe can retrive the county name of Polygon ID=1 by using the code chunk below:\n\n\nCode\nhunan$County[1]\n\n\n[1] \"Anxiang\"\n\n\nThe output reveals that Polygon ID=1 is Anxiang county.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\n\nCode\nhunan$NAME_3[c(2,3,4,57,85)]\n\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\n\nCode\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\n\nCode\nstr(wm_q) %&gt;% head(10)\n\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan, queen = TRUE)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nNULL\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe output might cut across several pages. Save the trees if you are going to print out the report.\n\n\n\n\n\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\n\nCode\nwm_r &lt;- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. We will calculate these in the sf package before moving onto the graphs. Getting Latitude and Longitude of Polygon Centroids\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid. We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\n\nCode\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\n\nCode\ncoords &lt;- cbind(longitude, latitude)\n\n\nWe check the first few observations to see if things are formatted correctly.\n\n\nCode\nhead(coords)\n\n\n     longitude latitude\n[1,]  112.1531 29.44362\n[2,]  112.0372 28.86489\n[3,]  111.8917 29.47107\n[4,]  111.7031 29.74499\n[5,]  111.6138 29.49258\n[6,]  111.0341 29.79863\n\n\n\n\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"blue\")\n\n\n\n\n\n\n\n\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"Queen Contiguity\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"blue\")\nplot(hunan$geometry, border=\"lightgrey\", main=\"Rook Contiguity\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#computing-distance-based-neighbours",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#computing-distance-based-neighbours",
    "title": "Hands on Exercise 2",
    "section": "",
    "text": "In this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package.\nThe function identifies neighbours of region points by Euclidean distance with a distance band with lower d1= and upper d2= bounds controlled by the bounds= argument. If unprojected coordinates are used and either specified in the coordinates object x or with x as a two column matrix and longlat=TRUE, great circle distances in km will be calculated assuming the WGS84 reference ellipsoid.\n\n\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)         \n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\n\nCode\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n\n\n\n\nTip\n\n\n\nQuiz: What is the meaning of “Average number of links: 3.681818” shown above?\nAs I understand, this should be number of nonzero links over number of regions.\n\n\nNext, we will use str() to display the structure of the weight matrix is to combine table() and card() of spdep.\n\n\nCode\ntable(hunan$County, card(wm_d62))\n\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 0 1 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       0 1 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          0 1 0 0 0 0\n  Dao           0 0 0 1 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 0 1 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 0 0 1 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 0 1\n  Hongjiang     0 0 0 0 1 0\n  Huarong       0 0 0 1 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 0 1 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 0 1 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 0 1 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      0 1 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       0 1 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 0 1 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 0 1\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 0 1 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       0 1 0 0 0 0\n  Tongdao       0 1 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 0 1 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 0 1 0 0\n  You           0 0 0 1 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 0 1 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 0 0 1 0\n  Zixing        0 0 1 0 0 0\n\n\nTo extract number of regions from the table:\n\n\nCode\nn_comp &lt;- n.comp.nb(wm_d62)\ntable(n_comp$comp.id)\n\n\n\n 1 \n88 \n\n\n\n\nNext, we will plot the distance weight matrix by using the code chunk below.\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"blue\", length=0.08)\n\n\n\n\n\nThe blue lines show the links of 1st nearest neighbours and the black lines show the links of neighbours within the cut-off distance of 62km.\nAlternatively, we can plot both of them next to each other by using the code chunk below.\n\n\nCode\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"blue\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\n\n\n\n\n\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode\nknn6 &lt;- knn2nb(knearneigh(coords, k=6))\nknn6\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\n\nNotice that each county has six neighbours, no less no more!\n\n\n\nWe can plot the weight matrix using the code chunk below.\n\n\nCode\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"blue\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#weights-based-on-idw",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#weights-based-on-idw",
    "title": "Hands on Exercise 2",
    "section": "",
    "text": "In this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\n\nCode\ndist &lt;- nbdists(wm_q, coords, longlat = TRUE)\nids &lt;- lapply(dist, function(x) 1/(x))\n\n\n\n\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode\nrswm_q &lt;- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q    \n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe zero.policy=TRUE option allows for lists of non-neighbors. This should be used with caution since the user may not be aware of missing neighbors in their dataset however, a zero.policy of FALSE would return an error.\nTo see the weight of the first polygon’s eight neighbors type:\n\n\nCode\nrswm_q$weights[10]\n\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\n\nEach neighbor is assigned a 0.125 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.2 before being tallied.\nUsing the same method, we can also derive a row standardised distance weight matrix by using the code chunk below.\n\n\nCode\nrswm_ids &lt;- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\n\n\n\nCode\nrswm_ids$weights[1]\n\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n\n\n\nCode\nsummary(unlist(rswm_ids$weights))\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#application-of-spatial-weight-matrix",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#application-of-spatial-weight-matrix",
    "title": "Hands on Exercise 2",
    "section": "",
    "text": "In this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and\nspatial window sum.\n\n\n\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values.\n\n\nCode\nGDPPC.lag &lt;- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\n\nRecalled in the previous section, we retrieved the GDPPC of these five countries by using the code chunk below.\n\n\nCode\nnb1 &lt;- wm_q[[1]]\nnb1 &lt;- hunan$GDPPC[nb1]\nnb1\n\n\n[1] 20981 34592 24473 21311 22879\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: Can you see the meaning of Spatial lag with row-standardized weights now?\n\n\nWe can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\n\nCode\nlag.list &lt;- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag.list)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag GDPPC\")\nhunan &lt;- left_join(hunan,lag.res)\n\n\nThe following table shows the average neighboring income values (stored in the Inc.lag object) for each county.\n\n\nCode\nhead(hunan)\n\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 110.4922 ymin: 28.61762 xmax: 112.3013 ymax: 30.12812\nGeodetic CRS:  WGS 84\n   NAME_2  ID_3  NAME_3   ENGTYPE_3  County GDPPC lag GDPPC\n1 Changde 21098 Anxiang      County Anxiang 23667  24847.20\n2 Changde 21100 Hanshou      County Hanshou 20981  22724.80\n3 Changde 21101  Jinshi County City  Jinshi 34592  24143.25\n4 Changde 21102      Li      County      Li 24473  27737.50\n5 Changde 21103   Linli      County   Linli 25554  27270.25\n6 Changde 21104  Shimen      County  Shimen 27137  21248.80\n                        geometry\n1 POLYGON ((112.0625 29.75523...\n2 POLYGON ((112.2288 29.11684...\n3 POLYGON ((111.8927 29.6013,...\n4 POLYGON ((111.3731 29.94649...\n5 POLYGON ((111.6324 29.76288...\n6 POLYGON ((110.8825 30.11675...\n\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_gdppc &lt;- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\n\nCode\nb_weights &lt;- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 &lt;- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\n\nCode\nlag_sum &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res &lt;- as.data.frame(lag_sum)\ncolnames(lag.res) &lt;- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: Can you understand the meaning of Spatial lag as a sum of neighboring values now?\n\n\nNext, we will append the lag_sum GDPPC field into hunan sf data frame by using the code chunk below.\n\n\nCode\nhunan &lt;- left_join(hunan, lag.res)\n\n\nNow, We can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc &lt;- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\nThe spatial window average uses row-standardized weights and includes the diagonal element. To do this in R, we need to go back to the neighbors structure and add the diagonal element before assigning weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode\nwm_qs &lt;- include.self(wm_q)\n\n\nNotice that the Number of nonzero links, Percentage nonzero weights and Average number of links are 536, 6.921488 and 6.090909 respectively as compared to wm_q of 448, 5.785124 and 5.090909\nLet us take a good look at the neighbour list of area [1] by using the code chunk below.\n\n\nCode\nwm_qs[[1]]\n\n\n[1]  1  2  3  4 57 85\n\n\nNotice that now [1] has six neighbours instead of five. Now we obtain weights with nb2listw()\n\n\nCode\nwm_qs &lt;- nb2listw(wm_qs)\nwm_qs\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 30.90265 357.5308\n\n\nAgain, we use nb2listw() and glist() to explicitly assign weight values.\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\n\nCode\nlag_w_avg_gpdpc &lt;- lag.listw(wm_qs, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n\n [1] 24650.50 22434.17 26233.00 27084.60 26927.00 22230.17 47621.20 37160.12\n [9] 49224.71 29886.89 26627.50 22690.17 25366.40 25825.75 30329.00 32682.83\n[17] 25948.62 23987.67 25463.14 21904.38 23127.50 25949.83 20018.75 19524.17\n[25] 18955.00 17800.40 15883.00 18831.33 14832.50 17965.00 17159.89 16199.44\n[33] 18764.50 26878.75 23188.86 20788.14 12365.20 15985.00 13764.83 11907.43\n[41] 17128.14 14593.62 11644.29 12706.00 21712.29 43548.25 35049.00 16226.83\n[49] 19294.40 18156.00 19954.75 18145.17 12132.75 18419.29 14050.83 23619.75\n[57] 24552.71 24733.67 16762.60 20932.60 19467.75 18334.00 22541.00 26028.00\n[65] 29128.50 46569.00 47576.60 36545.50 20838.50 22531.00 42115.50 27619.00\n[73] 27611.33 44523.29 18127.43 28746.38 20734.50 33880.62 14716.38 28516.22\n[81] 18086.14 21244.50 29568.80 48119.71 22310.75 43151.60 17133.40 17009.33\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode\nlag.list.wm_qs &lt;- list(hunan$NAME_3, lag.listw(wm_qs, hunan$GDPPC))\nlag_wm_qs.res &lt;- as.data.frame(lag.list.wm_qs)\ncolnames(lag_wm_qs.res) &lt;- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\n\nNote: The third command line on the code chunk above renames the field names of lag_wm_q1.res object into NAME_3 and lag_window_avg GDPPC respectively.\nNext, the code chunk below will be used to append lag_window_avg GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan, lag_wm_qs.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below. Let’s check the first 10 rows.\n\n\nCode\nhunan %&gt;%\n  select(\"County\", \"lag GDPPC\", \"lag_window_avg GDPPC\") %&gt;%\n  head(10) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\nCounty\nlag GDPPC\nlag_window_avg GDPPC\ngeometry\n\n\n\n\nAnxiang\n24847.20\n24650.50\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n22724.80\n22434.17\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n24143.25\n26233.00\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n27737.50\n27084.60\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n27270.25\n26927.00\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n21248.80\n22230.17\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n43747.00\n47621.20\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n33582.71\n37160.12\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n45651.17\n49224.71\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n32027.62\n29886.89\nPOLYGON ((113.1757 26.82734…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_gdppc and w_ave_gdppc maps next to each other for quick comparison.\n\n\nCode\nw_avg_gdppc &lt;- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(lag_gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\nFor more effective comparison, it is advicible to use the core tmap mapping functions.\n\n\n\n\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nTo add the diagonal element to the neighbour list, we just need to use include.self() from spdep.\n\n\nCode\nwm_qs &lt;- include.self(wm_q)\nwm_qs\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\n\nNext, we will assign binary weights to the neighbour structure that includes the diagonal element.\n\n\nCode\nb_weights &lt;- lapply(wm_qs, function(x) 0*x + 1)\nb_weights[1]\n\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nNotice that now [1] has six neighbours instead of five. Again, we use nb2listw() and glist() to explicitly assign weight values.\n\n\nCode\nb_weights2 &lt;- nb2listw(wm_qs, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 536 1072 14160\n\n\nWith our new weight structure, we can compute the lag variable with lag.listw().\n\n\nCode\nw_sum_gdppc &lt;- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 147903 134605 131165 135423 134635 133381 238106 297281 344573 268982\n[11] 106510 136141 126832 103303 151645 196097 207589 143926 178242 175235\n[21] 138765 155699 160150 117145 113730  89002  63532 112988  59330  35930\n[31] 154439 145795 112587 107515 162322 145517  61826  79925  82589  83352\n[41] 119897 116749  81510  63530 151986 174193 210294  97361  96472 108936\n[51]  79819 108871  48531 128935  84305 188958 171869 148402  83813 104663\n[61] 155742  73336 112705  78084  58257 279414 237883 219273  83354  90124\n[71] 168462 165714 165668 311663 126892 229971 165876 271045 117731 256646\n[81] 126603 127467 295688 336838 267729 431516  85667  51028\n\n\nNext, we will convert the lag variable listw object into a data.frame by using as.data.frame().\n\n\nCode\nw_sum_gdppc.res &lt;- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) &lt;- c(\"NAME_3\", \"w_sum GDPPC\")\n\n\nNote: The second command line on the code chunk above renames the field names of w_sum_gdppc.res object into NAME_3 and w_sum GDPPC respectively.\nNext, the code chunk below will be used to append w_sum GDPPC values onto hunan sf data.frame by using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan, w_sum_gdppc.res)\n\n\nTo compare the values of lag GDPPC and Spatial window average, kable() of Knitr package is used to prepare a table using the code chunk below.\n\n\nCode\nhunan %&gt;%\n  select(\"County\", \"lag_sum GDPPC\", \"w_sum GDPPC\") %&gt;%\n  head(10) %&gt;%\n  kable()\n\n\n\n\n\nCounty\nlag_sum GDPPC\nw_sum GDPPC\ngeometry\n\n\n\n\nAnxiang\n124236\n147903\nPOLYGON ((112.0625 29.75523…\n\n\nHanshou\n113624\n134605\nPOLYGON ((112.2288 29.11684…\n\n\nJinshi\n96573\n131165\nPOLYGON ((111.8927 29.6013,…\n\n\nLi\n110950\n135423\nPOLYGON ((111.3731 29.94649…\n\n\nLinli\n109081\n134635\nPOLYGON ((111.6324 29.76288…\n\n\nShimen\n106244\n133381\nPOLYGON ((110.8825 30.11675…\n\n\nLiuyang\n174988\n238106\nPOLYGON ((113.9905 28.5682,…\n\n\nNingxiang\n235079\n297281\nPOLYGON ((112.7181 28.38299…\n\n\nWangcheng\n273907\n344573\nPOLYGON ((112.7914 28.52688…\n\n\nAnren\n256221\n268982\nPOLYGON ((113.1757 26.82734…\n\n\n\n\n\nLastly, qtm() of tmap package is used to plot the lag_sum GDPPC and w_sum_gdppc maps next to each other for quick comparison.\n\n\nCode\nw_sum_gdppc &lt;- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(lag_sum_gdppc, w_sum_gdppc, asp=1, ncol=2)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#importing-data-into-r",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#importing-data-into-r",
    "title": "Hands on Exercise 2",
    "section": "1 Importing Data into R",
    "text": "1 Importing Data into R\nIn this section, you will learn how to bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n1.1 Import shapefile into r environment\nThe code chunk below uses st_read() of sf package to import Hunan shapefile into R. The imported shapefile will be simple features Object of sf.\n\n\nCode\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Handson_Ex/Handson_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\n\n\n1.2 Import csv file into r environment\nNext, we will import Hunan_2012.csv into R by using read_csv() of readr package. The output is R data frame class.\n\n\nCode\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\n\nCode\nhunan &lt;- left_join(hunan,hunan2012) %&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n1.3 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\n\nCode\nequal &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\")\n\nquantile &lt;- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal quantile classification\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#global-spatial-autocorrelation",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#global-spatial-autocorrelation",
    "title": "Hands on Exercise 2",
    "section": "2 Global Spatial Autocorrelation",
    "text": "2 Global Spatial Autocorrelation\nIn this section, you will learn how to compute global spatial autocorrelation statistics and to perform spatial complete randomness test for global spatial autocorrelation.\n\n2.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area.\nIn the code chunk below, poly2nb() of spdep package is used to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries. If you look at the documentation you will see that you can pass a “queen” argument that takes TRUE or FALSE as options. If you do not specify this argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nMore specifically, the code chunk below is used to compute Queen contiguity weight matrix.\n\n\nCode\nwm_q &lt;- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\n\n\n2.2 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”). This is accomplished by assigning the fraction 1/(#ofneighbors) to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the style=“W” option for simplicity’s sake but note that other more robust options are available, notably style=“B”.\n\n\nCode\nrswm_q &lt;- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nThe input of nb2listw() must be an object of class nb. The syntax of the function has two major arguments, namely style and zero.poly.\n\nstyle can take values “W”, “B”, “C”, “U”, “minmax” and “S”. B is the basic binary coding, W is row standardised (sums over all links to n), C is globally standardised (sums over all links to n), U is equal to C divided by the number of neighbours (sums over all links to unity), while S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).\nIf zero policy is set to TRUE, weights vectors of zero length are inserted for regions without neighbour in the neighbours list. These will in turn generate lag values of zero, equivalent to the sum of products of the zero row t(rep(0, length=length(neighbours))) %*% x, for arbitrary numerical vector x of length length(neighbours). The spatially lagged value of x for the zero-neighbour region will then be zero, which may (or may not) be a sensible choice.\n\n\n\n2.3 Moran’s I\nIn this section, you will learn how to perform Moran’s I statistics testing by using moran.test() of spdep.\n\n2.3.1 Maron’s I test\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\n\nCode\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nFrom ChatGPT:\nMoran’s I is a statistical measure used in spatial analysis to assess the spatial autocorrelation of data. Spatial autocorrelation refers to the relationship between the values of a variable in geographic space. In simpler terms, it explores whether nearby locations are similar or dissimilar in terms of a particular attribute or variable.\nMoran’s I statistic quantifies the degree of spatial clustering or dispersion in a dataset. It measures the correlation between the values of a variable at different locations. The values can range from -1 to 1, where:\n\nMoran’s I close to +1 indicates positive spatial autocorrelation, suggesting that similar values tend to cluster together in space. High values are indicative of a clustering pattern.\nMoran’s I close to -1 indicates negative spatial autocorrelation, suggesting dissimilar values tend to be close together in space. Low values are indicative of a dispersion or scattering pattern.\nMoran’s I close to 0 suggests no spatial autocorrelation, implying a random spatial pattern or no relationship between neighboring values.\n\nAs I understand, since the Moron I statistics standard deviate of 4.7351 indicates that the observed Moran’s I value (0.300749970) is almost 4.7 standard deviations away from what would be expected under the null hypothesis. And p-value is very close to 0 suggesting to reject null hypothesis, indicating a significant positive spatial autocorrelation in our graph.\n\n\n\n\n2.3.2 Computing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\n\nCode\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: What statistical conclustion can you draw from the output above?\nFrom ChaptGPT:\nThe Monte Carlo simulation of Moran’s I is a method used to assess the statistical significance of Moran’s I statistic by generating random spatial patterns through repeated simulations. This technique helps determine whether the observed spatial pattern in a dataset is significantly different from what would be expected under a null hypothesis of spatial randomness.\nAs I understand, this output shows a p-value close to 0, we should reject the null hypothesis again, and be confident in our conclusion that there is a statistically significant positive spatial autocorrelation in our graph.\n\n\n\n\n2.3.3 Visualising Monte Carlo Moran’s I\nIt is always a good practice for us the examine the simulated Moran’s I test statistics in greater detail. This can be achieved by plotting the distribution of the statistical values as a histogram by using the code chunk below.\nIn the code chunk below hist() and abline() of R Graphics are used.\n\n\nCode\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\n\nDIY: Using ggplot2 to plot.\n\n\nCode\nres = as.data.frame(bperm$res)\nggplot(res, \n       aes(x= bperm$res))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  geom_vline(aes(xintercept = 0, color = \"red\"), show.legend = FALSE) +\n  labs(title = \"Distribution of Moran’s I Simulation\",\n      x = \"Simulated Moran's I\",\n      y = \"Frequency\") +\n  theme(panel.background = element_blank(), panel.grid = element_blank())\n\n\n\n\n\n\n\n\n2.4 Geary’s\nIn this section, you will learn how to perform Geary’s c statistics testing by using appropriate functions of spdep package.\n\n2.4.1 Geary’s C test\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\n\nCode\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion: What statistical conclusion can you draw from the output above?\nFrom ChatGPT:\nGeary’s C statistic measures spatial autocorrelation by comparing the differences between values at neighboring locations to the overall mean difference in the entire dataset. It can be used to detect both positive and negative spatial autocorrelation, indicating clustering or dispersion of attribute values across space.\nGeary’s C values range from 0 to infinity:\n\nC&lt;1 suggests positive spatial autocorrelation (similar values are close together).\nC&gt;1 suggests negative spatial autocorrelation (dissimilar values are close together).\nC=1 implies no spatial autocorrelation (values are randomly distributed across space).\n\nAs I understand, since this test output 0.69 Geary’s C statistic which is &lt;1, and very small p-value suggesting reject of null hypothesis, indicating there is autocorrelation in our graph and it’s positive.\n\n\n\n\n2.4.2 Computing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\n\nCode\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\n\n\n\n\n\n\nTip\n\n\n\nAccording to the very small p-value, we can safely reject the null hypothesis and trust our result from Geary C test.\n\n\n\n\n2.4.3 Visualising the Monte Carlo Geary’s C\n\n\nCode\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#spatial-correlogram",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#spatial-correlogram",
    "title": "Hands on Exercise 2",
    "section": "3 Spatial Correlogram",
    "text": "3 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\n\n3.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I. The plot() of base Graph is then used to plot the output.\n\n\nCode\nMI_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\n\n\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\n\nCode\nprint(MI_corr)\n\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nTip\n\n\n\nQuestion:What statistical observation can you draw from the plot above?\nNot all autocorrelation values are statistically significant.\n\n\n\n\n3.2 Compute Geary’s C correlogram and plot\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Geary’s C. The plot() of base Graph is then used to plot the output.\n\n\nCode\nGC_corr &lt;- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\n\n\nSimilar to the previous step, we will print out the analysis report by using the code chunk below.\n\n\nCode\nprint(GC_corr)\n\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#cluster-and-outlier-analysis",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#cluster-and-outlier-analysis",
    "title": "Hands on Exercise 2",
    "section": "1 Cluster and Outlier Analysis",
    "text": "1 Cluster and Outlier Analysis\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable. For instance if we are studying cancer rates among census tracts in a given city local clusters in the rates mean that there are areas that have higher or lower rates than is to be expected by chance alone; that is, the values occurring are above or below those of a random distribution in space.\nIn this section, you will learn how to apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from GDP per capita 2012 of Hunan Province, PRC."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#computing-local-morans-i",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#computing-local-morans-i",
    "title": "Hands on Exercise 2",
    "section": "2 Computing local Moran’s I",
    "text": "2 Computing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\n\nCode\nfips &lt;- order(hunan$County)\nlocalMI &lt;- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\n\nCode\nprintCoefmat(data.frame(\n  localMI[fips,], \n  row.names=hunan$County[fips]),\n  check.names=FALSE)\n\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\n\n2.1 Mapping the local Moran’s I\nBefore mapping the local Moran’s I map, it is wise to append the local Moran’s I dataframe (i.e. localMI) onto hunan SpatialPolygonDataFrame. The code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\n\nCode\nhunan.localMI &lt;- cbind(hunan,localMI) %&gt;%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n\n2.2 Mapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\n\nCode\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n2.3 Mapping local Moran’s I p-values\nThe choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\n\nCode\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n2.4 Mapping both local Moran’s I values and p-values\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\n\nCode\nlocalMI.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)\n\npvalue.map &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#creating-a-lisa-cluster-map",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#creating-a-lisa-cluster-map",
    "title": "Hands on Exercise 2",
    "section": "3 Creating a LISA Cluster Map",
    "text": "3 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n3.1 Plotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\n\nCode\nnci &lt;- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\nNotice that the plot is split in 4 quadrants. The top right corner belongs to areas that have high GDPPC and are surrounded by other areas that have the average level of GDPPC. This are the high-high locations in the lesson slide.\n\n\n3.2 Plotting Moran scatterplot with standardised variable\nFirst we will use scale() to centers and scales the variable. Here centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\n\nCode\nhunan$Z.GDPPC &lt;- scale(hunan$GDPPC) %&gt;% \n  as.vector \n\n\nThe as.vector() added to the end is to make sure that the data type we get out of this is a vector, that map neatly into out dataframe.\nNow, we are ready to plot the Moran scatterplot again by using the code chunk below.\n\n\nCode\nnci2 &lt;- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n\n3.3 Preparing LISA map classes\nThe code chunks below show the steps to prepare a LISA cluster map.\n\n\nCode\nquadrant &lt;- vector(mode=\"numeric\",length=nrow(localMI))\n\n\nNext, derives the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\n\n\nCode\nhunan$lag_GDPPC &lt;- lag.listw(rswm_q, hunan$GDPPC)\nDV &lt;- hunan$lag_GDPPC - mean(hunan$lag_GDPPC)     \n\n\nThis is follow by centering the local Moran’s around the mean.\n\n\nCode\nLM_I &lt;- localMI[,1] - mean(localMI[,1])    \n\n\nNext, we will set a statistical significance level for the local Moran.\n\n\nCode\nsignif &lt;- 0.05       \n\n\nThese four command lines define the low-low (1), low-high (2), high-low (3) and high-high (4) categories.\n\n\nCode\nquadrant[DV &lt;0 & LM_I&gt;0] &lt;- 1\nquadrant[DV &gt;0 & LM_I&lt;0] &lt;- 2\nquadrant[DV &lt;0 & LM_I&lt;0] &lt;- 3  \nquadrant[DV &gt;0 & LM_I&gt;0] &lt;- 4      \n\n\nLastly, places non-significant Moran in the category 0.\n\n\nCode\nquadrant[localMI[,5]&gt;signif] &lt;- 0\n\n\n\n\n3.4 Plotting LISA map\nNow, we can build the LISA map by using the code chunks below.\n\n\nCode\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\n\n\n\n\nFor effective interpretation, it is better to plot both the local Moran’s I values map and its corresponding p-values map next to each other.\nThe code chunk below will be used to create such visualisation.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant &lt;- quadrant\ncolors &lt;- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters &lt;- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap &lt;- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)\n\ntmap_arrange(gdppc, LISAmap, \n             asp=1, ncol=2)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#hot-spot-and-cold-spot-area-analysis",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Hands on Exercise 2",
    "section": "4 Hot Spot and Cold Spot Area Analysis",
    "text": "4 Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\nThe term ‘hot spot’ has been used generically across disciplines to describe a region or value that is higher relative to its surroundings (Lepers et al 2005, Aben et al 2012, Isobe et al 2015).\n\n4.1 Getis and Ord’s G-Statistics\nAn alternative spatial statistics to detect spatial anomalies is the Getis and Ord’s G-statistics (Getis and Ord, 1972; Ord and Getis, 1995). It looks at neighbours within a defined proximity to identify where either high or low values clutser spatially. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\n\n\n4.2 Deriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix; and\nadaptive distance weight matrix.\n\n\n4.2.1 Deriving the centroid\nWe will need points to associate with each polygon before we can make our connectivity graph. It will be a little more complicated than just running st_centroid() on the sf object: us.bound. We need the coordinates in a separate data frame for this to work. To do this we will use a mapping function. The mapping function applies a given function to each element of a vector and returns a vector of the same length. Our input vector will be the geometry column of us.bound. Our function will be st_centroid(). We will be using map_dbl variation of map from the purrr package. For more documentation, check out map documentation\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound and access the longitude value through double bracket notation [[]] and 1. This allows us to get only the longitude, which is the first value in each centroid.\n\n\nCode\nlongitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\n\n\nWe do the same for latitude with one key difference. We access the second value per each centroid with [[2]].\n\n\nCode\nlatitude &lt;- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\n\n\nNow that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\n\nCode\ncoords &lt;- cbind(longitude, latitude)\n\n\n\n\n4.2.2 Determine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n\nCode\n#coords &lt;- coordinates(hunan)\nk1 &lt;- knn2nb(knearneigh(coords))\nk1dists &lt;- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n4.2.3 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\n\nCode\nwm_d62 &lt;- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\n\nCode\nwm62_lw &lt;- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\n\n4.3 Computing adaptive distance weight matrix\nOne of the characteristics of fixed distance weight matrix is that more densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours.\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\n\nCode\nknn &lt;- knn2nb(knearneigh(coords, k=8))\nknn\n\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nNext, nb2listw() is used to convert the nb object into spatial weights object.\n\n\nCode\nknn_lw &lt;- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#computing-gi-statistics",
    "href": "Handson_Ex/Handson_Ex02/Handson_Ex02.html#computing-gi-statistics",
    "title": "Hands on Exercise 2",
    "section": "5 Computing Gi statistics",
    "text": "5 Computing Gi statistics\n\n5.1 Gi statistics using fixed distance\n\n\nCode\nfips &lt;- order(hunan$County)\ngi.fixed &lt;- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"internals\")\n               Gi      E(Gi)        V(Gi)        Z(Gi) Pr(z != E(Gi))\n [1,] 0.064192949 0.05747126 2.375922e-04  0.436075843   6.627817e-01\n [2,] 0.042300020 0.04597701 1.917951e-04 -0.265505650   7.906200e-01\n [3,] 0.044961480 0.04597701 1.933486e-04 -0.073033665   9.417793e-01\n [4,] 0.039475779 0.03448276 1.461473e-04  0.413017033   6.795941e-01\n [5,] 0.049767939 0.04597701 1.927263e-04  0.273070579   7.847990e-01\n [6,] 0.008825335 0.01149425 4.998177e-05 -0.377510776   7.057941e-01\n [7,] 0.050807266 0.02298851 9.435398e-05  2.863898821   4.184617e-03\n [8,] 0.083966739 0.04597701 1.848292e-04  2.794350420   5.200409e-03\n [9,] 0.115751554 0.04597701 1.789361e-04  5.216125401   1.827045e-07\n[10,] 0.049115587 0.04597701 1.891013e-04  0.228236603   8.194623e-01\n[11,] 0.045819180 0.03448276 1.420884e-04  0.951035346   3.415864e-01\n[12,] 0.049183846 0.05747126 2.387633e-04 -0.536334231   5.917276e-01\n[13,] 0.048429181 0.04597701 1.924532e-04  0.176761556   8.596957e-01\n[14,] 0.034733752 0.02298851 9.651140e-05  1.195564020   2.318667e-01\n[15,] 0.011262043 0.01149425 4.945294e-05 -0.033020610   9.736582e-01\n[16,] 0.065131196 0.04597701 1.931870e-04  1.378081093   1.681783e-01\n[17,] 0.027587075 0.03448276 1.385862e-04 -0.585756761   5.580390e-01\n[18,] 0.029409313 0.03448276 1.461397e-04 -0.419680565   6.747188e-01\n[19,] 0.061466754 0.05747126 2.383385e-04  0.258805141   7.957856e-01\n[20,] 0.057656917 0.05747126 2.371303e-04  0.012056111   9.903808e-01\n[21,] 0.066518379 0.06896552 2.820326e-04 -0.145716531   8.841452e-01\n[22,] 0.045599896 0.04597701 1.928108e-04 -0.027158687   9.783332e-01\n[23,] 0.030646753 0.03448276 1.449523e-04 -0.318615290   7.500183e-01\n[24,] 0.035635552 0.04597701 1.906613e-04 -0.748946051   4.538897e-01\n[25,] 0.032606647 0.04597701 1.932888e-04 -0.961700582   3.362000e-01\n[26,] 0.035001352 0.04597701 1.897172e-04 -0.796851342   4.255374e-01\n[27,] 0.012746354 0.02298851 9.812587e-05 -1.033949773   3.011596e-01\n[28,] 0.061287917 0.06896552 2.773884e-04 -0.460979158   6.448136e-01\n[29,] 0.014277403 0.02298851 9.683314e-05 -0.885240161   3.760271e-01\n[30,] 0.009622875 0.01149425 4.924586e-05 -0.266671512   7.897221e-01\n[31,] 0.014258398 0.02298851 9.705244e-05 -0.886168613   3.755267e-01\n[32,] 0.005453443 0.01149425 4.986245e-05 -0.855476971   3.922871e-01\n[33,] 0.043283712 0.05747126 2.367109e-04 -0.922143185   3.564539e-01\n[34,] 0.020763514 0.03448276 1.393165e-04 -1.162328599   2.451020e-01\n[35,] 0.081261843 0.06896552 2.794398e-04  0.735582222   4.619850e-01\n[36,] 0.057419907 0.05747126 2.338437e-04 -0.003358489   9.973203e-01\n[37,] 0.013497133 0.02298851 9.624821e-05 -0.967459309   3.333145e-01\n[38,] 0.019289310 0.03448276 1.455643e-04 -1.259299080   2.079223e-01\n[39,] 0.025996272 0.04597701 1.892938e-04 -1.452256513   1.464303e-01\n[40,] 0.016092694 0.03448276 1.424776e-04 -1.540671121   1.233968e-01\n[41,] 0.035952614 0.05747126 2.379439e-04 -1.395011407   1.630124e-01\n[42,] 0.031690963 0.05747126 2.350604e-04 -1.681505286   9.266481e-02\n[43,] 0.018750079 0.03448276 1.433314e-04 -1.314110709   1.888090e-01\n[44,] 0.015449080 0.02298851 9.638666e-05 -0.767944457   4.425202e-01\n[45,] 0.065760689 0.06896552 2.760533e-04 -0.192889342   8.470456e-01\n[46,] 0.098966900 0.05747126 2.326002e-04  2.720804542   6.512325e-03\n[47,] 0.085415780 0.05747126 2.385746e-04  1.809191360   7.042128e-02\n[48,] 0.038816536 0.05747126 2.343951e-04 -1.218469473   2.230456e-01\n[49,] 0.038931873 0.04597701 1.893501e-04 -0.511984469   6.086619e-01\n[50,] 0.055098610 0.06896552 2.760948e-04 -0.834546363   4.039732e-01\n[51,] 0.033405005 0.04597701 1.916312e-04 -0.908179070   3.637836e-01\n[52,] 0.043040784 0.06896552 2.829941e-04 -1.541081516   1.232969e-01\n[53,] 0.011297699 0.02298851 9.615920e-05 -1.192199867   2.331829e-01\n[54,] 0.040968457 0.05747126 2.356318e-04 -1.075080164   2.823388e-01\n[55,] 0.023629663 0.04597701 1.877170e-04 -1.631075961   1.028743e-01\n[56,] 0.006281129 0.01149425 4.916619e-05 -0.743472246   4.571958e-01\n[57,] 0.063918654 0.05747126 2.369553e-04  0.418842387   6.753313e-01\n[58,] 0.070325003 0.05747126 2.381374e-04  0.832943753   4.048765e-01\n[59,] 0.025947288 0.03448276 1.444058e-04 -0.710289083   4.775249e-01\n[60,] 0.039752578 0.04597701 1.915656e-04 -0.449718820   6.529132e-01\n[61,] 0.049934283 0.05747126 2.334965e-04 -0.493238743   6.218439e-01\n[62,] 0.030964195 0.04597701 1.920248e-04 -1.083386776   2.786368e-01\n[63,] 0.058129184 0.05747126 2.343319e-04  0.042979051   9.657182e-01\n[64,] 0.046096514 0.04597701 1.932637e-04  0.008596093   9.931414e-01\n[65,] 0.012459080 0.01149425 5.008051e-05  0.136337469   8.915545e-01\n[66,] 0.091447733 0.05747126 2.377744e-04  2.203411744   2.756574e-02\n[67,] 0.049575872 0.02298851 9.766513e-05  2.690329952   7.138140e-03\n[68,] 0.107907212 0.04597701 1.933581e-04  4.453703219   8.440175e-06\n[69,] 0.019616151 0.02298851 9.789454e-05 -0.340842743   7.332220e-01\n[70,] 0.032923393 0.03448276 1.454032e-04 -0.129318589   8.971056e-01\n[71,] 0.030317663 0.02298851 9.867859e-05  0.737806634   4.606320e-01\n[72,] 0.019437582 0.03448276 1.455870e-04 -1.246912658   2.124295e-01\n[73,] 0.055245460 0.04597701 1.932838e-04  0.666667559   5.049845e-01\n[74,] 0.074278054 0.05747126 2.383538e-04  1.088613505   2.763244e-01\n[75,] 0.013269580 0.02298851 9.719982e-05 -0.985792573   3.242349e-01\n[76,] 0.049407829 0.03448276 1.463785e-04  1.233609606   2.173484e-01\n[77,] 0.028605749 0.03448276 1.455139e-04 -0.487196415   6.261191e-01\n[78,] 0.039087662 0.02298851 9.801040e-05  1.626174042   1.039126e-01\n[79,] 0.031447120 0.04597701 1.877464e-04 -1.060416797   2.889550e-01\n[80,] 0.064005294 0.05747126 2.359641e-04  0.425361422   6.705732e-01\n[81,] 0.044606529 0.05747126 2.357330e-04 -0.837897118   4.020885e-01\n[82,] 0.063700493 0.06896552 2.801427e-04 -0.314565243   7.530918e-01\n[83,] 0.051142205 0.04597701 1.933560e-04  0.371456331   7.102977e-01\n[84,] 0.102121112 0.04597701 1.610278e-04  4.424392623   9.671399e-06\n[85,] 0.021901462 0.02298851 9.843172e-05 -0.109566928   9.127528e-01\n[86,] 0.064931813 0.04597701 1.929430e-04  1.364597995   1.723794e-01\n[87,] 0.031747344 0.04597701 1.909867e-04 -1.029658605   3.031703e-01\n[88,] 0.015893319 0.02298851 9.765131e-05 -0.718000620   4.727569e-01\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\n\nCode\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.fixed)) %&gt;%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\n\n\n5.2 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\n\nCode\ngdppc &lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\n\n\n\n2.3 Gi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\n\nCode\nfips &lt;- order(hunan$County)\ngi.adaptive &lt;- localG(hunan$GDPPC, knn_lw)\nhunan.gi &lt;- cbind(hunan, as.matrix(gi.adaptive)) %&gt;%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n\n2.4 Mapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using adaptive distance weight matrix.\n\n\nCode\ngdppc&lt;- qtm(hunan, \"GDPPC\")\n\nGimap &lt;- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624 Applied Geospatial Analytics",
    "section": "",
    "text": "This is a Quarto website to introduce you about ISSS624 Applied Geospatial Analytics."
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html",
    "href": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data.\n\n\n\nThis take-home exercise is motivated by two main reasons. Firstly, despite increasing amounts of open data available for public consumption, there has not been significant practice research carried out to show how these disparate data sources can be integrated, analysed, and modelled to support policy making decisions.\nSecondly, there is a general lack of practical research to show how geospatial data science and analysis (GDSA) can be used to support decision-making.\nHence, your task for this take-home exercise is to conduct a case study to demonstrate the potential value of GDSA to integrate publicly available data from multiple sources for building a spatial interaction models to determine factors affecting urban mobility patterns of public bus transit.\n\n\n\n\n\nFor the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg.\n\n\n\n\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide.\n\n\n\n\n\n\n\nImportant\n\n\n\nThose specially collected data aim to use within this excercise content only, if intend to put in other usage, approach course instructor Dr. Kam Tin Seong and ask for permission first.\n\n\nFor starting, load needing packages.\n\n\nCode\npacman::p_load(sf, sp, spdep, tmap, tidyverse, sfdep, stplanr, reshape2, ggpubr, DT)\n\n\n\n\n\n\n\n\n\n\nDerive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nFirst of all, we need to import Bus Stop Location from LTA DataMall.\n\n\nCode\nbusstop = st_read(dsn = \"./data/geospatial/BusStopLocation_Jul2023\",\n                   layer = \"BusStop\")  %&gt;% st_transform(crs = 3414) %&gt;% \n  distinct(BUS_STOP_N, .keep_all = TRUE)\n\n\nReading layer `BusStop' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial/BusStopLocation_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nAlso import subzone geometry data as our background layer.\n\n\nCode\nsz = st_read(dsn = \"./data/geospatial\",\n                   layer = \"MP14_SUBZONE_WEB_PL\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThen we can Derive an analytical hexagon data of 375m.\n\n\nCode\nhexagon &lt;- st_sf(geometry = st_make_grid(busstop, cellsize = c(375,375), what = \"polygons\",square = FALSE)) %&gt;%\n  mutate(id = row_number()) %&gt;% \n  st_transform(crs = 3414) \n\n\nThen we can join bus stop with hexagon, and join with subzone to exclude hexagons out of range.\n\n\nCode\nbus_hex &lt;- st_join(\n  st_join(hexagon, busstop%&gt;%select(BUS_STOP_N,geometry), join = st_intersects),\n  sz) %&gt;%\n  drop_na() %&gt;%\n  distinct(BUS_STOP_N, .keep_all = TRUE)\n\n\nCheck for duplicate geometry.\n\n\nCode\nbus_hex %&gt;%\n  group_by(geometry)%&gt;%\n  filter(row_number()&gt;1)\n\n\n\n\n\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nImport bus passenger trips data.\n\n\nCode\nodbus = read_csv(\"./data/aspatial/origin_destination_bus_202310.csv\")  %&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),\n         DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))\n\n\nExtract passenger trips data during all peak time intervals.\n\n\nCode\npeak_trips &lt;- bind_rows(\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n    mutate(interval = \"weekdayam\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 &\n           TIME_PER_HOUR &lt;= 20) %&gt;%\n    mutate(interval = \"weekdaypm\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 &\n           TIME_PER_HOUR &lt;= 14) %&gt;%\n    mutate(interval = \"weekendam\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n    mutate(interval = \"weekendpm\")) %&gt;%\n  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE, interval) %&gt;%\n  reframe(TRIPS = sum(TOTAL_TRIPS)) \n\nglimpse(peak_trips)\n\n\nRows: 951,259\nColumns: 4\n$ ORIGIN_PT_CODE      &lt;fct&gt; 01012, 01012, 01012, 01012, 01012, 01012, 01012, 0…\n$ DESTINATION_PT_CODE &lt;fct&gt; 01112, 01112, 01112, 01112, 01113, 01113, 01113, 0…\n$ interval            &lt;chr&gt; \"weekdayam\", \"weekdaypm\", \"weekendam\", \"weekendpm\"…\n$ TRIPS               &lt;dbl&gt; 290, 540, 265, 201, 118, 516, 189, 165, 77, 303, 1…\n\n\nCheck any bus stops not in our origin ‘bus_hex’ list.\n\n\nCode\npeak_trips %&gt;%\n  filter(! ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  reframe(TRIPS = sum(TRIPS))\n\n\nExclude any bus stops not included in ‘bus_hex’ data before continue.\n\n\nCode\npeak_trips &lt;- peak_trips %&gt;%\n  filter(ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %&gt;%\n  filter(DESTINATION_PT_CODE %in% bus_hex$'BUS_STOP_N')\n\n\nDuplication check before continue.\n\n\nCode\npeak_trips %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nAfter that, we need to combine those passenger trip data with geospatial data by origin bus stops.\n\n\nCode\npeaktrip_hex &lt;- left_join(peak_trips %&gt;% \n                            group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %&gt;%\n                            reframe(TRIPS = sum(TRIPS)),\n                          bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\"))  %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         DESTIN_BS = DESTINATION_PT_CODE) \n\n\nDuplication check before continue:\n\n\nCode\npeaktrip_hex %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nThen we can continue to join again with geospatial data by destination bus stops.\n\n\nCode\npeaktrip_hex &lt;- left_join(peaktrip_hex, bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"DESTIN_BS\" = \"BUS_STOP_N\"),\n                          suffix = c(\".origin\", \".destin\")) \n\n\nDuplication check again.\n\n\nCode\npeaktrip_hex %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\n# A tibble: 0 × 5\n# ℹ 5 variables: ORIGIN_BS &lt;chr&gt;, DESTIN_BS &lt;chr&gt;, TRIPS &lt;dbl&gt;,\n#   geometry.origin &lt;GEOMETRY [m]&gt;, geometry.destin &lt;GEOMETRY [m]&gt;\n\n\nWe can save the output into a rds file.\n\n\nCode\nwrite_rds(peaktrip_hex, \"./data/rds/peaktrip_hex.rds\")\n\n\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods.\nFirst let’s ensure there aren’t any observations with same origin and destination.\n\n\nCode\npeaktrip_hex %&gt;%\n  filter(ORIGIN_BS==DESTIN_BS)\n\n\nThen we can create flow lines and check summary of data in case there are any zero.\n\n\nCode\npeaktrip_flow &lt;- od2line(flow = peaktrip_hex, \n                    zones = bus_hex,\n                    zone_code = \"BUS_STOP_N\")\n\nsummary(peaktrip_flow)\n\n\n  ORIGIN_BS          DESTIN_BS             TRIPS         \n Length:306940      Length:306940      Min.   :     1.0  \n Class :character   Class :character   1st Qu.:     6.0  \n Mode  :character   Mode  :character   Median :    27.0  \n                                       Mean   :   209.1  \n                                       3rd Qu.:   112.0  \n                                       Max.   :153711.0  \n      geometry.origin        geometry.destin            geometry     \n POLYGON      :306940   POLYGON      :306940   LINESTRING   :306940  \n epsg:3414    :     0   epsg:3414    :     0   epsg:3414    :     0  \n +proj=tmer...:     0   +proj=tmer...:     0   +proj=tmer...:     0  \n                                                                     \n                                                                     \n                                                                     \n\n\nTill now, we can plot out the bus trip flow during all 4 peak time intervals in total.\n\n\nCode\ntm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeaktrip_flow %&gt;% \n  filter(TRIPS &gt;= 2000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"TRIPS\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5)\n\n\n\n\n\nAnd we can visualize 4 peak time intervals in facets style to check any difference within.\nFirst need to wrangling the data to put trips data of different time intervals into different columns.\n\n\nCode\npeak_interval_trips &lt;- peak_trips %&gt;%\n  pivot_wider(names_from = interval, \n              values_from = TRIPS, \n              values_fill = 0)\n\npeak_interval_hex &lt;- left_join(peak_interval_trips,\n                          bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\"))  %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         DESTIN_BS = DESTINATION_PT_CODE) \n\n\npeak_interval_hex &lt;- left_join(peak_interval_hex, bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"DESTIN_BS\" = \"BUS_STOP_N\"),\n                          suffix = c(\".origin\", \".destin\")) \n\n\nBefore we continue, we can check duplication and save the result as a rds file.\n\n\nCode\npeak_interval_hex %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\n\n\nCode\nwrite_rds(peak_interval_hex, \"./data/rds/peak_interval_hex.rds\")\n\n\nThen we can create flow lines.\n\n\nCode\npeak_interval_flow &lt;- od2line(flow = peak_interval_hex, \n                    zones = bus_hex,\n                    zone_code = \"BUS_STOP_N\")\n\nsummary(peak_interval_flow)\n\n\n  ORIGIN_BS          DESTIN_BS           weekdayam          weekdaypm       \n Length:306940      Length:306940      Min.   :    0.00   Min.   :    0.00  \n Class :character   Class :character   1st Qu.:    1.00   1st Qu.:    1.00  \n Mode  :character   Mode  :character   Median :    6.00   Median :    8.00  \n                                       Mean   :   82.23   Mean   :   78.52  \n                                       3rd Qu.:   37.00   3rd Qu.:   38.00  \n                                       Max.   :74796.00   Max.   :42785.00  \n   weekendam          weekendpm             geometry.origin  \n Min.   :    0.00   Min.   :    0.00   POLYGON      :306940  \n 1st Qu.:    0.00   1st Qu.:    0.00   epsg:3414    :     0  \n Median :    3.00   Median :    3.00   +proj=tmer...:     0  \n Mean   :   24.16   Mean   :   24.23                         \n 3rd Qu.:   12.00   3rd Qu.:   12.00                         \n Max.   :15603.00   Max.   :23484.00                         \n      geometry.destin            geometry     \n POLYGON      :306940   LINESTRING   :306940  \n epsg:3414    :     0   epsg:3414    :     0  \n +proj=tmer...:     0   +proj=tmer...:     0  \n                                              \n                                              \n                                              \n\n\nThen we can plot out 4 peak intervals in facets.\n\n\nCode\nweekdayam_plot &lt;- tm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeak_interval_flow %&gt;% \n  filter(weekdayam &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"weekdayam\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5) +\n  tm_layout(main.title = \"Trips during Weekday 6am till 9am\",\n            main.title.size = 1.2)\n\nweekdaypm_plot &lt;- tm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeak_interval_flow %&gt;% \n  filter(weekdaypm &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"weekdaypm\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5) +\n  tm_layout(main.title = \"Trips during Weekday 5pm till 8pm\",\n            main.title.size = 1.2)\n\nweekendam_plot &lt;- tm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeak_interval_flow %&gt;% \n  filter(weekendam &gt;= 2000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"weekendam\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5) +\n  tm_layout(main.title = \"Trips during Weekend/Holiday 11am till 2pm\",\n            main.title.size = 1.2)\n\nweekendpm_plot &lt;- tm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeak_interval_flow %&gt;% \n  filter(weekendpm &gt;= 2000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"weekendpm\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5) +\n  tm_layout(main.title = \"Trips during Weekend/Holiday 4pm till 7pm\",\n            main.title.size = 1.2)\n\ntmap_arrange(weekdayam_plot, weekdaypm_plot, weekendam_plot, weekendpm_plot, asp=1, ncol=2,\n             outer.margins = 0)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\nDescribe the spatial patterns revealed by the geo-visualization (not more than 100 words per visual)\n\n\n\n\n\nFirstly import all those propulsive and Attractiveness variables.\n\n\nCode\nbusiness = st_read(dsn = \"./data/geospatial\",\n                   layer = \"Business\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `Business' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 6550 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3669.148 ymin: 25408.41 xmax: 47034.83 ymax: 50148.54\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nentertn = st_read(dsn = \"./data/geospatial\",\n                   layer = \"entertn\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `entertn' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 114 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 10809.34 ymin: 26528.63 xmax: 41600.62 ymax: 46375.77\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nfood = st_read(dsn = \"./data/geospatial\",\n                   layer = \"F&B\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `F&B' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1919 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 6010.495 ymin: 25343.27 xmax: 45462.43 ymax: 48796.21\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nfinance = st_read(dsn = \"./data/geospatial\",\n                   layer = \"FinServ\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `FinServ' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3320 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4881.527 ymin: 25171.88 xmax: 46526.16 ymax: 49338.02\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nleisure = st_read(dsn = \"./data/geospatial\",\n                   layer = \"Leisure&Recreation\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `Leisure&Recreation' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1217 features and 30 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 6010.495 ymin: 25134.28 xmax: 48439.77 ymax: 50078.88\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nretail = st_read(dsn = \"./data/geospatial\",\n                   layer = \"Retails\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `Retails' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 37635 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4737.982 ymin: 25171.88 xmax: 48265.04 ymax: 50135.28\nProjected CRS: SVY21 / Singapore TM\n\n\nThen we can assemble all those variables by using st_join.\n\n\nCode\nfactors &lt;- st_join(retail %&gt;% select(POI_NAME, geometry), \n                   business %&gt;% select(POI_NAME, geometry), \n                   join = st_nearest_feature,\n                   suffix = c(\"_retail\", \"_business\")) %&gt;% \n  st_join(finance %&gt;% select(POI_NAME, geometry) %&gt;% rename(finance = POI_NAME),\n          join = st_nearest_feature) %&gt;% \n  st_join(food %&gt;% select(POI_NAME, geometry) %&gt;% rename(food = POI_NAME),\n          join = st_nearest_feature) %&gt;% \n  st_join(leisure %&gt;% select(POI_NAME, geometry) %&gt;% rename(leisure = POI_NAME),\n          join = st_nearest_feature) %&gt;% \n  st_join(entertn %&gt;% select(POI_NAME, geometry) %&gt;% rename(entertn = POI_NAME),\n          join = st_nearest_feature) %&gt;%\n  st_join(bus_hex%&gt;%select(BUS_STOP_N, geometry), join = st_nearest_feature)\n\n\nThen we need to count the point of each variable around each bus stop.\n\n\nCode\nfactor_count &lt;- \n  aggregate(POI_NAME_retail ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n  rename(retail_poi = POI_NAME_retail) %&gt;%\n  left_join(\n    aggregate(POI_NAME_business ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(business_poi = POI_NAME_business),\n    by = join_by(BUS_STOP_N)) %&gt;%\n  left_join(\n    aggregate(finance ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(finance_poi = finance),\n    by = join_by(BUS_STOP_N)) %&gt;%\n  left_join(\n    aggregate(food ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(food_poi = food),\n    by = join_by(BUS_STOP_N)) %&gt;% \n  left_join(\n    aggregate(leisure ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(leisure_poi = leisure),\n    by = join_by(BUS_STOP_N)) %&gt;% \n  left_join(\n    aggregate(entertn ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(entertn_poi = entertn),\n    by = join_by(BUS_STOP_N)) \n\n\nCheck summary of assembled factors before continue.\n\n\nCode\nsummary(factor_count)\n\n\n  BUS_STOP_N          retail_poi      business_poi     finance_poi    \n Length:1661        Min.   :  1.00   Min.   : 1.000   Min.   : 1.000  \n Class :character   1st Qu.:  2.00   1st Qu.: 1.000   1st Qu.: 1.000  \n Mode  :character   Median :  5.00   Median : 1.000   Median : 1.000  \n                    Mean   : 22.12   Mean   : 2.198   Mean   : 1.745  \n                    3rd Qu.: 18.00   3rd Qu.: 2.000   3rd Qu.: 2.000  \n                    Max.   :889.00   Max.   :27.000   Max.   :16.000  \n    food_poi       leisure_poi      entertn_poi   \n Min.   : 1.000   Min.   : 1.000   Min.   :1.000  \n 1st Qu.: 1.000   1st Qu.: 1.000   1st Qu.:1.000  \n Median : 1.000   Median : 1.000   Median :1.000  \n Mean   : 1.703   Mean   : 1.565   Mean   :1.139  \n 3rd Qu.: 2.000   3rd Qu.: 2.000   3rd Qu.:1.000  \n Max.   :37.000   Max.   :17.000   Max.   :6.000  \n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nSince left = TRUE is default in st_join, so here I will join those variables in the sequence of number of observations, which is, retail &lt;- business &lt;- finance &lt;- food &lt;- leisure &lt;- entertain.\nAnd to avoid NA count in some variable, I will set the “join = st_nearest_feature” instead of default “st_intersect”.\n\n\n\n\n\n\nFirst we need to convert the bus_hex we have generated into Spatial Polygons Data Frame.\n\n\nCode\nbus_sp &lt;- as(bus_hex, \"Spatial\")\nbus_sp\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 5140 \nextent      : 3595.122, 48595.12, 26265.59, 50081.29  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       :   id, BUS_STOP_N, OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR, ... \nmin values  :    3,      01012,        1,          1, ADMIRALTY,    AMSZ02,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949, 25813.3546, ... \nmax values  : 9888,      99189,      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FF2AA0D68743AE28,      16409, 49502.4866, 49482.5953, ... \n\n\nThen we can compute distance matrix of our bus stops hexagons and check the head 5.\n\n\nCode\ndist &lt;- spDists(bus_sp, \n                longlat = FALSE)\nhead(dist, n=c(5, 5))\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,]    0.0000  992.1567 2704.1635 3333.0729  992.1567\n[2,]  992.1567    0.0000 1948.5572 2598.0762  375.0000\n[3,] 2704.1635 1948.5572    0.0000  649.5191 2281.0359\n[4,] 3333.0729 2598.0762  649.5191    0.0000 2928.8436\n[5,]  992.1567  375.0000 2281.0359 2928.8436    0.0000\n\n\nThen we can label the bus stops and convert it to pair-distance.\n\n\nCode\nbusstop_N &lt;- bus_hex$BUS_STOP_N\n\ncolnames(dist) &lt;- paste0(busstop_N)\nrownames(dist) &lt;- paste0(busstop_N)\n\ndistPair &lt;- melt(dist) %&gt;%\n  rename(dist = value,\n         orig = Var1,\n         dest = Var2) %&gt;%\n  filter(orig != dest) %&gt;%\n  mutate(orig = as.character(orig),\n         dest = as.character(dest))\n\nhead(distPair, 10)\n\n\n    orig  dest      dist\n1  25751 25059  992.1567\n2  26379 25059 2704.1635\n3  26369 25059 3333.0729\n4  25761 25059  992.1567\n5  26389 25059 2459.0394\n6  25719 25059 2250.0000\n7  25711 25059 2250.0000\n8  26299 25059 4056.2452\n9  25741 25059 1634.5871\n10 25729 25059 2087.9116\n\n\nLet’s give a constant distance of 50m to intra-hexagon bus stops pairs before continue.\n\n\nCode\ndistPair$dist &lt;- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\n\nsummary(distPair)\n\n\n     orig               dest                dist      \n Length:26414460    Length:26414460    Min.   :   50  \n Class :character   Class :character   1st Qu.: 7803  \n Mode  :character   Mode  :character   Median :12667  \n                                       Mean   :13299  \n                                       3rd Qu.:17863  \n                                       Max.   :44926  \n\n\nSave the output of pairwise distance as rds file.\n\n\nCode\nwrite_rds(distPair, \"./data/rds/distPair.rds\") \n\n\n\n\n\n\n\n\nFor this part, I will focus on weekend/holiday evening peak time interval for further analysis.\n\n\nCode\nweekendpm_trips &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %&gt;%\n  reframe(TRIPS = sum(TOTAL_TRIPS))\n \nweekendpm_trips &lt;- weekendpm_trips %&gt;%\n  filter(ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %&gt;%\n  filter(DESTINATION_PT_CODE %in% bus_hex$'BUS_STOP_N')\n\nweekendpm_hex &lt;- left_join(weekendpm_trips,bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\"))  %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         DESTIN_BS = DESTINATION_PT_CODE) \n\nweekendpm_hex &lt;- left_join(weekendpm_hex, bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"DESTIN_BS\" = \"BUS_STOP_N\"),\n                          suffix = c(\".origin\", \".destin\")) \n\n\nCreate flow lines for those trips data.\n\n\nCode\nweekendpm_flow &lt;- od2line(flow = weekendpm_hex, \n                    zones = bus_hex,\n                    zone_code = \"BUS_STOP_N\") \n\nsummary(weekendpm_flow)\n\n\n  ORIGIN_BS          DESTIN_BS             TRIPS         \n Length:218487      Length:218487      Min.   :    1.00  \n Class :character   Class :character   1st Qu.:    2.00  \n Mode  :character   Mode  :character   Median :    6.00  \n                                       Mean   :   34.05  \n                                       3rd Qu.:   21.00  \n                                       Max.   :23484.00  \n      geometry.origin        geometry.destin            geometry     \n POLYGON      :218487   POLYGON      :218487   LINESTRING   :218487  \n epsg:3414    :     0   epsg:3414    :     0   epsg:3414    :     0  \n +proj=tmer...:     0   +proj=tmer...:     0   +proj=tmer...:     0  \n                                                                     \n                                                                     \n                                                                     \n\n\nTo ensure, let’s check any case of origin == destination before continue.\n\n\nCode\nweekendpm_flow %&gt;%\n  filter(ORIGIN_BS == DESTIN_BS)\n\n\nSimple feature collection with 0 features and 3 fields\nActive geometry column: geometry\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: SVY21 / Singapore TM\n[1] ORIGIN_BS       DESTIN_BS       TRIPS           geometry.origin\n[5] geometry.destin geometry       \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nThen we can join the flow data with pairwise distance.\n\n\nCode\nweekendpm_flow &lt;- weekendpm_flow %&gt;%\n  left_join (distPair,\n             by = c(\"ORIGIN_BS\" = \"orig\",\n                    \"DESTIN_BS\" = \"dest\"))\n\n\n\n\nCode\nweekendpm_flow %&gt;%\n  filter(dist != NA)\n\n\nSimple feature collection with 0 features and 4 fields\nActive geometry column: geometry\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: SVY21 / Singapore TM\n[1] ORIGIN_BS       DESTIN_BS       TRIPS           dist           \n[5] geometry.origin geometry.destin geometry       \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\nPresent the modelling results by using appropriate geovisualization and graphical visualization methods.\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nWith reference to the Spatial Interaction Model output tables, maps and data visualization prepared, describe the modelling results"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html#setting-the-scene",
    "href": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html#setting-the-scene",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "What are the driving forces behind urban dwellers to weak up early in morning to commute from their home locations to their work places? What are the impact of removing a public bus service on the commuters reside along the corridor of the bus route? These and many other questions related to urban mobility are challenges faced by transport operators and urban managers.\nTo provide answer to this question, traditionally, commuters survey will be used. However, commuters survey is a very costly, time-consuming and laborous, not to mention that the survey data tend to take a long time to clean and analyse. As a result, it is not unusual, by the time the survey report was ready, most of the information already out-of-date!\nAs city-wide urban infrastructures such as public buses, mass rapid transits, public utilities and roads become digital, the data sets obtained can be used as a framework for tracking movement patterns through space and time. This is particularly true with the recent trend of massive deployment of pervasive computing technologies such as GPS on the vehicles and SMART cards used by public transport commuters.\nUnfortunately, this explosive growth of geospatially-referenced data has far outpaced the planner’s ability to utilize and transform the data into insightful information thus creating an adverse impact on the return on the investment made to collect and manage this data."
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html#objectives",
    "href": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html#objectives",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "This take-home exercise is motivated by two main reasons. Firstly, despite increasing amounts of open data available for public consumption, there has not been significant practice research carried out to show how these disparate data sources can be integrated, analysed, and modelled to support policy making decisions.\nSecondly, there is a general lack of practical research to show how geospatial data science and analysis (GDSA) can be used to support decision-making.\nHence, your task for this take-home exercise is to conduct a case study to demonstrate the potential value of GDSA to integrate publicly available data from multiple sources for building a spatial interaction models to determine factors affecting urban mobility patterns of public bus transit."
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html#the-data",
    "href": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html#the-data",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "For the purpose of this assignment, data from several open government sources will be used:\n\nPassenger Volume by Origin Destination Bus Stops, Bus Stop Location, Train Station and Train Station Exit Point, just to name a few of them, from LTA DataMall.\nMaster Plan 2019 Subzone Boundary, HDB Property Information, School Directory and Information and other relevant data from Data.gov.sg.\n\n\n\n\n\nBusiness, entertn, F&B, FinServ, Leisure&Recreation and Retails are geospatial data sets of the locations of business establishments, entertainments, food and beverage outlets, financial centres, leisure and recreation centres, retail and services stores/outlets I compiled for urban mobility study. They are available on in the geospatial folder to Take-home Exercise 2 data folder.\nHDB: This data set is the geocoded version of HDB Property Information data from data.gov. The data set is prepared using September 2021 data. If you want to prepare you own data by using the latest HDB Property Information provided on data.gov.sg, this link provides a useful step-by-step guide.\n\n\n\n\n\n\n\nImportant\n\n\n\nThose specially collected data aim to use within this excercise content only, if intend to put in other usage, approach course instructor Dr. Kam Tin Seong and ask for permission first.\n\n\nFor starting, load needing packages.\n\n\nCode\npacman::p_load(sf, sp, spdep, tmap, tidyverse, sfdep, stplanr, reshape2, ggpubr, DT)"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html#the-task",
    "href": "Takehome_Ex/Takehome_Ex02/Takehome_Ex02.html#the-task",
    "title": "Take Home Exercise 2",
    "section": "",
    "text": "Derive an analytical hexagon data of 375m (this distance is the perpendicular distance between the centre of the hexagon and its edges) to represent the traffic analysis zone (TAZ).\nFirst of all, we need to import Bus Stop Location from LTA DataMall.\n\n\nCode\nbusstop = st_read(dsn = \"./data/geospatial/BusStopLocation_Jul2023\",\n                   layer = \"BusStop\")  %&gt;% st_transform(crs = 3414) %&gt;% \n  distinct(BUS_STOP_N, .keep_all = TRUE)\n\n\nReading layer `BusStop' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial/BusStopLocation_Jul2023' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nAlso import subzone geometry data as our background layer.\n\n\nCode\nsz = st_read(dsn = \"./data/geospatial\",\n                   layer = \"MP14_SUBZONE_WEB_PL\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nThen we can Derive an analytical hexagon data of 375m.\n\n\nCode\nhexagon &lt;- st_sf(geometry = st_make_grid(busstop, cellsize = c(375,375), what = \"polygons\",square = FALSE)) %&gt;%\n  mutate(id = row_number()) %&gt;% \n  st_transform(crs = 3414) \n\n\nThen we can join bus stop with hexagon, and join with subzone to exclude hexagons out of range.\n\n\nCode\nbus_hex &lt;- st_join(\n  st_join(hexagon, busstop%&gt;%select(BUS_STOP_N,geometry), join = st_intersects),\n  sz) %&gt;%\n  drop_na() %&gt;%\n  distinct(BUS_STOP_N, .keep_all = TRUE)\n\n\nCheck for duplicate geometry.\n\n\nCode\nbus_hex %&gt;%\n  group_by(geometry)%&gt;%\n  filter(row_number()&gt;1)\n\n\n\n\n\nWith reference to the time intervals provided in the table below, construct an O-D matrix of commuter flows for a time interval of your choice by integrating Passenger Volume by Origin Destination Bus Stops and Bus Stop Location from LTA DataMall. The O-D matrix must be aggregated at the analytics hexagon level\n\n\n\nPeak hour period\nBus tap on time\n\n\n\n\nWeekday morning peak\n6am to 9am\n\n\nWeekday afternoon peak\n5pm to 8pm\n\n\nWeekend/holiday morning peak\n11am to 2pm\n\n\nWeekend/holiday evening peak\n4pm to 7pm\n\n\n\nImport bus passenger trips data.\n\n\nCode\nodbus = read_csv(\"./data/aspatial/origin_destination_bus_202310.csv\")  %&gt;%\n  mutate(ORIGIN_PT_CODE = as.factor(ORIGIN_PT_CODE),\n         DESTINATION_PT_CODE = as.factor(DESTINATION_PT_CODE))\n\n\nExtract passenger trips data during all peak time intervals.\n\n\nCode\npeak_trips &lt;- bind_rows(\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n    mutate(interval = \"weekdayam\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 17 &\n           TIME_PER_HOUR &lt;= 20) %&gt;%\n    mutate(interval = \"weekdaypm\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 11 &\n           TIME_PER_HOUR &lt;= 14) %&gt;%\n    mutate(interval = \"weekendam\"),\n  odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n    mutate(interval = \"weekendpm\")) %&gt;%\n  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE, interval) %&gt;%\n  reframe(TRIPS = sum(TOTAL_TRIPS)) \n\nglimpse(peak_trips)\n\n\nRows: 951,259\nColumns: 4\n$ ORIGIN_PT_CODE      &lt;fct&gt; 01012, 01012, 01012, 01012, 01012, 01012, 01012, 0…\n$ DESTINATION_PT_CODE &lt;fct&gt; 01112, 01112, 01112, 01112, 01113, 01113, 01113, 0…\n$ interval            &lt;chr&gt; \"weekdayam\", \"weekdaypm\", \"weekendam\", \"weekendpm\"…\n$ TRIPS               &lt;dbl&gt; 290, 540, 265, 201, 118, 516, 189, 165, 77, 303, 1…\n\n\nCheck any bus stops not in our origin ‘bus_hex’ list.\n\n\nCode\npeak_trips %&gt;%\n  filter(! ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %&gt;%\n  group_by(ORIGIN_PT_CODE) %&gt;%\n  reframe(TRIPS = sum(TRIPS))\n\n\nExclude any bus stops not included in ‘bus_hex’ data before continue.\n\n\nCode\npeak_trips &lt;- peak_trips %&gt;%\n  filter(ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %&gt;%\n  filter(DESTINATION_PT_CODE %in% bus_hex$'BUS_STOP_N')\n\n\nDuplication check before continue.\n\n\nCode\npeak_trips %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nAfter that, we need to combine those passenger trip data with geospatial data by origin bus stops.\n\n\nCode\npeaktrip_hex &lt;- left_join(peak_trips %&gt;% \n                            group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %&gt;%\n                            reframe(TRIPS = sum(TRIPS)),\n                          bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\"))  %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         DESTIN_BS = DESTINATION_PT_CODE) \n\n\nDuplication check before continue:\n\n\nCode\npeaktrip_hex %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\nThen we can continue to join again with geospatial data by destination bus stops.\n\n\nCode\npeaktrip_hex &lt;- left_join(peaktrip_hex, bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"DESTIN_BS\" = \"BUS_STOP_N\"),\n                          suffix = c(\".origin\", \".destin\")) \n\n\nDuplication check again.\n\n\nCode\npeaktrip_hex %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\n# A tibble: 0 × 5\n# ℹ 5 variables: ORIGIN_BS &lt;chr&gt;, DESTIN_BS &lt;chr&gt;, TRIPS &lt;dbl&gt;,\n#   geometry.origin &lt;GEOMETRY [m]&gt;, geometry.destin &lt;GEOMETRY [m]&gt;\n\n\nWe can save the output into a rds file.\n\n\nCode\nwrite_rds(peaktrip_hex, \"./data/rds/peaktrip_hex.rds\")\n\n\n\n\n\nDisplay the O-D flows of the passenger trips by using appropriate geovisualisation methods.\nFirst let’s ensure there aren’t any observations with same origin and destination.\n\n\nCode\npeaktrip_hex %&gt;%\n  filter(ORIGIN_BS==DESTIN_BS)\n\n\nThen we can create flow lines and check summary of data in case there are any zero.\n\n\nCode\npeaktrip_flow &lt;- od2line(flow = peaktrip_hex, \n                    zones = bus_hex,\n                    zone_code = \"BUS_STOP_N\")\n\nsummary(peaktrip_flow)\n\n\n  ORIGIN_BS          DESTIN_BS             TRIPS         \n Length:306940      Length:306940      Min.   :     1.0  \n Class :character   Class :character   1st Qu.:     6.0  \n Mode  :character   Mode  :character   Median :    27.0  \n                                       Mean   :   209.1  \n                                       3rd Qu.:   112.0  \n                                       Max.   :153711.0  \n      geometry.origin        geometry.destin            geometry     \n POLYGON      :306940   POLYGON      :306940   LINESTRING   :306940  \n epsg:3414    :     0   epsg:3414    :     0   epsg:3414    :     0  \n +proj=tmer...:     0   +proj=tmer...:     0   +proj=tmer...:     0  \n                                                                     \n                                                                     \n                                                                     \n\n\nTill now, we can plot out the bus trip flow during all 4 peak time intervals in total.\n\n\nCode\ntm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeaktrip_flow %&gt;% \n  filter(TRIPS &gt;= 2000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"TRIPS\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5)\n\n\n\n\n\nAnd we can visualize 4 peak time intervals in facets style to check any difference within.\nFirst need to wrangling the data to put trips data of different time intervals into different columns.\n\n\nCode\npeak_interval_trips &lt;- peak_trips %&gt;%\n  pivot_wider(names_from = interval, \n              values_from = TRIPS, \n              values_fill = 0)\n\npeak_interval_hex &lt;- left_join(peak_interval_trips,\n                          bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\"))  %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         DESTIN_BS = DESTINATION_PT_CODE) \n\n\npeak_interval_hex &lt;- left_join(peak_interval_hex, bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"DESTIN_BS\" = \"BUS_STOP_N\"),\n                          suffix = c(\".origin\", \".destin\")) \n\n\nBefore we continue, we can check duplication and save the result as a rds file.\n\n\nCode\npeak_interval_hex %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\n\n\nCode\nwrite_rds(peak_interval_hex, \"./data/rds/peak_interval_hex.rds\")\n\n\nThen we can create flow lines.\n\n\nCode\npeak_interval_flow &lt;- od2line(flow = peak_interval_hex, \n                    zones = bus_hex,\n                    zone_code = \"BUS_STOP_N\")\n\nsummary(peak_interval_flow)\n\n\n  ORIGIN_BS          DESTIN_BS           weekdayam          weekdaypm       \n Length:306940      Length:306940      Min.   :    0.00   Min.   :    0.00  \n Class :character   Class :character   1st Qu.:    1.00   1st Qu.:    1.00  \n Mode  :character   Mode  :character   Median :    6.00   Median :    8.00  \n                                       Mean   :   82.23   Mean   :   78.52  \n                                       3rd Qu.:   37.00   3rd Qu.:   38.00  \n                                       Max.   :74796.00   Max.   :42785.00  \n   weekendam          weekendpm             geometry.origin  \n Min.   :    0.00   Min.   :    0.00   POLYGON      :306940  \n 1st Qu.:    0.00   1st Qu.:    0.00   epsg:3414    :     0  \n Median :    3.00   Median :    3.00   +proj=tmer...:     0  \n Mean   :   24.16   Mean   :   24.23                         \n 3rd Qu.:   12.00   3rd Qu.:   12.00                         \n Max.   :15603.00   Max.   :23484.00                         \n      geometry.destin            geometry     \n POLYGON      :306940   LINESTRING   :306940  \n epsg:3414    :     0   epsg:3414    :     0  \n +proj=tmer...:     0   +proj=tmer...:     0  \n                                              \n                                              \n                                              \n\n\nThen we can plot out 4 peak intervals in facets.\n\n\nCode\nweekdayam_plot &lt;- tm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeak_interval_flow %&gt;% \n  filter(weekdayam &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"weekdayam\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5) +\n  tm_layout(main.title = \"Trips during Weekday 6am till 9am\",\n            main.title.size = 1.2)\n\nweekdaypm_plot &lt;- tm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeak_interval_flow %&gt;% \n  filter(weekdaypm &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"weekdaypm\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5) +\n  tm_layout(main.title = \"Trips during Weekday 5pm till 8pm\",\n            main.title.size = 1.2)\n\nweekendam_plot &lt;- tm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeak_interval_flow %&gt;% \n  filter(weekendam &gt;= 2000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"weekendam\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5) +\n  tm_layout(main.title = \"Trips during Weekend/Holiday 11am till 2pm\",\n            main.title.size = 1.2)\n\nweekendpm_plot &lt;- tm_shape(sz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\npeak_interval_flow %&gt;% \n  filter(weekendpm &gt;= 2000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"weekendpm\",\n           style = \"quantile\",\n           scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5) +\n  tm_layout(main.title = \"Trips during Weekend/Holiday 4pm till 7pm\",\n            main.title.size = 1.2)\n\ntmap_arrange(weekdayam_plot, weekdaypm_plot, weekendam_plot, weekendpm_plot, asp=1, ncol=2,\n             outer.margins = 0)\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nObservations:\nDescribe the spatial patterns revealed by the geo-visualization (not more than 100 words per visual)\n\n\n\n\n\nFirstly import all those propulsive and Attractiveness variables.\n\n\nCode\nbusiness = st_read(dsn = \"./data/geospatial\",\n                   layer = \"Business\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `Business' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 6550 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3669.148 ymin: 25408.41 xmax: 47034.83 ymax: 50148.54\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nentertn = st_read(dsn = \"./data/geospatial\",\n                   layer = \"entertn\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `entertn' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 114 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 10809.34 ymin: 26528.63 xmax: 41600.62 ymax: 46375.77\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nfood = st_read(dsn = \"./data/geospatial\",\n                   layer = \"F&B\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `F&B' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1919 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 6010.495 ymin: 25343.27 xmax: 45462.43 ymax: 48796.21\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nfinance = st_read(dsn = \"./data/geospatial\",\n                   layer = \"FinServ\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `FinServ' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 3320 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4881.527 ymin: 25171.88 xmax: 46526.16 ymax: 49338.02\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nleisure = st_read(dsn = \"./data/geospatial\",\n                   layer = \"Leisure&Recreation\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `Leisure&Recreation' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1217 features and 30 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 6010.495 ymin: 25134.28 xmax: 48439.77 ymax: 50078.88\nProjected CRS: SVY21 / Singapore TM\n\n\nCode\nretail = st_read(dsn = \"./data/geospatial\",\n                   layer = \"Retails\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `Retails' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Takehome_Ex/Takehome_Ex02/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 37635 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 4737.982 ymin: 25171.88 xmax: 48265.04 ymax: 50135.28\nProjected CRS: SVY21 / Singapore TM\n\n\nThen we can assemble all those variables by using st_join.\n\n\nCode\nfactors &lt;- st_join(retail %&gt;% select(POI_NAME, geometry), \n                   business %&gt;% select(POI_NAME, geometry), \n                   join = st_nearest_feature,\n                   suffix = c(\"_retail\", \"_business\")) %&gt;% \n  st_join(finance %&gt;% select(POI_NAME, geometry) %&gt;% rename(finance = POI_NAME),\n          join = st_nearest_feature) %&gt;% \n  st_join(food %&gt;% select(POI_NAME, geometry) %&gt;% rename(food = POI_NAME),\n          join = st_nearest_feature) %&gt;% \n  st_join(leisure %&gt;% select(POI_NAME, geometry) %&gt;% rename(leisure = POI_NAME),\n          join = st_nearest_feature) %&gt;% \n  st_join(entertn %&gt;% select(POI_NAME, geometry) %&gt;% rename(entertn = POI_NAME),\n          join = st_nearest_feature) %&gt;%\n  st_join(bus_hex%&gt;%select(BUS_STOP_N, geometry), join = st_nearest_feature)\n\n\nThen we need to count the point of each variable around each bus stop.\n\n\nCode\nfactor_count &lt;- \n  aggregate(POI_NAME_retail ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n  rename(retail_poi = POI_NAME_retail) %&gt;%\n  left_join(\n    aggregate(POI_NAME_business ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(business_poi = POI_NAME_business),\n    by = join_by(BUS_STOP_N)) %&gt;%\n  left_join(\n    aggregate(finance ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(finance_poi = finance),\n    by = join_by(BUS_STOP_N)) %&gt;%\n  left_join(\n    aggregate(food ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(food_poi = food),\n    by = join_by(BUS_STOP_N)) %&gt;% \n  left_join(\n    aggregate(leisure ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(leisure_poi = leisure),\n    by = join_by(BUS_STOP_N)) %&gt;% \n  left_join(\n    aggregate(entertn ~ BUS_STOP_N, data = factors, function(x) length(unique(x))) %&gt;%\n    rename(entertn_poi = entertn),\n    by = join_by(BUS_STOP_N)) \n\n\nCheck summary of assembled factors before continue.\n\n\nCode\nsummary(factor_count)\n\n\n  BUS_STOP_N          retail_poi      business_poi     finance_poi    \n Length:1661        Min.   :  1.00   Min.   : 1.000   Min.   : 1.000  \n Class :character   1st Qu.:  2.00   1st Qu.: 1.000   1st Qu.: 1.000  \n Mode  :character   Median :  5.00   Median : 1.000   Median : 1.000  \n                    Mean   : 22.12   Mean   : 2.198   Mean   : 1.745  \n                    3rd Qu.: 18.00   3rd Qu.: 2.000   3rd Qu.: 2.000  \n                    Max.   :889.00   Max.   :27.000   Max.   :16.000  \n    food_poi       leisure_poi      entertn_poi   \n Min.   : 1.000   Min.   : 1.000   Min.   :1.000  \n 1st Qu.: 1.000   1st Qu.: 1.000   1st Qu.:1.000  \n Median : 1.000   Median : 1.000   Median :1.000  \n Mean   : 1.703   Mean   : 1.565   Mean   :1.139  \n 3rd Qu.: 2.000   3rd Qu.: 2.000   3rd Qu.:1.000  \n Max.   :37.000   Max.   :17.000   Max.   :6.000  \n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nSince left = TRUE is default in st_join, so here I will join those variables in the sequence of number of observations, which is, retail &lt;- business &lt;- finance &lt;- food &lt;- leisure &lt;- entertain.\nAnd to avoid NA count in some variable, I will set the “join = st_nearest_feature” instead of default “st_intersect”.\n\n\n\n\n\n\nFirst we need to convert the bus_hex we have generated into Spatial Polygons Data Frame.\n\n\nCode\nbus_sp &lt;- as(bus_hex, \"Spatial\")\nbus_sp\n\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 5140 \nextent      : 3595.122, 48595.12, 26265.59, 50081.29  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 17\nnames       :   id, BUS_STOP_N, OBJECTID, SUBZONE_NO, SUBZONE_N, SUBZONE_C, CA_IND, PLN_AREA_N, PLN_AREA_C,       REGION_N, REGION_C,          INC_CRC, FMEL_UPD_D,     X_ADDR,     Y_ADDR, ... \nmin values  :    3,      01012,        1,          1, ADMIRALTY,    AMSZ02,      N, ANG MO KIO,         AM, CENTRAL REGION,       CR, 00F5E30B5C9B7AD8,      16409,  5092.8949, 25813.3546, ... \nmax values  : 9888,      99189,      323,         17,    YUNNAN,    YSSZ09,      Y,     YISHUN,         YS,    WEST REGION,       WR, FF2AA0D68743AE28,      16409, 49502.4866, 49482.5953, ... \n\n\nThen we can compute distance matrix of our bus stops hexagons and check the head 5.\n\n\nCode\ndist &lt;- spDists(bus_sp, \n                longlat = FALSE)\nhead(dist, n=c(5, 5))\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,]    0.0000  992.1567 2704.1635 3333.0729  992.1567\n[2,]  992.1567    0.0000 1948.5572 2598.0762  375.0000\n[3,] 2704.1635 1948.5572    0.0000  649.5191 2281.0359\n[4,] 3333.0729 2598.0762  649.5191    0.0000 2928.8436\n[5,]  992.1567  375.0000 2281.0359 2928.8436    0.0000\n\n\nThen we can label the bus stops and convert it to pair-distance.\n\n\nCode\nbusstop_N &lt;- bus_hex$BUS_STOP_N\n\ncolnames(dist) &lt;- paste0(busstop_N)\nrownames(dist) &lt;- paste0(busstop_N)\n\ndistPair &lt;- melt(dist) %&gt;%\n  rename(dist = value,\n         orig = Var1,\n         dest = Var2) %&gt;%\n  filter(orig != dest) %&gt;%\n  mutate(orig = as.character(orig),\n         dest = as.character(dest))\n\nhead(distPair, 10)\n\n\n    orig  dest      dist\n1  25751 25059  992.1567\n2  26379 25059 2704.1635\n3  26369 25059 3333.0729\n4  25761 25059  992.1567\n5  26389 25059 2459.0394\n6  25719 25059 2250.0000\n7  25711 25059 2250.0000\n8  26299 25059 4056.2452\n9  25741 25059 1634.5871\n10 25729 25059 2087.9116\n\n\nLet’s give a constant distance of 50m to intra-hexagon bus stops pairs before continue.\n\n\nCode\ndistPair$dist &lt;- ifelse(distPair$dist == 0,\n                        50, distPair$dist)\n\nsummary(distPair)\n\n\n     orig               dest                dist      \n Length:26414460    Length:26414460    Min.   :   50  \n Class :character   Class :character   1st Qu.: 7803  \n Mode  :character   Mode  :character   Median :12667  \n                                       Mean   :13299  \n                                       3rd Qu.:17863  \n                                       Max.   :44926  \n\n\nSave the output of pairwise distance as rds file.\n\n\nCode\nwrite_rds(distPair, \"./data/rds/distPair.rds\") \n\n\n\n\n\n\n\n\nFor this part, I will focus on weekend/holiday evening peak time interval for further analysis.\n\n\nCode\nweekendpm_trips &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKENDS/HOLIDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 16 &\n           TIME_PER_HOUR &lt;= 19) %&gt;%\n  group_by(ORIGIN_PT_CODE, DESTINATION_PT_CODE) %&gt;%\n  reframe(TRIPS = sum(TOTAL_TRIPS))\n \nweekendpm_trips &lt;- weekendpm_trips %&gt;%\n  filter(ORIGIN_PT_CODE %in% bus_hex$'BUS_STOP_N') %&gt;%\n  filter(DESTINATION_PT_CODE %in% bus_hex$'BUS_STOP_N')\n\nweekendpm_hex &lt;- left_join(weekendpm_trips,bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\"))  %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         DESTIN_BS = DESTINATION_PT_CODE) \n\nweekendpm_hex &lt;- left_join(weekendpm_hex, bus_hex %&gt;% select(BUS_STOP_N, geometry), \n                          by = c(\"DESTIN_BS\" = \"BUS_STOP_N\"),\n                          suffix = c(\".origin\", \".destin\")) \n\n\nCreate flow lines for those trips data.\n\n\nCode\nweekendpm_flow &lt;- od2line(flow = weekendpm_hex, \n                    zones = bus_hex,\n                    zone_code = \"BUS_STOP_N\") \n\nsummary(weekendpm_flow)\n\n\n  ORIGIN_BS          DESTIN_BS             TRIPS         \n Length:218487      Length:218487      Min.   :    1.00  \n Class :character   Class :character   1st Qu.:    2.00  \n Mode  :character   Mode  :character   Median :    6.00  \n                                       Mean   :   34.05  \n                                       3rd Qu.:   21.00  \n                                       Max.   :23484.00  \n      geometry.origin        geometry.destin            geometry     \n POLYGON      :218487   POLYGON      :218487   LINESTRING   :218487  \n epsg:3414    :     0   epsg:3414    :     0   epsg:3414    :     0  \n +proj=tmer...:     0   +proj=tmer...:     0   +proj=tmer...:     0  \n                                                                     \n                                                                     \n                                                                     \n\n\nTo ensure, let’s check any case of origin == destination before continue.\n\n\nCode\nweekendpm_flow %&gt;%\n  filter(ORIGIN_BS == DESTIN_BS)\n\n\nSimple feature collection with 0 features and 3 fields\nActive geometry column: geometry\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: SVY21 / Singapore TM\n[1] ORIGIN_BS       DESTIN_BS       TRIPS           geometry.origin\n[5] geometry.destin geometry       \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nThen we can join the flow data with pairwise distance.\n\n\nCode\nweekendpm_flow &lt;- weekendpm_flow %&gt;%\n  left_join (distPair,\n             by = c(\"ORIGIN_BS\" = \"orig\",\n                    \"DESTIN_BS\" = \"dest\"))\n\n\n\n\nCode\nweekendpm_flow %&gt;%\n  filter(dist != NA)\n\n\nSimple feature collection with 0 features and 4 fields\nActive geometry column: geometry\nBounding box:  xmin: NA ymin: NA xmax: NA ymax: NA\nProjected CRS: SVY21 / Singapore TM\n[1] ORIGIN_BS       DESTIN_BS       TRIPS           dist           \n[5] geometry.origin geometry.destin geometry       \n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n\nPresent the modelling results by using appropriate geovisualization and graphical visualization methods.\n\n\n\n\n\n\nNote\n\n\n\nObservations:\n\nWith reference to the Spatial Interaction Model output tables, maps and data visualization prepared, describe the modelling results"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex01/data/geospatial/hexagon/hexagon.html",
    "href": "Takehome_Ex/Takehome_Ex01/data/geospatial/hexagon/hexagon.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n                 +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs 0 0     false"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#exploratory-data-analysis-eda",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#exploratory-data-analysis-eda",
    "title": "Hands on Excercise 4",
    "section": "3 Exploratory Data Analysis (EDA)",
    "text": "3 Exploratory Data Analysis (EDA)\n\n3.1 EDA using statistical graphics\nWe can plot the distribution of SELLING_PRICE by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.\n\n\nCode\nggplot(data=condo_resale.sf, aes(x=`SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  theme_light()\n\n\n\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices.\nStatistically, the skewed dsitribution can be normalised by using log transformation. The code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\n\nCode\ncondo_resale.sf &lt;- condo_resale.sf %&gt;%\n  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\n\nNow, you can plot the LOG_SELLING_PRICE using the code chunk below.\n\n\nCode\nggplot(data=condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\") +\n  theme_light()\n\n\n\n\n\n\n\n3.2 Multiple Histogram Plots distribution of variables\nIn this section, you will learn how to draw a small multiple histograms (also known as trellis plot) by using ggarrange() of ggpubr package.\nThe code chunk below is used to create 12 histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\n\nCode\nAREA_SQM &lt;- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nAGE &lt;- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CHILDCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_URA_GROWTH_AREA &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER_MARKET &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_KINDERGARTEN &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_TOP_PRIMARY_SCH &lt;- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\n\n\n3.3 Drawing Statistical Point Map\nLastly, we want to reveal the geospatial distribution condominium resale prices in Singapore. The map will be prepared by using tmap package.\nFirst, we will turn on the interactive mode of tmap by using the code chunk below.\n\n\nCode\n#tmap_mode(\"view\")\n\n\nNext, the code chunks below is used to create an interactive point symbol map.\n\n\nCode\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\nNotice that tm_dots() is used instead of tm_bubbles().\nset.zoom.limits argument of tm_view() sets the minimum and maximum zoom level to 11 and 14 respectively.\nBefore moving on to the next section, the code below will be used to turn R display into plot mode.\n\n\nCode\n#tmap_mode(\"plot\")"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#hedonic-pricing-modelling-in-r",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#hedonic-pricing-modelling-in-r",
    "title": "Hands on Excercise 4",
    "section": "4 Hedonic Pricing Modelling in R",
    "text": "4 Hedonic Pricing Modelling in R\nIn this section, you will learn how to building hedonic pricing models for condominium resale units using lm() of R base.\n\n4.1 Simple Linear Regression Method\nFirst, we will build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\n\nCode\ncondo.slr &lt;- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\n\nlm() returns an object of class \"lm\" or for multiple responses of class c(\"mlm\", \"lm\").\nThe functions summary() and anova() can be used to obtain and print a summary and analysis of variance table of the results. The generic accessor functions coefficients, effects, fitted.values and residuals extract various useful features of the value returned by lm.\n\n\nCode\nsummary(condo.slr)\n\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: &lt; 2.2e-16\n\n\nThe output report reveals that the SELLING_PRICE can be explained by using the formula:\n      *y = -258121.1 + 14719x1*\nThe R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE. This will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\nThe Coefficients: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.\nTo visualise the best fit curve on a scatterplot, we can incorporate lm() as a method function in ggplot's geometry as shown in the code chunk below.\n\n\nCode\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm) +\n  theme_light()\n\n\n\n\n\nFigure above reveals that there are a few statistical outliers with relatively high selling prices.\n\n\n4.2 Multiple Linear Regression Method\n\n4.2.1 Visualising the relationships of the independent variables\nBefore building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.\nCorrelation matrix is commonly used to visualise the relationships between the independent variables. Beside the pairs() of R, there are many packages support the display of a correlation matrix. In this section, the corrplot package will be used.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\n\nCode\ncorrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = \"AOE\",\n         tl.pos = \"td\", tl.cex = 1, number.cex = 1, method = \"number\", type = \"upper\")\n\n\n\n\n\nMatrix reorder is very important for mining the hiden structure and patter in the matrix. There are four methods in corrplot (parameter order), named \"AOE\", \"FPC\", \"hclust\", \"alphabet\". In the code chunk above, AOE order is used. It orders the variables by using the angular order of the eigenvectors method suggested by Michael Friendly.\nFrom the scatterplot matrix, it is clear that Freehold is highly correlated to LEASE_99YEAR. In view of this, it is wiser to only include either one of them in the subsequent model building. As a result, LEASE_99YEAR is excluded in the subsequent model building.\n\n\n\n4.3 Building a hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\n\nCode\ncondo.mlr &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  &lt; 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  &lt; 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  &lt; 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  &lt; 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n4.4 Preparing Publication Quality Table: olsrr method\nWith reference to the report above, it is clear that not all the independent variables are statistically significant. We will revised the model by removing those variables which are not statistically significant.\nNow, we are ready to calibrate the revised model by using the code chunk below.\n\n\nCode\ncondo.mlr1 &lt;- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n\n                             Model Summary                               \n------------------------------------------------------------------------\nR                       0.807       RMSE                     755957.289 \nR-Squared               0.651       Coef. Var                    43.168 \nAdj. R-Squared          0.647       MSE                571471422208.592 \nPred R-Squared          0.638       MAE                      414819.628 \n------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.592                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\n\n\n4.5 Preparing Publication Quality Table: gtsummary method\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report.\n\n\nCode\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWith gtsummary package, model statistics can be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\n\nCode\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %&gt;% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n&lt;0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n&lt;0.001\n    AGE\n-24,688\n-30,092, -19,284\n&lt;0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n&lt;0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n&lt;0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n&lt;0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n&lt;0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n&lt;0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n&lt;0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n&lt;0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n&lt;0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = &lt;0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nFor more customisation options, refer to Tutorial: tbl_regression\n\n4.5.1 Checking for multicolinearity\nIn this section, we would like to introduce you a fantastic R package specially programmed for performing OLS regression. It is called olsrr. It provides a collection of very useful methods for building better multiple linear regression models:\n\ncomprehensive regression output\nresidual diagnostics\nmeasures of influence\nheteroskedasticity tests\ncollinearity diagnostics\nmodel fit assessment\nvariable contribution assessment\nvariable selection procedures\n\nIn the code chunk below, the ols_vif_tol() of olsrr package is used to test if there are sign of multicollinearity.\n\n\nCode\nols_vif_tol(condo.mlr1)\n\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nSince the VIF of the independent variables are less than 10. We can safely conclude that there are no sign of multicollinearity among the independent variables.\n\n\n4.5.2 Test for Non-Linearity\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n\nCode\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n\n4.5.3 Test for Normality Assumption\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\n\nCode\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\n\nCode\nols_test_normality(condo.mlr1)\n\n\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.6856         0.0000 \nKolmogorov-Smirnov        0.1366         0.0000 \nCramer-von Mises         121.0768        0.0000 \nAnderson-Darling         67.9551         0.0000 \n-----------------------------------------------\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n\n4.5.4 Testing for Spatial Autocorrelation\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\nIn order to perform spatial autocorrelation test, we need to convert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\n\nCode\nmlr.output &lt;- as.data.frame(condo.mlr1$residuals)\n\n\nNext, we will join the newly created data frame with condo_resale.sf object.\n\n\nCode\ncondo_resale.res.sf &lt;- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %&gt;%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\n\n\nNext, we will convert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nThe code chunk below will be used to perform the data conversion process.\n\n\nCode\ncondo_resale.sp &lt;- as_Spatial(condo_resale.res.sf)\ncondo_resale.sp\n\n\nclass       : SpatialPointsDataFrame \nfeatures    : 1436 \nextent      : 14940.85, 43352.45, 24765.67, 48382.81  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 23\nnames       : POSTCODE, SELLING_PRICE, AREA_SQM, AGE,    PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN,    PROX_MRT,   PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH, PROX_SHOPPING_MALL, ... \nmin values  :    18965,        540000,       34,   0, 0.386916393,    0.004927023,      0.054508623,          0.214539508,        0.051817113,       0.004927023, 0.052779424, 0.029064164,      0.077106132,          0.077106132,                  0, ... \nmax values  :   828833,       1.8e+07,      619,  37, 19.18042832,     3.46572633,      3.949157205,           9.15540001,        5.374348075,       2.229045366,  3.48037319,  2.16104919,      3.928989144,          6.748192062,        3.477433767, ... \n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\nThe code chunks below is used to create an interactive point symbol map.\n\n\nCode\ntm_shape(mpsz)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nTo proof that our observation is indeed true, the Moran's I test will be performed\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\n\nCode\nnb &lt;- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nsummary(nb)\n\n\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\n\nNext, nb2listw() of spdep packge will be used to convert the output neighbours lists (i.e. nb) into a spatial weights.\n\n\nCode\nnb_lw &lt;- nb2listw(nb, style = 'W')\nsummary(nb_lw)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 1436 \nNumber of nonzero links: 66266 \nPercentage nonzero weights: 3.213526 \nAverage number of links: 46.14624 \n10 disjoint connected subgraphs\nLink number distribution:\n\n  1   3   5   7   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24 \n  3   3   9   4   3  15  10  19  17  45  19   5  14  29  19   6  35  45  18  47 \n 25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44 \n 16  43  22  26  21  11   9  23  22  13  16  25  21  37  16  18   8  21   4  12 \n 45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64 \n  8  36  18  14  14  43  11  12   8  13  12  13   4   5   6  12  11  20  29  33 \n 65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84 \n 15  20  10  14  15  15  11  16  12  10   8  19  12  14   9   8   4  13  11   6 \n 85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 \n  4   9   4   4   4   6   2  16   9   4   5   9   3   9   4   2   1   2   1   1 \n105 106 107 108 109 110 112 116 125 \n  1   5   9   2   1   3   1   1   1 \n3 least connected regions:\n193 194 277 with 1 link\n1 most connected region:\n285 with 125 links\n\nWeights style: W \nWeights constants summary:\n     n      nn   S0       S1       S2\nW 1436 2062096 1436 94.81916 5798.341\n\n\nNext, lm.morantest() of spdep package will be used to perform Moran's I test for residual spatial autocorrelation\n\n\nCode\nlm.morantest(condo.mlr1, nb_lw)\n\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nThe Global Moran's I test for residual spatial autocorrelation shows that it's p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution."
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#building-hedonic-pricing-models-using-gwmodel",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#building-hedonic-pricing-models-using-gwmodel",
    "title": "Hands on Excercise 4",
    "section": "5 Building Hedonic Pricing Models using GWmodel",
    "text": "5 Building Hedonic Pricing Models using GWmodel\nIn this section, you are going to learn how to modelling hedonic pricing using both the fixed and adaptive bandwidth schemes\n\n5.1 Building Fixed Bandwidth GWR Model\n\n5.1.1 Computing fixed bandwith\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model. Notice that the argument adaptive is set to FALSE indicates that we are interested to compute the fixed bandwidth.\nThere are two possible approaches can be uused to determine the stopping rule, they are: CV cross-validation approach and AIC corrected (AICc) approach. We define the stopping rule using approach argeement.\n\n\nCode\nbw.fixed &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.379526e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3396 CV score: 4.721292e+14 \nFixed bandwidth: 971.3402 CV score: 4.721292e+14 \nFixed bandwidth: 971.3398 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3399 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3405 metres.\n\n\n\n\n\n\nTip\n\n\n\n(Quiz: Do you know why it is in metre?)\nCause we used “st_transform(crs = 3414)” from the begining to change the coordinate from decimal degree to meters.\n\n\n\n\n5.1.2 GWModel method - fixed bandwith\nNow we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\n\nCode\ngwr.fixed &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\n\nThe output is saved in a list of class \"gwrm\". The code below can be used to display the model output.\n\n\nCode\ngwr.fixed\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-12-05 21:54:55.608436 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.34 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3599e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7426e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5001e+06 -1.5970e+05  3.1970e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8074e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112794435\n   AREA_SQM                 21575\n   AGE                     434203\n   PROX_CBD               2704604\n   PROX_CHILDCARE         1654086\n   PROX_ELDERLYCARE      38867861\n   PROX_URA_GROWTH_AREA  78515805\n   PROX_MRT               3124325\n   PROX_PARK             18122439\n   PROX_PRIMARY_SCH       4637517\n   PROX_SHOPPING_MALL     1529953\n   PROX_BUS_STOP         11342209\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720745\n   FREEHOLD               6073642\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3807 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6193 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.534069e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430418 \n\n   ***********************************************************************\n   Program stops at: 2023-12-05 21:54:56.785658 \n\n\nThe report shows that the AICc of the gwr is 42263.61 which is significantly smaller than the globel multiple linear regression model of 42967.1.\n\n\n\n5.2 Building Adaptive Bandwidth GWR Model\nIn this section, we will calibrate the gwr-based hedonic pricing model by using adaptive bandwidth approach.\n\n5.2.1 Computing the adaptive bandwidth\nSimilar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.\nThe code chunk used look very similar to the one used to compute the fixed bandwidth except the adaptive argument has changed to TRUE.\n\n\nCode\nbw.adaptive &lt;- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result shows that the 30 is the recommended data points to be used.\n\n\n5.2.2 Constructing the adaptive bandwidth gwr model\nNow, we can go ahead to calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\n\nCode\ngwr.adaptive &lt;- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\n\n\n\n\nCode\ngwr.adaptive\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2023-12-05 21:55:05.866257 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  &lt; 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  &lt; 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  &lt; 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  &lt; 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: &lt; 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2023-12-05 21:55:07.386846 \n\n\nThe report shows that the AICc the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61.\n\n\n\n5.3 Visualising GWR Output\nIn addition to regression residuals, the output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors:\n\nCondition Number: this diagnostic evaluates local collinearity. In the presence of strong local collinearity, results become unstable. Results associated with condition numbers larger than 30, may be unreliable.\nLocal R2: these values range between 0.0 and 1.0 and indicate how well the local regression model fits observed y values. Very low values indicate the local model is performing poorly. Mapping the Local R2 values to see where GWR predicts well and where it predicts poorly may provide clues about important variables that may be missing from the regression model.\nPredicted: these are the estimated (or fitted) y values 3. computed by GWR.\nResiduals: to obtain the residual values, the fitted y values are subtracted from the observed y values. Standardized residuals have a mean of zero and a standard deviation of 1. A cold-to-hot rendered map of standardized residuals can be produce by using these values.\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\n\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its \"data\" slot in an object called SDF of the output list.\n\n\n5.4 Converting SDF into sf data.frame\nTo visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.\n\n\nCode\ncondo_resale.sf.adaptive &lt;- st_as_sf(gwr.adaptive$SDF) %&gt;%\n  st_transform(crs=3414)\n\n\n\n\nCode\ncondo_resale.sf.adaptive.svy21 &lt;- st_transform(condo_resale.sf.adaptive, 3414)\ngwr.adaptive.output &lt;- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive &lt;- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nglimpse(condo_resale.sf.adaptive)\n\n\nRows: 1,436\nColumns: 77\n$ POSTCODE                &lt;dbl&gt; 118635, 288420, 267833, 258380, 467169, 466472…\n$ SELLING_PRICE           &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ AREA_SQM                &lt;dbl&gt; 309, 290, 248, 127, 145, 139, 218, 141, 165, 1…\n$ AGE                     &lt;dbl&gt; 30, 32, 33, 7, 28, 22, 24, 24, 27, 31, 17, 22,…\n$ PROX_CBD                &lt;dbl&gt; 7.941259, 6.609797, 6.898000, 4.038861, 11.783…\n$ PROX_CHILDCARE          &lt;dbl&gt; 0.16597932, 0.28027246, 0.42922669, 0.39473543…\n$ PROX_ELDERLYCARE        &lt;dbl&gt; 2.5198118, 1.9333338, 0.5021395, 1.9910316, 1.…\n$ PROX_URA_GROWTH_AREA    &lt;dbl&gt; 6.618741, 7.505109, 6.463887, 4.906512, 6.4106…\n$ PROX_HAWKER_MARKET      &lt;dbl&gt; 1.76542207, 0.54507614, 0.37789301, 1.68259969…\n$ PROX_KINDERGARTEN       &lt;dbl&gt; 0.05835552, 0.61592412, 0.14120309, 0.38200076…\n$ PROX_MRT                &lt;dbl&gt; 0.5607188, 0.6584461, 0.3053433, 0.6910183, 0.…\n$ PROX_PARK               &lt;dbl&gt; 1.1710446, 0.1992269, 0.2779886, 0.9832843, 0.…\n$ PROX_PRIMARY_SCH        &lt;dbl&gt; 1.6340256, 0.9747834, 1.4715016, 1.4546324, 0.…\n$ PROX_TOP_PRIMARY_SCH    &lt;dbl&gt; 3.3273195, 0.9747834, 1.4715016, 2.3006394, 0.…\n$ PROX_SHOPPING_MALL      &lt;dbl&gt; 2.2102717, 2.9374279, 1.2256850, 0.3525671, 1.…\n$ PROX_SUPERMARKET        &lt;dbl&gt; 0.9103958, 0.5900617, 0.4135583, 0.4162219, 0.…\n$ PROX_BUS_STOP           &lt;dbl&gt; 0.10336166, 0.28673408, 0.28504777, 0.29872340…\n$ NO_Of_UNITS             &lt;dbl&gt; 18, 20, 27, 30, 30, 31, 32, 32, 32, 32, 34, 34…\n$ FAMILY_FRIENDLY         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ FREEHOLD                &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1…\n$ LEASEHOLD_99YR          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ LOG_SELLING_PRICE       &lt;dbl&gt; 14.91412, 15.17135, 15.01698, 15.26243, 14.151…\n$ MLR_RES                 &lt;dbl&gt; -1489099.55, 415494.57, 194129.69, 1088992.71,…\n$ Intercept               &lt;dbl&gt; 2050011.67, 1633128.24, 3433608.17, 234358.91,…\n$ AREA_SQM.1              &lt;dbl&gt; 9561.892, 16576.853, 13091.861, 20730.601, 672…\n$ AGE.1                   &lt;dbl&gt; -9514.634, -58185.479, -26707.386, -93308.988,…\n$ PROX_CBD.1              &lt;dbl&gt; -120681.94, -149434.22, -259397.77, 2426853.66…\n$ PROX_CHILDCARE.1        &lt;dbl&gt; 319266.925, 441102.177, -120116.816, 480825.28…\n$ PROX_ELDERLYCARE.1      &lt;dbl&gt; -393417.795, 325188.741, 535855.806, 314783.72…\n$ PROX_URA_GROWTH_AREA.1  &lt;dbl&gt; -159980.203, -142290.389, -253621.206, -267929…\n$ PROX_MRT.1              &lt;dbl&gt; -299742.96, -2510522.23, -936853.28, -2039479.…\n$ PROX_PARK.1             &lt;dbl&gt; -172104.47, 523379.72, 209099.85, -759153.26, …\n$ PROX_PRIMARY_SCH.1      &lt;dbl&gt; 242668.03, 1106830.66, 571462.33, 3127477.21, …\n$ PROX_SHOPPING_MALL.1    &lt;dbl&gt; 300881.390, -87693.378, -126732.712, -29593.34…\n$ PROX_BUS_STOP.1         &lt;dbl&gt; 1210615.44, 1843587.22, 1411924.90, 7225577.51…\n$ NO_Of_UNITS.1           &lt;dbl&gt; 104.8290640, -288.3441183, -9.5532945, -161.35…\n$ FAMILY_FRIENDLY.1       &lt;dbl&gt; -9075.370, 310074.664, 5949.746, 1556178.531, …\n$ FREEHOLD.1              &lt;dbl&gt; 303955.61, 396221.27, 168821.75, 1212515.58, 3…\n$ y                       &lt;dbl&gt; 3000000, 3880000, 3325000, 4250000, 1400000, 1…\n$ yhat                    &lt;dbl&gt; 2886531.8, 3466801.5, 3616527.2, 5435481.6, 13…\n$ residual                &lt;dbl&gt; 113468.16, 413198.52, -291527.20, -1185481.63,…\n$ CV_Score                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Stud_residual           &lt;dbl&gt; 0.38207013, 1.01433140, -0.83780678, -2.846146…\n$ Intercept_SE            &lt;dbl&gt; 516105.5, 488083.5, 963711.4, 444185.5, 211962…\n$ AREA_SQM_SE             &lt;dbl&gt; 823.2860, 825.2380, 988.2240, 617.4007, 1376.2…\n$ AGE_SE                  &lt;dbl&gt; 5889.782, 6226.916, 6510.236, 6010.511, 8180.3…\n$ PROX_CBD_SE             &lt;dbl&gt; 37411.22, 23615.06, 56103.77, 469337.41, 41064…\n$ PROX_CHILDCARE_SE       &lt;dbl&gt; 319111.1, 299705.3, 349128.5, 304965.2, 698720…\n$ PROX_ELDERLYCARE_SE     &lt;dbl&gt; 120633.34, 84546.69, 129687.07, 127150.69, 327…\n$ PROX_URA_GROWTH_AREA_SE &lt;dbl&gt; 56207.39, 76956.50, 95774.60, 470762.12, 47433…\n$ PROX_MRT_SE             &lt;dbl&gt; 185181.3, 281133.9, 275483.7, 279877.1, 363830…\n$ PROX_PARK_SE            &lt;dbl&gt; 205499.6, 229358.7, 314124.3, 227249.4, 364580…\n$ PROX_PRIMARY_SCH_SE     &lt;dbl&gt; 152400.7, 165150.7, 196662.6, 240878.9, 249087…\n$ PROX_SHOPPING_MALL_SE   &lt;dbl&gt; 109268.8, 98906.8, 119913.3, 177104.1, 301032.…\n$ PROX_BUS_STOP_SE        &lt;dbl&gt; 600668.6, 410222.1, 464156.7, 562810.8, 740922…\n$ NO_Of_UNITS_SE          &lt;dbl&gt; 218.1258, 208.9410, 210.9828, 361.7767, 299.50…\n$ FAMILY_FRIENDLY_SE      &lt;dbl&gt; 131474.73, 114989.07, 146607.22, 108726.62, 16…\n$ FREEHOLD_SE             &lt;dbl&gt; 115954.0, 130110.0, 141031.5, 138239.1, 210641…\n$ Intercept_TV            &lt;dbl&gt; 3.9720784, 3.3460017, 3.5629010, 0.5276150, 1.…\n$ AREA_SQM_TV             &lt;dbl&gt; 11.614302, 20.087361, 13.247868, 33.577223, 4.…\n$ AGE_TV                  &lt;dbl&gt; -1.6154474, -9.3441881, -4.1023685, -15.524301…\n$ PROX_CBD_TV             &lt;dbl&gt; -3.22582173, -6.32792021, -4.62353528, 5.17080…\n$ PROX_CHILDCARE_TV       &lt;dbl&gt; 1.000488185, 1.471786337, -0.344047555, 1.5766…\n$ PROX_ELDERLYCARE_TV     &lt;dbl&gt; -3.26126929, 3.84626245, 4.13191383, 2.4756745…\n$ PROX_URA_GROWTH_AREA_TV &lt;dbl&gt; -2.846248368, -1.848971738, -2.648105057, -5.6…\n$ PROX_MRT_TV             &lt;dbl&gt; -1.61864578, -8.92998600, -3.40075727, -7.2870…\n$ PROX_PARK_TV            &lt;dbl&gt; -0.83749312, 2.28192684, 0.66565951, -3.340617…\n$ PROX_PRIMARY_SCH_TV     &lt;dbl&gt; 1.59230221, 6.70194543, 2.90580089, 12.9836104…\n$ PROX_SHOPPING_MALL_TV   &lt;dbl&gt; 2.753588422, -0.886626400, -1.056869486, -0.16…\n$ PROX_BUS_STOP_TV        &lt;dbl&gt; 2.0154464, 4.4941192, 3.0419145, 12.8383775, 0…\n$ NO_Of_UNITS_TV          &lt;dbl&gt; 0.480589953, -1.380026395, -0.045279967, -0.44…\n$ FAMILY_FRIENDLY_TV      &lt;dbl&gt; -0.06902748, 2.69655779, 0.04058290, 14.312764…\n$ FREEHOLD_TV             &lt;dbl&gt; 2.6213469, 3.0452799, 1.1970499, 8.7711485, 1.…\n$ Local_R2                &lt;dbl&gt; 0.8846744, 0.8899773, 0.8947007, 0.9073605, 0.…\n$ coords.x1               &lt;dbl&gt; 22085.12, 25656.84, 23963.99, 27044.28, 41042.…\n$ coords.x2               &lt;dbl&gt; 29951.54, 34546.20, 32890.80, 32319.77, 33743.…\n$ geometry                &lt;POINT [m]&gt; POINT (22085.12 29951.54), POINT (25656.…\n\n\nSummary of yhat of SDF.\n\n\nCode\nsummary(gwr.adaptive$SDF$yhat)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n5.5 Visualising local R2\nThe code chunks below is used to create an interactive point symbol map.\n\n\nCode\n#tmap_mode(\"view\")\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\n\n\n\n\n\n5.6 Visualising coefficient estimates\nThe code chunks below is used to create an interactive point symbol map.\n\n\nCode\n#tmap_mode(\"view\")\nAREA_SQM_SE &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV &lt;- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\n\n\n\n\n\n5.6.1 By URA Plannign Region\n\n\nCode\ntm_shape(mpsz[mpsz$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)"
  },
  {
    "objectID": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#reference",
    "href": "Handson_Ex/Handson_Ex04/Handson_Ex04.html#reference",
    "title": "Hands on Excercise 4",
    "section": "6 Reference",
    "text": "6 Reference\nGollini I, Lu B, Charlton M, Brunsdon C, Harris P (2015) \"GWmodel: an R Package for exploring Spatial Heterogeneity using Geographically Weighted Models\". Journal of Statistical Software, 63(17):1-50, http://www.jstatsoft.org/v63/i17/\nLu B, Harris P, Charlton M, Brunsdon C (2014) \"The GWmodel R Package: further topics for exploring Spatial Heterogeneity using GeographicallyWeighted Models\". Geo-spatial Information Science 17(2): 85-101, http://www.tandfonline.com/doi/abs/10.1080/1009502.2014.917453"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#get-ready",
    "href": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#get-ready",
    "title": "In Class Exercise 4",
    "section": "2 Get ready",
    "text": "2 Get ready\n\n\nCode\npacman::p_load(sf, tmap, tidyverse, httr, ggpubr)"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#geocoding-using-sla-api",
    "href": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#geocoding-using-sla-api",
    "title": "In Class Exercise 4",
    "section": "3 Geocoding using SLA API",
    "text": "3 Geocoding using SLA API\n\n3.1 Geospatial data.\nImport mpsz data.\n\n\nCode\nmpsz = st_read(dsn = \"./data/geospatial\",\n                   layer = \"MP14_SUBZONE_WEB_PL\")  %&gt;% st_transform(crs = 3414) \n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `/Users/SMU/liangyao2023/ISSS624/Inclass_Ex/Inclass_Ex04/data/geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\nCode\nurl &lt;- \"https://www.onemap.gov.sg/api/common/elastic/search\"\n\n\n\n\nCode\ncsv &lt;- read_csv(\"./data/aspatial/Generalinformationofschools.csv\")\npostcodes &lt;- csv$'postal_code'\n\n\n\n\nCode\nfound &lt;- data.frame()\nnot_found &lt;- data.frame()\n\nfor(postcode in postcodes){\n  query&lt;- list('searchVal'=postcode, 'returnGeom'='Y', 'getAddrDetails'='Y', 'pageNum'='1')\n  res &lt;- GET(url, query = query)\n  \n  if((content(res)$found)!=0){\n    found &lt;- rbind(found, data.frame(content(res))[4:13])\n  } else{\n    not_found = data.frame(postcode)\n  }\n}\n\n\nCombine found and not_found data.\n\n\nCode\nmerged &lt;- merge(csv, found, by.x = 'postal_code', by.y = 'results.POSTAL', all = TRUE) %&gt;%\n  drop_na()\n\n\nSave the results.\n\n\nCode\nwrite.csv(merged, file = './data/aspatial/schools.csv')\nwrite.csv(not_found, file = './data/aspatial/not_found.csv')\n\n\n\n\n3.2 Aspatial Data\nImporting schools data.\n\n\nCode\nschools &lt;- read_csv('./data/aspatial/schools.csv')\n\n\nWrangling data.\n\n\nCode\nschools &lt;- schools %&gt;%\n  rename(latitude = results.LATITUDE,\n         longitude = results.LONGITUDE) %&gt;%\n  select(postal_code, school_name, latitude, longitude)\n\n\nConverting an aspatial data into sf tibble data.frame\n\n\nCode\nschools_sf &lt;- st_as_sf(schools, \n                       coords = c(\"longitude\",\"latitude\"),\n                       crs = 4326) %&gt;%\n  st_transform(crs = 3414)\n\n\n\n\n\n\n\n\nTip\n\n\n\nfor ‘coords’, always pass longitude firstly and then latitude."
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#plotting",
    "href": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#plotting",
    "title": "In Class Exercise 4",
    "section": "4 Plotting",
    "text": "4 Plotting\n\n\nCode\ntm_shape(mpsz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\ntm_shape(schools_sf) +\n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAlways remember to set tm_view(set.zoom.limits = c(11,14)) to restrict zoom if you use tmap_mode(‘view’).\n\n\nCount schools in mpsz and check the summary.\n\n\nCode\nmpsz$'SCHOOL_COUNT' &lt;- lengths(\n  st_intersects(mpsz, schools_sf)\n)\n\nsummary(mpsz$SCHOOL_COUNT)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00    0.00    1.08    2.00   12.00 \n\n\nPlot out.\n\n\nCode\ntm_shape(mpsz) +\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons() +\n  tm_dots(size = 'SCHOOL_COUNT',\n          col = \"navy\",\n          alpha = 0.6)"
  },
  {
    "objectID": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#flow-line-analysis",
    "href": "Inclass_Ex/Inclass_Ex04/Inclass_Ex04.html#flow-line-analysis",
    "title": "In Class Exercise 4",
    "section": "5 Flow line analysis",
    "text": "5 Flow line analysis\nRead flow line rds file.\n\n\nCode\nflow_data &lt;- read_rds('./data/rds/flow_data_tidy.rds')\n\nglimpse(flow_data)\n\n\nRows: 14,734\nColumns: 13\n$ ORIGIN_SZ       &lt;chr&gt; \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMS…\n$ DESTIN_SZ       &lt;chr&gt; \"AMSZ01\", \"AMSZ02\", \"AMSZ03\", \"AMSZ04\", \"AMSZ05\", \"AMS…\n$ MORNING_PEAK    &lt;dbl&gt; 1998, 8289, 8971, 2252, 6136, 2148, 1620, 1925, 1773, …\n$ dist            &lt;dbl&gt; 50.0000, 810.4491, 1360.9294, 840.4432, 1076.7916, 805…\n$ ORIGIN_AGE7_12  &lt;dbl&gt; 310, 310, 310, 310, 310, 310, 310, 310, 310, 310, 310,…\n$ ORIGIN_AGE13_24 &lt;dbl&gt; 710, 710, 710, 710, 710, 710, 710, 710, 710, 710, 710,…\n$ ORIGIN_AGE25_64 &lt;dbl&gt; 2780, 2780, 2780, 2780, 2780, 2780, 2780, 2780, 2780, …\n$ DESTIN_AGE7_12  &lt;dbl&gt; 310.00, 1140.00, 1010.00, 980.00, 810.00, 1050.00, 420…\n$ DESTIN_AGE13_24 &lt;dbl&gt; 710.00, 2770.00, 2650.00, 2000.00, 1920.00, 2390.00, 1…\n$ DESTIN_AGE25_64 &lt;dbl&gt; 2780.00, 15700.00, 14240.00, 11320.00, 9650.00, 12460.…\n$ SCHOOL_COUNT    &lt;dbl&gt; 0.99, 2.00, 2.00, 1.00, 3.00, 2.00, 0.99, 0.99, 3.00, …\n$ RETAIL_COUNT    &lt;dbl&gt; 1.00, 0.99, 6.00, 0.99, 0.99, 0.99, 1.00, 117.00, 0.99…\n$ geometry        &lt;LINESTRING [m]&gt; LINESTRING (29501.77 39419...., LINESTRING …\n\n\nExtract inter-zonal flow data.\n\n\nCode\nflow_data$FlowNoIntra &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 0, flow_data$MORNING_PEAK)\nflow_data$offset &lt;- ifelse(\n  flow_data$ORIGIN_SZ == flow_data$DESTIN_SZ, 0.000001,1)\n\ninter_zonal_flow &lt;- flow_data %&gt;% filter(FlowNoIntra &gt; 0)\n\ninter_zonal_flow &lt;- inter_zonal_flow %&gt;%\n  rename(TRIPS = MORNING_PEAK,\n         DIST = dist)\n\n\n\n5.1 Origin constrained SIM\n\n\nCode\norcSIM_Poisson &lt;- glm(formula = TRIPS ~\n                        ORIGIN_SZ +\n                        log(SCHOOL_COUNT) +\n                        log(RETAIL_COUNT) +\n                        log(DIST) - 1,\n                      family = poisson(link = 'log'),\n                      data = inter_zonal_flow,\n                      na.action = na.exclude)  # this line is just in case\nsummary(orcSIM_Poisson)\n\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + log(SCHOOL_COUNT) + log(RETAIL_COUNT) + \n    log(DIST) - 1, family = poisson(link = \"log\"), data = inter_zonal_flow, \n    na.action = na.exclude)\n\nCoefficients:\n                    Estimate Std. Error  z value Pr(&gt;|z|)    \nORIGIN_SZAMSZ01   19.8739840  0.0047627  4172.84   &lt;2e-16 ***\nORIGIN_SZAMSZ02   20.5902203  0.0042786  4812.33   &lt;2e-16 ***\nORIGIN_SZAMSZ03   20.2327026  0.0045531  4443.70   &lt;2e-16 ***\nORIGIN_SZAMSZ04   19.7744438  0.0049837  3967.79   &lt;2e-16 ***\nORIGIN_SZAMSZ05   19.6574529  0.0056396  3485.61   &lt;2e-16 ***\nORIGIN_SZAMSZ06   19.9659115  0.0048946  4079.16   &lt;2e-16 ***\nORIGIN_SZAMSZ07   18.6746164  0.0096316  1938.90   &lt;2e-16 ***\nORIGIN_SZAMSZ08   19.2701601  0.0090776  2122.82   &lt;2e-16 ***\nORIGIN_SZAMSZ09   19.9889467  0.0052858  3781.64   &lt;2e-16 ***\nORIGIN_SZAMSZ10   20.3422035  0.0045778  4443.62   &lt;2e-16 ***\nORIGIN_SZAMSZ11   18.3944113  0.0129212  1423.58   &lt;2e-16 ***\nORIGIN_SZAMSZ12   18.3484209  0.0109652  1673.33   &lt;2e-16 ***\nORIGIN_SZBDSZ01   20.9668587  0.0043388  4832.36   &lt;2e-16 ***\nORIGIN_SZBDSZ02   20.4059518  0.0050601  4032.75   &lt;2e-16 ***\nORIGIN_SZBDSZ03   20.6725514  0.0045276  4565.93   &lt;2e-16 ***\nORIGIN_SZBDSZ04   21.6703853  0.0038930  5566.44   &lt;2e-16 ***\nORIGIN_SZBDSZ05   20.7497445  0.0046085  4502.46   &lt;2e-16 ***\nORIGIN_SZBDSZ06   20.9119361  0.0046432  4503.77   &lt;2e-16 ***\nORIGIN_SZBDSZ07   18.9749815  0.0097896  1938.28   &lt;2e-16 ***\nORIGIN_SZBDSZ08   19.1933901  0.0091312  2101.95   &lt;2e-16 ***\nORIGIN_SZBKSZ01   19.5422606  0.0064732  3018.96   &lt;2e-16 ***\nORIGIN_SZBKSZ02   20.1748913  0.0050076  4028.89   &lt;2e-16 ***\nORIGIN_SZBKSZ03   20.3984624  0.0047226  4319.35   &lt;2e-16 ***\nORIGIN_SZBKSZ04   19.6182212  0.0059652  3288.76   &lt;2e-16 ***\nORIGIN_SZBKSZ05   19.6033818  0.0063181  3102.74   &lt;2e-16 ***\nORIGIN_SZBKSZ06   19.7145224  0.0056372  3497.20   &lt;2e-16 ***\nORIGIN_SZBKSZ07   20.4237448  0.0041912  4873.03   &lt;2e-16 ***\nORIGIN_SZBKSZ08   19.7992538  0.0050405  3928.02   &lt;2e-16 ***\nORIGIN_SZBKSZ09   19.7821586  0.0055558  3560.66   &lt;2e-16 ***\nORIGIN_SZBLSZ01   17.7977276  0.0149058  1194.01   &lt;2e-16 ***\nORIGIN_SZBLSZ02   17.4287491  0.0192364   906.03   &lt;2e-16 ***\nORIGIN_SZBLSZ03   16.5884288  0.0459848   360.74   &lt;2e-16 ***\nORIGIN_SZBLSZ04   17.7851626  0.0232823   763.89   &lt;2e-16 ***\nORIGIN_SZBMSZ01   20.0751840  0.0052887  3795.89   &lt;2e-16 ***\nORIGIN_SZBMSZ02   18.6956140  0.0066656  2804.80   &lt;2e-16 ***\nORIGIN_SZBMSZ03   19.3204425  0.0054755  3528.56   &lt;2e-16 ***\nORIGIN_SZBMSZ04   19.4724220  0.0049390  3942.59   &lt;2e-16 ***\nORIGIN_SZBMSZ05   16.9581801  0.0168804  1004.61   &lt;2e-16 ***\nORIGIN_SZBMSZ06   16.9898638  0.0181852   934.27   &lt;2e-16 ***\nORIGIN_SZBMSZ07   19.2868403  0.0056231  3429.91   &lt;2e-16 ***\nORIGIN_SZBMSZ08   19.1477543  0.0055918  3424.28   &lt;2e-16 ***\nORIGIN_SZBMSZ09   18.7564539  0.0086298  2173.46   &lt;2e-16 ***\nORIGIN_SZBMSZ10   18.3617854  0.0089250  2057.35   &lt;2e-16 ***\nORIGIN_SZBMSZ11   18.9167941  0.0063340  2986.54   &lt;2e-16 ***\nORIGIN_SZBMSZ12   18.7874661  0.0093024  2019.63   &lt;2e-16 ***\nORIGIN_SZBMSZ13   19.5654046  0.0057517  3401.70   &lt;2e-16 ***\nORIGIN_SZBMSZ14   19.0685619  0.0063346  3010.24   &lt;2e-16 ***\nORIGIN_SZBMSZ15   19.4403124  0.0058147  3343.30   &lt;2e-16 ***\nORIGIN_SZBMSZ16   18.4469203  0.0092638  1991.28   &lt;2e-16 ***\nORIGIN_SZBMSZ17   18.3430175  0.0157692  1163.22   &lt;2e-16 ***\nORIGIN_SZBPSZ01   20.1806714  0.0053660  3760.81   &lt;2e-16 ***\nORIGIN_SZBPSZ02   19.8116707  0.0061485  3222.19   &lt;2e-16 ***\nORIGIN_SZBPSZ03   19.8467602  0.0059769  3320.57   &lt;2e-16 ***\nORIGIN_SZBPSZ04   20.4613200  0.0048398  4227.72   &lt;2e-16 ***\nORIGIN_SZBPSZ05   20.5379711  0.0043769  4692.39   &lt;2e-16 ***\nORIGIN_SZBPSZ06   18.8948034  0.0093668  2017.21   &lt;2e-16 ***\nORIGIN_SZBPSZ07   19.4104568  0.0087961  2206.70   &lt;2e-16 ***\nORIGIN_SZBSSZ01   20.0139503  0.0056561  3538.45   &lt;2e-16 ***\nORIGIN_SZBSSZ02   20.2543885  0.0047198  4291.38   &lt;2e-16 ***\nORIGIN_SZBSSZ03   19.5428803  0.0052713  3707.41   &lt;2e-16 ***\nORIGIN_SZBTSZ01   20.0198045  0.0058541  3419.77   &lt;2e-16 ***\nORIGIN_SZBTSZ02   19.3618525  0.0081472  2376.51   &lt;2e-16 ***\nORIGIN_SZBTSZ03   19.5883853  0.0068935  2841.59   &lt;2e-16 ***\nORIGIN_SZBTSZ04   18.7720238  0.0103909  1806.58   &lt;2e-16 ***\nORIGIN_SZBTSZ05   18.8069026  0.0120628  1559.08   &lt;2e-16 ***\nORIGIN_SZBTSZ06   18.7068633  0.0094575  1978.00   &lt;2e-16 ***\nORIGIN_SZBTSZ07   17.6292257  0.0141551  1245.43   &lt;2e-16 ***\nORIGIN_SZBTSZ08   18.6989374  0.0109610  1705.94   &lt;2e-16 ***\nORIGIN_SZCBSZ01   18.2189868  0.0548317   332.27   &lt;2e-16 ***\nORIGIN_SZCCSZ01   18.9734563  0.0139450  1360.59   &lt;2e-16 ***\nORIGIN_SZCHSZ01   19.5955119  0.0121035  1619.00   &lt;2e-16 ***\nORIGIN_SZCHSZ02   19.3320960  0.0081620  2368.55   &lt;2e-16 ***\nORIGIN_SZCHSZ03   21.2164518  0.0063552  3338.43   &lt;2e-16 ***\nORIGIN_SZCKSZ01   20.1046845  0.0049333  4075.29   &lt;2e-16 ***\nORIGIN_SZCKSZ02   20.5371946  0.0050256  4086.53   &lt;2e-16 ***\nORIGIN_SZCKSZ03   20.7210560  0.0042184  4912.07   &lt;2e-16 ***\nORIGIN_SZCKSZ04   21.4013886  0.0042524  5032.80   &lt;2e-16 ***\nORIGIN_SZCKSZ05   20.9413146  0.0049434  4236.18   &lt;2e-16 ***\nORIGIN_SZCKSZ06   20.2557727  0.0071832  2819.88   &lt;2e-16 ***\nORIGIN_SZCLSZ01   19.3383703  0.0076634  2523.46   &lt;2e-16 ***\nORIGIN_SZCLSZ02   18.5226956  0.0135522  1366.77   &lt;2e-16 ***\nORIGIN_SZCLSZ03   19.0225512  0.0080145  2373.51   &lt;2e-16 ***\nORIGIN_SZCLSZ04   20.7981505  0.0042400  4905.22   &lt;2e-16 ***\nORIGIN_SZCLSZ05   18.3015625  0.0146815  1246.58   &lt;2e-16 ***\nORIGIN_SZCLSZ06   20.8207386  0.0039567  5262.09   &lt;2e-16 ***\nORIGIN_SZCLSZ07   19.6728958  0.0054199  3629.76   &lt;2e-16 ***\nORIGIN_SZCLSZ08   20.0851929  0.0056956  3526.43   &lt;2e-16 ***\nORIGIN_SZCLSZ09   18.5749589  0.0165415  1122.93   &lt;2e-16 ***\nORIGIN_SZDTSZ02   15.8276209  0.0833992   189.78   &lt;2e-16 ***\nORIGIN_SZDTSZ03   16.2512838  0.0737972   220.22   &lt;2e-16 ***\nORIGIN_SZDTSZ13   16.7744385  0.0312450   536.87   &lt;2e-16 ***\nORIGIN_SZGLSZ01   18.2368248  0.0096104  1897.62   &lt;2e-16 ***\nORIGIN_SZGLSZ02   19.8705255  0.0049014  4054.06   &lt;2e-16 ***\nORIGIN_SZGLSZ03   19.8249435  0.0053109  3732.85   &lt;2e-16 ***\nORIGIN_SZGLSZ04   20.7800335  0.0041261  5036.20   &lt;2e-16 ***\nORIGIN_SZGLSZ05   20.6040494  0.0043049  4786.23   &lt;2e-16 ***\nORIGIN_SZHGSZ01   20.0273475  0.0044824  4468.04   &lt;2e-16 ***\nORIGIN_SZHGSZ02   20.2480656  0.0044575  4542.47   &lt;2e-16 ***\nORIGIN_SZHGSZ03   20.0756442  0.0049003  4096.81   &lt;2e-16 ***\nORIGIN_SZHGSZ04   20.7577748  0.0040465  5129.84   &lt;2e-16 ***\nORIGIN_SZHGSZ05   20.9779992  0.0040123  5228.42   &lt;2e-16 ***\nORIGIN_SZHGSZ06   19.7403058  0.0054229  3640.20   &lt;2e-16 ***\nORIGIN_SZHGSZ07   20.1896268  0.0046051  4384.22   &lt;2e-16 ***\nORIGIN_SZHGSZ08   19.8646492  0.0052403  3790.72   &lt;2e-16 ***\nORIGIN_SZHGSZ09   18.3647736  0.0069196  2654.04   &lt;2e-16 ***\nORIGIN_SZHGSZ10   16.8720475  0.0421046   400.72   &lt;2e-16 ***\nORIGIN_SZJESZ01   20.2673794  0.0046723  4337.79   &lt;2e-16 ***\nORIGIN_SZJESZ02   20.0595982  0.0046503  4313.61   &lt;2e-16 ***\nORIGIN_SZJESZ03   19.9128778  0.0049848  3994.75   &lt;2e-16 ***\nORIGIN_SZJESZ04   18.5053667  0.0099227  1864.94   &lt;2e-16 ***\nORIGIN_SZJESZ05   17.8172930  0.0138840  1283.29   &lt;2e-16 ***\nORIGIN_SZJESZ06   20.0124157  0.0045009  4446.36   &lt;2e-16 ***\nORIGIN_SZJESZ07   18.1821423  0.0117267  1550.49   &lt;2e-16 ***\nORIGIN_SZJESZ08   18.8713046  0.0116456  1620.46   &lt;2e-16 ***\nORIGIN_SZJESZ09   20.5535527  0.0048456  4241.72   &lt;2e-16 ***\nORIGIN_SZJESZ10   18.4922322  0.0191243   966.95   &lt;2e-16 ***\nORIGIN_SZJESZ11   18.2891211  0.0197114   927.85   &lt;2e-16 ***\nORIGIN_SZJWSZ01   20.4912737  0.0063102  3247.35   &lt;2e-16 ***\nORIGIN_SZJWSZ02   20.8236694  0.0042249  4928.82   &lt;2e-16 ***\nORIGIN_SZJWSZ03   21.2587613  0.0039733  5350.40   &lt;2e-16 ***\nORIGIN_SZJWSZ04   20.3816464  0.0046199  4411.67   &lt;2e-16 ***\nORIGIN_SZJWSZ05   18.0607448  0.0128857  1401.61   &lt;2e-16 ***\nORIGIN_SZJWSZ06   18.7015202  0.0107614  1737.83   &lt;2e-16 ***\nORIGIN_SZJWSZ07   17.3991822  0.0277096   627.91   &lt;2e-16 ***\nORIGIN_SZJWSZ08   21.8044465  0.0037356  5836.95   &lt;2e-16 ***\nORIGIN_SZJWSZ09   21.5414930  0.0036033  5978.19   &lt;2e-16 ***\nORIGIN_SZKLSZ01   20.0307712  0.0047868  4184.59   &lt;2e-16 ***\nORIGIN_SZKLSZ02   19.0634769  0.0062318  3059.05   &lt;2e-16 ***\nORIGIN_SZKLSZ03   19.2685700  0.0057172  3370.25   &lt;2e-16 ***\nORIGIN_SZKLSZ04   17.7085067  0.0119809  1478.06   &lt;2e-16 ***\nORIGIN_SZKLSZ05   18.6384471  0.0107596  1732.26   &lt;2e-16 ***\nORIGIN_SZKLSZ06   13.7280296  0.1857160    73.92   &lt;2e-16 ***\nORIGIN_SZKLSZ07   18.6425146  0.0084952  2194.47   &lt;2e-16 ***\nORIGIN_SZKLSZ08   18.0928506  0.0101567  1781.37   &lt;2e-16 ***\nORIGIN_SZLKSZ01   17.8907138  0.0397083   450.55   &lt;2e-16 ***\nORIGIN_SZMDSZ01   18.7605188  0.0285455   657.22   &lt;2e-16 ***\nORIGIN_SZMDSZ02   19.1533927  0.0102815  1862.90   &lt;2e-16 ***\nORIGIN_SZMDSZ03   17.8404982  0.0169690  1051.36   &lt;2e-16 ***\nORIGIN_SZMPSZ01   19.0765941  0.0083937  2272.74   &lt;2e-16 ***\nORIGIN_SZMPSZ02   19.2162527  0.0068331  2812.24   &lt;2e-16 ***\nORIGIN_SZMPSZ03   19.9965344  0.0054569  3664.44   &lt;2e-16 ***\nORIGIN_SZMUSZ02   15.9130765  0.1037472   153.38   &lt;2e-16 ***\nORIGIN_SZNTSZ01   17.0840999  0.0352513   484.64   &lt;2e-16 ***\nORIGIN_SZNTSZ02   16.5792122  0.0233186   710.99   &lt;2e-16 ***\nORIGIN_SZNTSZ03   18.9506415  0.0075957  2494.93   &lt;2e-16 ***\nORIGIN_SZNTSZ05   15.8770261  0.0495825   320.21   &lt;2e-16 ***\nORIGIN_SZNTSZ06   15.3997415  0.0557029   276.46   &lt;2e-16 ***\nORIGIN_SZNVSZ01   20.2241694  0.0043487  4650.65   &lt;2e-16 ***\nORIGIN_SZNVSZ02   19.1897826  0.0065383  2934.97   &lt;2e-16 ***\nORIGIN_SZNVSZ03   18.8854268  0.0080459  2347.22   &lt;2e-16 ***\nORIGIN_SZNVSZ04   18.8940191  0.0090985  2076.61   &lt;2e-16 ***\nORIGIN_SZNVSZ05   17.6278585  0.0168107  1048.61   &lt;2e-16 ***\nORIGIN_SZPGSZ01   19.4825220  0.0122960  1584.46   &lt;2e-16 ***\nORIGIN_SZPGSZ02   19.4726761  0.0073116  2663.25   &lt;2e-16 ***\nORIGIN_SZPGSZ03   20.5515713  0.0045631  4503.86   &lt;2e-16 ***\nORIGIN_SZPGSZ04   21.0527131  0.0041500  5072.89   &lt;2e-16 ***\nORIGIN_SZPGSZ05   20.1436604  0.0057267  3517.48   &lt;2e-16 ***\nORIGIN_SZPLSZ01   19.1832002  0.0120006  1598.53   &lt;2e-16 ***\nORIGIN_SZPLSZ02   18.8752206  0.0149740  1260.53   &lt;2e-16 ***\nORIGIN_SZPLSZ03   18.1000818  0.0371769   486.86   &lt;2e-16 ***\nORIGIN_SZPLSZ04   17.1730559  0.0370280   463.79   &lt;2e-16 ***\nORIGIN_SZPLSZ05   17.9084439  0.0225031   795.82   &lt;2e-16 ***\nORIGIN_SZPNSZ01   21.0804425  0.0044829  4702.41   &lt;2e-16 ***\nORIGIN_SZPNSZ02   19.8822123  0.0111507  1783.05   &lt;2e-16 ***\nORIGIN_SZPNSZ03   17.9293289  0.0193571   926.24   &lt;2e-16 ***\nORIGIN_SZPNSZ04   17.1039594  0.0334954   510.64   &lt;2e-16 ***\nORIGIN_SZPNSZ05   18.2543864  0.0275554   662.46   &lt;2e-16 ***\nORIGIN_SZPRSZ01   19.8777935  0.0117586  1690.49   &lt;2e-16 ***\nORIGIN_SZPRSZ02   21.0751780  0.0044832  4700.88   &lt;2e-16 ***\nORIGIN_SZPRSZ03   20.6717019  0.0045577  4535.55   &lt;2e-16 ***\nORIGIN_SZPRSZ04   19.6365125  0.0074923  2620.90   &lt;2e-16 ***\nORIGIN_SZPRSZ05   21.3132151  0.0042119  5060.24   &lt;2e-16 ***\nORIGIN_SZPRSZ06   18.9314574  0.0117278  1614.24   &lt;2e-16 ***\nORIGIN_SZPRSZ07   17.2822918  0.0162430  1063.98   &lt;2e-16 ***\nORIGIN_SZPRSZ08   19.9267642  0.0062298  3198.62   &lt;2e-16 ***\nORIGIN_SZQTSZ01   19.7357175  0.0066359  2974.08   &lt;2e-16 ***\nORIGIN_SZQTSZ02   19.2082141  0.0061402  3128.26   &lt;2e-16 ***\nORIGIN_SZQTSZ03   19.7771883  0.0056220  3517.83   &lt;2e-16 ***\nORIGIN_SZQTSZ04   18.7114421  0.0072842  2568.76   &lt;2e-16 ***\nORIGIN_SZQTSZ05   19.3049324  0.0062401  3093.69   &lt;2e-16 ***\nORIGIN_SZQTSZ06   19.2643228  0.0065590  2937.09   &lt;2e-16 ***\nORIGIN_SZQTSZ07   18.5697347  0.0095373  1947.06   &lt;2e-16 ***\nORIGIN_SZQTSZ08   19.6147001  0.0061330  3198.21   &lt;2e-16 ***\nORIGIN_SZQTSZ09   19.2550793  0.0069947  2752.82   &lt;2e-16 ***\nORIGIN_SZQTSZ10   19.5801866  0.0064513  3035.07   &lt;2e-16 ***\nORIGIN_SZQTSZ11   17.7398366  0.0143648  1234.95   &lt;2e-16 ***\nORIGIN_SZQTSZ12   17.2420354  0.0186736   923.34   &lt;2e-16 ***\nORIGIN_SZQTSZ13   19.3857418  0.0078878  2457.69   &lt;2e-16 ***\nORIGIN_SZQTSZ14   18.1300753  0.0122096  1484.90   &lt;2e-16 ***\nORIGIN_SZQTSZ15   19.4222283  0.0120871  1606.86   &lt;2e-16 ***\nORIGIN_SZRCSZ01   18.1549045  0.0125108  1451.13   &lt;2e-16 ***\nORIGIN_SZRCSZ06   18.8836400  0.0082161  2298.38   &lt;2e-16 ***\nORIGIN_SZRVSZ01   16.7864438  0.0323796   518.43   &lt;2e-16 ***\nORIGIN_SZRVSZ02   16.4203244  0.0276836   593.14   &lt;2e-16 ***\nORIGIN_SZRVSZ03   16.6453738  0.0244992   679.42   &lt;2e-16 ***\nORIGIN_SZRVSZ04   15.9559213  0.0556344   286.80   &lt;2e-16 ***\nORIGIN_SZRVSZ05   17.0476331  0.0164122  1038.71   &lt;2e-16 ***\nORIGIN_SZSBSZ01   20.0417968  0.0062488  3207.29   &lt;2e-16 ***\nORIGIN_SZSBSZ02   19.1869565  0.0081051  2367.26   &lt;2e-16 ***\nORIGIN_SZSBSZ03   20.5769861  0.0045108  4561.70   &lt;2e-16 ***\nORIGIN_SZSBSZ04   20.5154199  0.0050548  4058.57   &lt;2e-16 ***\nORIGIN_SZSBSZ05   19.6250669  0.0065562  2993.35   &lt;2e-16 ***\nORIGIN_SZSBSZ06   18.8419757  0.0171135  1101.00   &lt;2e-16 ***\nORIGIN_SZSBSZ07   19.4897259  0.0124528  1565.09   &lt;2e-16 ***\nORIGIN_SZSBSZ08   18.7027917  0.0140545  1330.73   &lt;2e-16 ***\nORIGIN_SZSBSZ09   18.8893480  0.0088571  2132.67   &lt;2e-16 ***\nORIGIN_SZSESZ02   20.8962192  0.0041665  5015.34   &lt;2e-16 ***\nORIGIN_SZSESZ03   20.9452771  0.0039737  5270.94   &lt;2e-16 ***\nORIGIN_SZSESZ04   20.6576142  0.0046364  4455.55   &lt;2e-16 ***\nORIGIN_SZSESZ05   19.5170732  0.0058912  3312.92   &lt;2e-16 ***\nORIGIN_SZSESZ06   20.7595824  0.0045747  4537.89   &lt;2e-16 ***\nORIGIN_SZSESZ07   17.6888256  0.0195787   903.47   &lt;2e-16 ***\nORIGIN_SZSGSZ01   19.1359250  0.0085781  2230.79   &lt;2e-16 ***\nORIGIN_SZSGSZ02   18.5614369  0.0102037  1819.10   &lt;2e-16 ***\nORIGIN_SZSGSZ03   19.9933176  0.0050434  3964.23   &lt;2e-16 ***\nORIGIN_SZSGSZ04   20.2426871  0.0047211  4287.71   &lt;2e-16 ***\nORIGIN_SZSGSZ05   18.0114965  0.0107743  1671.70   &lt;2e-16 ***\nORIGIN_SZSGSZ06   20.2593194  0.0044538  4548.76   &lt;2e-16 ***\nORIGIN_SZSGSZ07   19.0763664  0.0062968  3029.54   &lt;2e-16 ***\nORIGIN_SZSKSZ01   19.9222451  0.0085136  2340.04   &lt;2e-16 ***\nORIGIN_SZSKSZ02   20.8633383  0.0055248  3776.33   &lt;2e-16 ***\nORIGIN_SZSKSZ03   19.6528148  0.0080534  2440.33   &lt;2e-16 ***\nORIGIN_SZSKSZ04   18.0754470  0.0275771   655.45   &lt;2e-16 ***\nORIGIN_SZSKSZ05   19.1192521  0.0155579  1228.91   &lt;2e-16 ***\nORIGIN_SZSLSZ01   17.1501034  0.0329384   520.67   &lt;2e-16 ***\nORIGIN_SZSLSZ04   19.5949774  0.0076753  2552.98   &lt;2e-16 ***\nORIGIN_SZSRSZ01   16.9761403  0.0162020  1047.78   &lt;2e-16 ***\nORIGIN_SZTHSZ01   17.9695687  0.0488559   367.81   &lt;2e-16 ***\nORIGIN_SZTHSZ03   18.5427522  0.0223617   829.22   &lt;2e-16 ***\nORIGIN_SZTHSZ04   17.4760374  0.0286247   610.52   &lt;2e-16 ***\nORIGIN_SZTHSZ06   17.8401186  0.0183322   973.16   &lt;2e-16 ***\nORIGIN_SZTMSZ01   20.3406361  0.0056607  3593.33   &lt;2e-16 ***\nORIGIN_SZTMSZ02   22.0307026  0.0037386  5892.85   &lt;2e-16 ***\nORIGIN_SZTMSZ03   21.3451920  0.0040606  5256.65   &lt;2e-16 ***\nORIGIN_SZTMSZ04   20.6611593  0.0049896  4140.87   &lt;2e-16 ***\nORIGIN_SZTMSZ05   19.3323133  0.0112868  1712.82   &lt;2e-16 ***\nORIGIN_SZTNSZ01   17.9513571  0.0128266  1399.54   &lt;2e-16 ***\nORIGIN_SZTNSZ02   18.0267387  0.0098372  1832.51   &lt;2e-16 ***\nORIGIN_SZTNSZ03   17.7253700  0.0134668  1316.23   &lt;2e-16 ***\nORIGIN_SZTNSZ04   19.4474075  0.0073760  2636.59   &lt;2e-16 ***\nORIGIN_SZTPSZ01   19.1078631  0.0065635  2911.25   &lt;2e-16 ***\nORIGIN_SZTPSZ02   20.2837634  0.0041411  4898.18   &lt;2e-16 ***\nORIGIN_SZTPSZ03   19.1838238  0.0059552  3221.37   &lt;2e-16 ***\nORIGIN_SZTPSZ04   19.1805388  0.0054778  3501.53   &lt;2e-16 ***\nORIGIN_SZTPSZ05   19.3718076  0.0058610  3305.18   &lt;2e-16 ***\nORIGIN_SZTPSZ06   19.6605723  0.0054968  3576.70   &lt;2e-16 ***\nORIGIN_SZTPSZ07   19.4499807  0.0060491  3215.36   &lt;2e-16 ***\nORIGIN_SZTPSZ08   18.7996538  0.0095757  1963.28   &lt;2e-16 ***\nORIGIN_SZTPSZ09   19.0025110  0.0067068  2833.31   &lt;2e-16 ***\nORIGIN_SZTPSZ10   18.8899657  0.0076094  2482.46   &lt;2e-16 ***\nORIGIN_SZTPSZ11   19.6277780  0.0053983  3635.93   &lt;2e-16 ***\nORIGIN_SZTPSZ12   19.1471104  0.0065742  2912.45   &lt;2e-16 ***\nORIGIN_SZTSSZ01   17.4901113  0.0478954   365.17   &lt;2e-16 ***\nORIGIN_SZTSSZ02   20.4997466  0.0081850  2504.55   &lt;2e-16 ***\nORIGIN_SZTSSZ03   20.1076553  0.0084728  2373.19   &lt;2e-16 ***\nORIGIN_SZTSSZ04   20.0646610  0.0089008  2254.26   &lt;2e-16 ***\nORIGIN_SZTSSZ05   19.3962067  0.0151392  1281.19   &lt;2e-16 ***\nORIGIN_SZTSSZ06   20.9235857  0.0178278  1173.65   &lt;2e-16 ***\nORIGIN_SZWCSZ01   20.8411600  0.0086519  2408.86   &lt;2e-16 ***\nORIGIN_SZWCSZ02   17.7355404  0.0328889   539.26   &lt;2e-16 ***\nORIGIN_SZWCSZ03   14.9380886  0.1240699   120.40   &lt;2e-16 ***\nORIGIN_SZWDSZ01   21.1969012  0.0037830  5603.23   &lt;2e-16 ***\nORIGIN_SZWDSZ02   20.5930001  0.0044572  4620.13   &lt;2e-16 ***\nORIGIN_SZWDSZ03   21.2521867  0.0041672  5099.85   &lt;2e-16 ***\nORIGIN_SZWDSZ04   21.0702687  0.0048648  4331.13   &lt;2e-16 ***\nORIGIN_SZWDSZ05   20.4008998  0.0051801  3938.35   &lt;2e-16 ***\nORIGIN_SZWDSZ06   20.6669176  0.0049280  4193.78   &lt;2e-16 ***\nORIGIN_SZWDSZ07   19.0500370  0.0082729  2302.71   &lt;2e-16 ***\nORIGIN_SZWDSZ08   19.0816252  0.0080667  2365.49   &lt;2e-16 ***\nORIGIN_SZWDSZ09   21.4182096  0.0040391  5302.73   &lt;2e-16 ***\nORIGIN_SZYSSZ01   19.5355157  0.0057540  3395.14   &lt;2e-16 ***\nORIGIN_SZYSSZ02   20.8737972  0.0048278  4323.64   &lt;2e-16 ***\nORIGIN_SZYSSZ03   21.6614437  0.0040011  5413.81   &lt;2e-16 ***\nORIGIN_SZYSSZ04   20.9305289  0.0043595  4801.10   &lt;2e-16 ***\nORIGIN_SZYSSZ05   20.1727678  0.0058466  3450.34   &lt;2e-16 ***\nORIGIN_SZYSSZ06   19.1481507  0.0116724  1640.47   &lt;2e-16 ***\nORIGIN_SZYSSZ07   18.7919074  0.0141636  1326.78   &lt;2e-16 ***\nORIGIN_SZYSSZ08   19.9733515  0.0061229  3262.07   &lt;2e-16 ***\nORIGIN_SZYSSZ09   20.9366181  0.0040347  5189.15   &lt;2e-16 ***\nlog(SCHOOL_COUNT)  0.4755516  0.0004701  1011.55   &lt;2e-16 ***\nlog(RETAIL_COUNT)  0.1796905  0.0001856   968.12   &lt;2e-16 ***\nlog(DIST)         -1.6929522  0.0004093 -4136.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 189463537  on 14471  degrees of freedom\nResidual deviance:  15526121  on 14189  degrees of freedom\nAIC: 15615824\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\n\nTip\n\n\n\nRead ‘Estimate’ of log(factor) to check the relationship between those attractiveness and propulsive factors and number of trips. Also check the p-value to ensure the factors’ significance (&lt;0.05).\n\n\n\n\n5.2 Goodness of Fit\nCheck the R-Squared of our poisson analysis.\n\n\nCode\ncor(orcSIM_Poisson$data$TRIPS, orcSIM_Poisson$fitted.values) ^2\n\n\n[1] 0.4362208\n\n\nOr you can create a function to find R-squared for further use.\n\n\nCode\nRSquared &lt;- function(observed, estimated){\n  r &lt;- cor(observed, estimated)\n  R2 &lt;- r^2\n  R2\n}\n\n\n\n\n5.3 Doubly constrained\n\n\nCode\ndbcSIM_Poisson &lt;- glm(formula = TRIPS ~\n                        ORIGIN_SZ +\n                        DESTIN_SZ +\n                        log(DIST),\n                      family = poisson(link = 'log'),\n                      data = inter_zonal_flow,\n                      na.action = na.exclude)  # this line is just in case\nsummary(dbcSIM_Poisson)\n\n\n\nCall:\nglm(formula = TRIPS ~ ORIGIN_SZ + DESTIN_SZ + log(DIST), family = poisson(link = \"log\"), \n    data = inter_zonal_flow, na.action = na.exclude)\n\nCoefficients:\n                  Estimate Std. Error   z value Pr(&gt;|z|)    \n(Intercept)     21.8312374  0.0059160  3690.190  &lt; 2e-16 ***\nORIGIN_SZAMSZ02  0.5263502  0.0048031   109.585  &lt; 2e-16 ***\nORIGIN_SZAMSZ03  0.3139982  0.0049254    63.751  &lt; 2e-16 ***\nORIGIN_SZAMSZ04 -0.2146257  0.0053639   -40.013  &lt; 2e-16 ***\nORIGIN_SZAMSZ05 -0.1890446  0.0060386   -31.306  &lt; 2e-16 ***\nORIGIN_SZAMSZ06  0.1539201  0.0054401    28.294  &lt; 2e-16 ***\nORIGIN_SZAMSZ07 -0.9826565  0.0098676   -99.584  &lt; 2e-16 ***\nORIGIN_SZAMSZ08 -0.4488417  0.0093070   -48.226  &lt; 2e-16 ***\nORIGIN_SZAMSZ09  0.0713474  0.0057402    12.429  &lt; 2e-16 ***\nORIGIN_SZAMSZ10  0.4313742  0.0050370    85.641  &lt; 2e-16 ***\nORIGIN_SZAMSZ11 -1.4712226  0.0131178  -112.154  &lt; 2e-16 ***\nORIGIN_SZAMSZ12 -1.7250733  0.0111603  -154.573  &lt; 2e-16 ***\nORIGIN_SZBDSZ01  0.8810576  0.0048168   182.914  &lt; 2e-16 ***\nORIGIN_SZBDSZ02  0.1100240  0.0055529    19.814  &lt; 2e-16 ***\nORIGIN_SZBDSZ03  0.3606166  0.0050672    71.167  &lt; 2e-16 ***\nORIGIN_SZBDSZ04  1.4624347  0.0044212   330.781  &lt; 2e-16 ***\nORIGIN_SZBDSZ05  0.6207557  0.0050843   122.092  &lt; 2e-16 ***\nORIGIN_SZBDSZ06  0.6712973  0.0051953   129.214  &lt; 2e-16 ***\nORIGIN_SZBDSZ07 -1.2338669  0.0100775  -122.437  &lt; 2e-16 ***\nORIGIN_SZBDSZ08 -1.0444562  0.0094555  -110.460  &lt; 2e-16 ***\nORIGIN_SZBKSZ01 -0.2838426  0.0071354   -39.780  &lt; 2e-16 ***\nORIGIN_SZBKSZ02  0.5550522  0.0059014    94.054  &lt; 2e-16 ***\nORIGIN_SZBKSZ03  0.7396640  0.0056796   130.231  &lt; 2e-16 ***\nORIGIN_SZBKSZ04 -0.2242451  0.0067482   -33.230  &lt; 2e-16 ***\nORIGIN_SZBKSZ05 -0.2371614  0.0069386   -34.180  &lt; 2e-16 ***\nORIGIN_SZBKSZ06 -0.1413812  0.0065035   -21.739  &lt; 2e-16 ***\nORIGIN_SZBKSZ07  0.7089989  0.0051843   136.758  &lt; 2e-16 ***\nORIGIN_SZBKSZ08 -0.0907065  0.0059157   -15.333  &lt; 2e-16 ***\nORIGIN_SZBKSZ09 -0.1775146  0.0063302   -28.042  &lt; 2e-16 ***\nORIGIN_SZBLSZ01 -2.3684539  0.0154280  -153.516  &lt; 2e-16 ***\nORIGIN_SZBLSZ02 -2.8078475  0.0197839  -141.926  &lt; 2e-16 ***\nORIGIN_SZBLSZ03 -3.3122763  0.0466091   -71.065  &lt; 2e-16 ***\nORIGIN_SZBLSZ04 -2.6770542  0.0241793  -110.717  &lt; 2e-16 ***\nORIGIN_SZBMSZ01  0.0618035  0.0059400    10.405  &lt; 2e-16 ***\nORIGIN_SZBMSZ02 -1.3535767  0.0073741  -183.557  &lt; 2e-16 ***\nORIGIN_SZBMSZ03 -0.7569095  0.0063187  -119.790  &lt; 2e-16 ***\nORIGIN_SZBMSZ04 -0.2949304  0.0059603   -49.483  &lt; 2e-16 ***\nORIGIN_SZBMSZ05 -2.6131992  0.0172376  -151.599  &lt; 2e-16 ***\nORIGIN_SZBMSZ06 -3.0315024  0.0185502  -163.422  &lt; 2e-16 ***\nORIGIN_SZBMSZ07 -0.6962524  0.0064068  -108.674  &lt; 2e-16 ***\nORIGIN_SZBMSZ08 -0.9310730  0.0064541  -144.261  &lt; 2e-16 ***\nORIGIN_SZBMSZ09 -1.2911253  0.0092047  -140.268  &lt; 2e-16 ***\nORIGIN_SZBMSZ10 -1.6687004  0.0095708  -174.353  &lt; 2e-16 ***\nORIGIN_SZBMSZ11 -1.1152794  0.0072027  -154.841  &lt; 2e-16 ***\nORIGIN_SZBMSZ12 -1.5323954  0.0099932  -153.344  &lt; 2e-16 ***\nORIGIN_SZBMSZ13 -0.6267376  0.0065863   -95.158  &lt; 2e-16 ***\nORIGIN_SZBMSZ14 -1.0475467  0.0072472  -144.544  &lt; 2e-16 ***\nORIGIN_SZBMSZ15 -0.5049444  0.0067390   -74.929  &lt; 2e-16 ***\nORIGIN_SZBMSZ16 -1.5282897  0.0099545  -153.527  &lt; 2e-16 ***\nORIGIN_SZBMSZ17 -1.5722349  0.0161533   -97.332  &lt; 2e-16 ***\nORIGIN_SZBPSZ01  0.5814175  0.0062904    92.429  &lt; 2e-16 ***\nORIGIN_SZBPSZ02  0.0875442  0.0072190    12.127  &lt; 2e-16 ***\nORIGIN_SZBPSZ03  0.3358227  0.0070460    47.662  &lt; 2e-16 ***\nORIGIN_SZBPSZ04  0.6507586  0.0057726   112.733  &lt; 2e-16 ***\nORIGIN_SZBPSZ05  0.9502124  0.0052971   179.384  &lt; 2e-16 ***\nORIGIN_SZBPSZ06 -1.0480314  0.0098191  -106.734  &lt; 2e-16 ***\nORIGIN_SZBPSZ07 -0.5467931  0.0091676   -59.644  &lt; 2e-16 ***\nORIGIN_SZBSSZ01  0.2998334  0.0059193    50.654  &lt; 2e-16 ***\nORIGIN_SZBSSZ02  0.2841036  0.0050863    55.856  &lt; 2e-16 ***\nORIGIN_SZBSSZ03 -0.2331505  0.0056565   -41.218  &lt; 2e-16 ***\nORIGIN_SZBTSZ01  0.0987284  0.0063715    15.495  &lt; 2e-16 ***\nORIGIN_SZBTSZ02 -0.6261229  0.0084604   -74.006  &lt; 2e-16 ***\nORIGIN_SZBTSZ03 -0.4326963  0.0073452   -58.909  &lt; 2e-16 ***\nORIGIN_SZBTSZ04 -1.4998668  0.0110013  -136.336  &lt; 2e-16 ***\nORIGIN_SZBTSZ05 -0.9564768  0.0122202   -78.270  &lt; 2e-16 ***\nORIGIN_SZBTSZ06 -1.2853131  0.0099328  -129.401  &lt; 2e-16 ***\nORIGIN_SZBTSZ07 -2.3870991  0.0144589  -165.096  &lt; 2e-16 ***\nORIGIN_SZBTSZ08 -1.3715855  0.0113825  -120.499  &lt; 2e-16 ***\nORIGIN_SZCBSZ01 -3.5940232  0.0548979   -65.467  &lt; 2e-16 ***\nORIGIN_SZCCSZ01 -0.7008220  0.0140373   -49.926  &lt; 2e-16 ***\nORIGIN_SZCHSZ01 -0.9109524  0.0122869   -74.140  &lt; 2e-16 ***\nORIGIN_SZCHSZ02 -0.8566547  0.0088749   -96.526  &lt; 2e-16 ***\nORIGIN_SZCHSZ03  1.1153731  0.0066136   168.650  &lt; 2e-16 ***\nORIGIN_SZCKSZ01  0.3001815  0.0058548    51.271  &lt; 2e-16 ***\nORIGIN_SZCKSZ02  0.7185711  0.0060595   118.585  &lt; 2e-16 ***\nORIGIN_SZCKSZ03  1.1389824  0.0053179   214.178  &lt; 2e-16 ***\nORIGIN_SZCKSZ04  1.6281772  0.0054761   297.324  &lt; 2e-16 ***\nORIGIN_SZCKSZ05  0.8338470  0.0064178   129.927  &lt; 2e-16 ***\nORIGIN_SZCKSZ06  0.6528993  0.0082375    79.259  &lt; 2e-16 ***\nORIGIN_SZCLSZ01 -0.7174758  0.0082123   -87.366  &lt; 2e-16 ***\nORIGIN_SZCLSZ02 -1.7513100  0.0139062  -125.938  &lt; 2e-16 ***\nORIGIN_SZCLSZ03 -1.0362873  0.0085485  -121.224  &lt; 2e-16 ***\nORIGIN_SZCLSZ04  0.6160017  0.0051276   120.136  &lt; 2e-16 ***\nORIGIN_SZCLSZ05 -2.1005122  0.0150228  -139.821  &lt; 2e-16 ***\nORIGIN_SZCLSZ06  0.7252108  0.0049447   146.665  &lt; 2e-16 ***\nORIGIN_SZCLSZ07 -0.5343482  0.0062500   -85.496  &lt; 2e-16 ***\nORIGIN_SZCLSZ08 -0.2153408  0.0067571   -31.869  &lt; 2e-16 ***\nORIGIN_SZCLSZ09 -1.8019961  0.0169078  -106.578  &lt; 2e-16 ***\nORIGIN_SZDTSZ02 -3.9057711  0.0834668   -46.794  &lt; 2e-16 ***\nORIGIN_SZDTSZ03 -3.4152419  0.0738650   -46.236  &lt; 2e-16 ***\nORIGIN_SZDTSZ13 -3.0183438  0.0315257   -95.742  &lt; 2e-16 ***\nORIGIN_SZGLSZ01 -1.7812384  0.0099367  -179.258  &lt; 2e-16 ***\nORIGIN_SZGLSZ02 -0.1074991  0.0054325   -19.788  &lt; 2e-16 ***\nORIGIN_SZGLSZ03 -0.2461106  0.0057176   -43.045  &lt; 2e-16 ***\nORIGIN_SZGLSZ04  0.8657186  0.0046413   186.524  &lt; 2e-16 ***\nORIGIN_SZGLSZ05  0.5871393  0.0047939   122.477  &lt; 2e-16 ***\nORIGIN_SZHGSZ01  0.3543819  0.0050461    70.229  &lt; 2e-16 ***\nORIGIN_SZHGSZ02  0.4218178  0.0050820    83.003  &lt; 2e-16 ***\nORIGIN_SZHGSZ03  0.2411309  0.0054241    44.456  &lt; 2e-16 ***\nORIGIN_SZHGSZ04  0.8180622  0.0046153   177.252  &lt; 2e-16 ***\nORIGIN_SZHGSZ05  1.2173687  0.0045655   266.647  &lt; 2e-16 ***\nORIGIN_SZHGSZ06 -0.1826300  0.0058214   -31.372  &lt; 2e-16 ***\nORIGIN_SZHGSZ07  0.3172839  0.0050733    62.540  &lt; 2e-16 ***\nORIGIN_SZHGSZ08 -0.1151369  0.0057067   -20.176  &lt; 2e-16 ***\nORIGIN_SZHGSZ09 -1.2873441  0.0091690  -140.401  &lt; 2e-16 ***\nORIGIN_SZHGSZ10 -3.3783178  0.0424682   -79.549  &lt; 2e-16 ***\nORIGIN_SZJESZ01  0.4859234  0.0055927    86.885  &lt; 2e-16 ***\nORIGIN_SZJESZ02  0.1766088  0.0055800    31.650  &lt; 2e-16 ***\nORIGIN_SZJESZ03 -0.2177441  0.0059535   -36.574  &lt; 2e-16 ***\nORIGIN_SZJESZ04 -1.5532182  0.0104526  -148.597  &lt; 2e-16 ***\nORIGIN_SZJESZ05 -2.3332926  0.0142701  -163.509  &lt; 2e-16 ***\nORIGIN_SZJESZ06  0.3007382  0.0055019    54.661  &lt; 2e-16 ***\nORIGIN_SZJESZ07 -1.9687994  0.0121092  -162.587  &lt; 2e-16 ***\nORIGIN_SZJESZ08 -1.3032070  0.0122024  -106.800  &lt; 2e-16 ***\nORIGIN_SZJESZ09  0.5762635  0.0058766    98.061  &lt; 2e-16 ***\nORIGIN_SZJESZ10 -1.4423113  0.0194773   -74.051  &lt; 2e-16 ***\nORIGIN_SZJESZ11 -1.9720897  0.0200811   -98.206  &lt; 2e-16 ***\nORIGIN_SZJWSZ01  0.3808627  0.0071357    53.374  &lt; 2e-16 ***\nORIGIN_SZJWSZ02  0.7963999  0.0053150   149.840  &lt; 2e-16 ***\nORIGIN_SZJWSZ03  1.5429636  0.0049961   308.834  &lt; 2e-16 ***\nORIGIN_SZJWSZ04  0.6410760  0.0056711   113.042  &lt; 2e-16 ***\nORIGIN_SZJWSZ05 -2.1571049  0.0133584  -161.479  &lt; 2e-16 ***\nORIGIN_SZJWSZ06 -1.5174532  0.0113384  -133.833  &lt; 2e-16 ***\nORIGIN_SZJWSZ07 -2.7089963  0.0280439   -96.598  &lt; 2e-16 ***\nORIGIN_SZJWSZ08  1.5343415  0.0051711   296.713  &lt; 2e-16 ***\nORIGIN_SZJWSZ09  1.8837410  0.0048845   385.656  &lt; 2e-16 ***\nORIGIN_SZKLSZ01  0.1081286  0.0053307    20.284  &lt; 2e-16 ***\nORIGIN_SZKLSZ02 -0.8844695  0.0067728  -130.591  &lt; 2e-16 ***\nORIGIN_SZKLSZ03 -0.6872640  0.0062857  -109.337  &lt; 2e-16 ***\nORIGIN_SZKLSZ04 -2.2090319  0.0122440  -180.418  &lt; 2e-16 ***\nORIGIN_SZKLSZ05 -1.1728726  0.0110765  -105.888  &lt; 2e-16 ***\nORIGIN_SZKLSZ06 -6.1162315  0.1857789   -32.922  &lt; 2e-16 ***\nORIGIN_SZKLSZ07 -1.4082749  0.0092299  -152.578  &lt; 2e-16 ***\nORIGIN_SZKLSZ08 -1.7781551  0.0104682  -169.862  &lt; 2e-16 ***\nORIGIN_SZLKSZ01 -2.0531568  0.0398803   -51.483  &lt; 2e-16 ***\nORIGIN_SZMDSZ01 -0.8825639  0.0287621   -30.685  &lt; 2e-16 ***\nORIGIN_SZMDSZ02 -0.6219993  0.0107388   -57.921  &lt; 2e-16 ***\nORIGIN_SZMDSZ03 -2.0840156  0.0173117  -120.382  &lt; 2e-16 ***\nORIGIN_SZMPSZ01 -0.9659093  0.0086972  -111.060  &lt; 2e-16 ***\nORIGIN_SZMPSZ02 -1.0411153  0.0073403  -141.836  &lt; 2e-16 ***\nORIGIN_SZMPSZ03  0.0001659  0.0059401     0.028 0.977719    \nORIGIN_SZMUSZ02 -3.7599031  0.1037937   -36.225  &lt; 2e-16 ***\nORIGIN_SZNTSZ01 -3.0388366  0.0355325   -85.523  &lt; 2e-16 ***\nORIGIN_SZNTSZ02 -3.4230640  0.0235902  -145.106  &lt; 2e-16 ***\nORIGIN_SZNTSZ03 -0.9094796  0.0082551  -110.172  &lt; 2e-16 ***\nORIGIN_SZNTSZ05 -4.0861681  0.0499630   -81.784  &lt; 2e-16 ***\nORIGIN_SZNTSZ06 -3.9497128  0.0565388   -69.858  &lt; 2e-16 ***\nORIGIN_SZNVSZ01  0.3235636  0.0049439    65.447  &lt; 2e-16 ***\nORIGIN_SZNVSZ02 -0.6946748  0.0070536   -98.485  &lt; 2e-16 ***\nORIGIN_SZNVSZ03 -1.0540196  0.0083781  -125.806  &lt; 2e-16 ***\nORIGIN_SZNVSZ04 -0.9897977  0.0093463  -105.903  &lt; 2e-16 ***\nORIGIN_SZNVSZ05 -2.2578432  0.0169180  -133.458  &lt; 2e-16 ***\nORIGIN_SZPGSZ01  0.2399827  0.0130436    18.398  &lt; 2e-16 ***\nORIGIN_SZPGSZ02 -0.3352342  0.0078451   -42.732  &lt; 2e-16 ***\nORIGIN_SZPGSZ03  0.9515148  0.0051376   185.207  &lt; 2e-16 ***\nORIGIN_SZPGSZ04  1.3998952  0.0047991   291.697  &lt; 2e-16 ***\nORIGIN_SZPGSZ05  0.4451629  0.0063423    70.189  &lt; 2e-16 ***\nORIGIN_SZPLSZ01 -0.9705918  0.0122781   -79.050  &lt; 2e-16 ***\nORIGIN_SZPLSZ02 -1.0670151  0.0153358   -69.577  &lt; 2e-16 ***\nORIGIN_SZPLSZ03 -2.1229124  0.0373527   -56.834  &lt; 2e-16 ***\nORIGIN_SZPLSZ04 -3.0911932  0.0371296   -83.254  &lt; 2e-16 ***\nORIGIN_SZPLSZ05 -2.1705708  0.0226085   -96.007  &lt; 2e-16 ***\nORIGIN_SZPNSZ01  0.9052637  0.0065952   137.262  &lt; 2e-16 ***\nORIGIN_SZPNSZ02 -0.1720425  0.0125658   -13.691  &lt; 2e-16 ***\nORIGIN_SZPNSZ03 -2.3973459  0.0201408  -119.029  &lt; 2e-16 ***\nORIGIN_SZPNSZ04 -3.4483689  0.0343741  -100.319  &lt; 2e-16 ***\nORIGIN_SZPNSZ05 -2.0588530  0.0282328   -72.924  &lt; 2e-16 ***\nORIGIN_SZPRSZ01 -0.6399015  0.0120470   -53.117  &lt; 2e-16 ***\nORIGIN_SZPRSZ02  0.8122270  0.0050886   159.617  &lt; 2e-16 ***\nORIGIN_SZPRSZ03  0.3990960  0.0051810    77.031  &lt; 2e-16 ***\nORIGIN_SZPRSZ04 -0.8485348  0.0079236  -107.089  &lt; 2e-16 ***\nORIGIN_SZPRSZ05  0.8008791  0.0048532   165.021  &lt; 2e-16 ***\nORIGIN_SZPRSZ06 -1.4498806  0.0121422  -119.408  &lt; 2e-16 ***\nORIGIN_SZPRSZ07 -3.2025045  0.0167118  -191.631  &lt; 2e-16 ***\nORIGIN_SZPRSZ08 -0.5862269  0.0067255   -87.165  &lt; 2e-16 ***\nORIGIN_SZQTSZ01 -0.1859270  0.0075531   -24.616  &lt; 2e-16 ***\nORIGIN_SZQTSZ02 -0.8715122  0.0068124  -127.929  &lt; 2e-16 ***\nORIGIN_SZQTSZ03 -0.1259816  0.0064796   -19.443  &lt; 2e-16 ***\nORIGIN_SZQTSZ04 -1.4620032  0.0079848  -183.098  &lt; 2e-16 ***\nORIGIN_SZQTSZ05 -0.6675643  0.0069616   -95.892  &lt; 2e-16 ***\nORIGIN_SZQTSZ06 -0.8190026  0.0072713  -112.634  &lt; 2e-16 ***\nORIGIN_SZQTSZ07 -1.5189403  0.0099864  -152.101  &lt; 2e-16 ***\nORIGIN_SZQTSZ08 -0.4976238  0.0067874   -73.316  &lt; 2e-16 ***\nORIGIN_SZQTSZ09 -0.9006162  0.0075978  -118.536  &lt; 2e-16 ***\nORIGIN_SZQTSZ10 -0.6690184  0.0071574   -93.473  &lt; 2e-16 ***\nORIGIN_SZQTSZ11 -2.5203437  0.0147000  -171.452  &lt; 2e-16 ***\nORIGIN_SZQTSZ12 -3.0461675  0.0190193  -160.162  &lt; 2e-16 ***\nORIGIN_SZQTSZ13 -0.7501068  0.0084481   -88.791  &lt; 2e-16 ***\nORIGIN_SZQTSZ14 -1.9321849  0.0126114  -153.209  &lt; 2e-16 ***\nORIGIN_SZQTSZ15 -0.9576828  0.0127157   -75.315  &lt; 2e-16 ***\nORIGIN_SZRCSZ01 -1.8167951  0.0129234  -140.582  &lt; 2e-16 ***\nORIGIN_SZRCSZ06 -0.5560563  0.0090507   -61.438  &lt; 2e-16 ***\nORIGIN_SZRVSZ01 -2.8862570  0.0325532   -88.663  &lt; 2e-16 ***\nORIGIN_SZRVSZ02 -3.1555662  0.0281279  -112.186  &lt; 2e-16 ***\nORIGIN_SZRVSZ03 -2.9836089  0.0248449  -120.089  &lt; 2e-16 ***\nORIGIN_SZRVSZ04 -3.5520422  0.0561371   -63.274  &lt; 2e-16 ***\nORIGIN_SZRVSZ05 -2.5866584  0.0180382  -143.399  &lt; 2e-16 ***\nORIGIN_SZSBSZ01  0.2867444  0.0071098    40.331  &lt; 2e-16 ***\nORIGIN_SZSBSZ02 -0.9012334  0.0087262  -103.278  &lt; 2e-16 ***\nORIGIN_SZSBSZ03  0.8311038  0.0055422   149.958  &lt; 2e-16 ***\nORIGIN_SZSBSZ04  0.4044170  0.0062047    65.179  &lt; 2e-16 ***\nORIGIN_SZSBSZ05 -0.2661845  0.0074162   -35.892  &lt; 2e-16 ***\nORIGIN_SZSBSZ06 -0.9023075  0.0175046   -51.547  &lt; 2e-16 ***\nORIGIN_SZSBSZ07  0.0505870  0.0131317     3.852 0.000117 ***\nORIGIN_SZSBSZ08 -1.1158011  0.0145416   -76.732  &lt; 2e-16 ***\nORIGIN_SZSBSZ09 -0.9682835  0.0095396  -101.501  &lt; 2e-16 ***\nORIGIN_SZSESZ02  1.1452735  0.0047810   239.548  &lt; 2e-16 ***\nORIGIN_SZSESZ03  1.2815277  0.0045677   280.564  &lt; 2e-16 ***\nORIGIN_SZSESZ04  0.8085857  0.0052756   153.269  &lt; 2e-16 ***\nORIGIN_SZSESZ05 -0.2329413  0.0063113   -36.909  &lt; 2e-16 ***\nORIGIN_SZSESZ06  1.0576879  0.0049909   211.925  &lt; 2e-16 ***\nORIGIN_SZSESZ07 -2.3165908  0.0196831  -117.695  &lt; 2e-16 ***\nORIGIN_SZSGSZ01 -0.6606350  0.0088079   -75.005  &lt; 2e-16 ***\nORIGIN_SZSGSZ02 -1.3638984  0.0104040  -131.094  &lt; 2e-16 ***\nORIGIN_SZSGSZ03  0.1152591  0.0054649    21.091  &lt; 2e-16 ***\nORIGIN_SZSGSZ04  0.2954067  0.0050865    58.077  &lt; 2e-16 ***\nORIGIN_SZSGSZ05 -2.0792678  0.0109882  -189.227  &lt; 2e-16 ***\nORIGIN_SZSGSZ06  0.4563227  0.0048880    93.356  &lt; 2e-16 ***\nORIGIN_SZSGSZ07 -0.8955254  0.0067100  -133.461  &lt; 2e-16 ***\nORIGIN_SZSKSZ01 -0.3184402  0.0093032   -34.229  &lt; 2e-16 ***\nORIGIN_SZSKSZ02  1.1160484  0.0063851   174.790  &lt; 2e-16 ***\nORIGIN_SZSKSZ03 -0.2566692  0.0086021   -29.838  &lt; 2e-16 ***\nORIGIN_SZSKSZ04 -1.5781827  0.0279394   -56.486  &lt; 2e-16 ***\nORIGIN_SZSKSZ05 -0.2724361  0.0163597   -16.653  &lt; 2e-16 ***\nORIGIN_SZSLSZ01 -2.4458625  0.0330301   -74.050  &lt; 2e-16 ***\nORIGIN_SZSLSZ04 -0.0987076  0.0079626   -12.396  &lt; 2e-16 ***\nORIGIN_SZSRSZ01 -2.2584977  0.0176647  -127.854  &lt; 2e-16 ***\nORIGIN_SZTHSZ01 -2.5878524  0.0489998   -52.814  &lt; 2e-16 ***\nORIGIN_SZTHSZ03 -0.8101746  0.0226814   -35.720  &lt; 2e-16 ***\nORIGIN_SZTHSZ04 -2.4186655  0.0288663   -83.789  &lt; 2e-16 ***\nORIGIN_SZTHSZ06 -1.7080541  0.0186353   -91.657  &lt; 2e-16 ***\nORIGIN_SZTMSZ01 -0.2193476  0.0061823   -35.480  &lt; 2e-16 ***\nORIGIN_SZTMSZ02  1.7772464  0.0043394   409.558  &lt; 2e-16 ***\nORIGIN_SZTMSZ03  1.0051343  0.0046055   218.249  &lt; 2e-16 ***\nORIGIN_SZTMSZ04  0.1642370  0.0055078    29.819  &lt; 2e-16 ***\nORIGIN_SZTMSZ05 -1.2878706  0.0114828  -112.157  &lt; 2e-16 ***\nORIGIN_SZTNSZ01 -1.7163504  0.0131268  -130.751  &lt; 2e-16 ***\nORIGIN_SZTNSZ02 -1.6508988  0.0103851  -158.968  &lt; 2e-16 ***\nORIGIN_SZTNSZ03 -2.1545577  0.0137947  -156.187  &lt; 2e-16 ***\nORIGIN_SZTNSZ04 -0.3949120  0.0078496   -50.310  &lt; 2e-16 ***\nORIGIN_SZTPSZ01 -0.8058100  0.0069916  -115.253  &lt; 2e-16 ***\nORIGIN_SZTPSZ02  0.5369060  0.0047272   113.577  &lt; 2e-16 ***\nORIGIN_SZTPSZ03 -0.7779333  0.0064278  -121.027  &lt; 2e-16 ***\nORIGIN_SZTPSZ04 -0.8153581  0.0061387  -132.823  &lt; 2e-16 ***\nORIGIN_SZTPSZ05 -0.5073676  0.0067771   -74.865  &lt; 2e-16 ***\nORIGIN_SZTPSZ06  0.0847301  0.0065717    12.893  &lt; 2e-16 ***\nORIGIN_SZTPSZ07 -0.5839519  0.0066148   -88.280  &lt; 2e-16 ***\nORIGIN_SZTPSZ08 -1.0577941  0.0098480  -107.412  &lt; 2e-16 ***\nORIGIN_SZTPSZ09 -0.9067707  0.0071367  -127.057  &lt; 2e-16 ***\nORIGIN_SZTPSZ10 -1.1362091  0.0080905  -140.438  &lt; 2e-16 ***\nORIGIN_SZTPSZ11 -0.2374621  0.0059472   -39.928  &lt; 2e-16 ***\nORIGIN_SZTPSZ12 -0.8028874  0.0069663  -115.253  &lt; 2e-16 ***\nORIGIN_SZTSSZ01 -2.7809271  0.0482843   -57.595  &lt; 2e-16 ***\nORIGIN_SZTSSZ02  0.0425804  0.0105088     4.052 5.08e-05 ***\nORIGIN_SZTSSZ03  0.1142369  0.0109412    10.441  &lt; 2e-16 ***\nORIGIN_SZTSSZ04 -0.6186261  0.0116324   -53.181  &lt; 2e-16 ***\nORIGIN_SZTSSZ05 -1.0846732  0.0173555   -62.497  &lt; 2e-16 ***\nORIGIN_SZTSSZ06  0.3980173  0.0198100    20.092  &lt; 2e-16 ***\nORIGIN_SZWCSZ01  1.3545143  0.0092002   147.227  &lt; 2e-16 ***\nORIGIN_SZWCSZ02 -2.9863278  0.0330906   -90.247  &lt; 2e-16 ***\nORIGIN_SZWCSZ03 -5.0504916  0.1241385   -40.684  &lt; 2e-16 ***\nORIGIN_SZWDSZ01  1.5238429  0.0049404   308.448  &lt; 2e-16 ***\nORIGIN_SZWDSZ02  0.2832576  0.0056218    50.386  &lt; 2e-16 ***\nORIGIN_SZWDSZ03  1.3702524  0.0053266   257.245  &lt; 2e-16 ***\nORIGIN_SZWDSZ04  1.0248225  0.0059272   172.903  &lt; 2e-16 ***\nORIGIN_SZWDSZ05  0.2356778  0.0060587    38.899  &lt; 2e-16 ***\nORIGIN_SZWDSZ06  0.3146925  0.0059919    52.520  &lt; 2e-16 ***\nORIGIN_SZWDSZ07 -1.4971897  0.0091243  -164.088  &lt; 2e-16 ***\nORIGIN_SZWDSZ08 -0.8894079  0.0087414  -101.747  &lt; 2e-16 ***\nORIGIN_SZWDSZ09  1.4437633  0.0053160   271.590  &lt; 2e-16 ***\nORIGIN_SZYSSZ01 -0.2519398  0.0064443   -39.095  &lt; 2e-16 ***\nORIGIN_SZYSSZ02  0.8726785  0.0057658   151.354  &lt; 2e-16 ***\nORIGIN_SZYSSZ03  1.7868139  0.0050674   352.611  &lt; 2e-16 ***\nORIGIN_SZYSSZ04  0.8418040  0.0051738   162.704  &lt; 2e-16 ***\nORIGIN_SZYSSZ05  0.4292096  0.0062520    68.652  &lt; 2e-16 ***\nORIGIN_SZYSSZ06 -0.7459961  0.0119123   -62.624  &lt; 2e-16 ***\nORIGIN_SZYSSZ07 -0.8422281  0.0144559   -58.262  &lt; 2e-16 ***\nORIGIN_SZYSSZ08  0.1829428  0.0067885    26.949  &lt; 2e-16 ***\nORIGIN_SZYSSZ09  1.1159712  0.0050760   219.853  &lt; 2e-16 ***\nDESTIN_SZAMSZ02  0.0694567  0.0045966    15.111  &lt; 2e-16 ***\nDESTIN_SZAMSZ03  0.0760100  0.0044639    17.028  &lt; 2e-16 ***\nDESTIN_SZAMSZ04 -1.1306391  0.0064373  -175.639  &lt; 2e-16 ***\nDESTIN_SZAMSZ05 -1.0751133  0.0065164  -164.985  &lt; 2e-16 ***\nDESTIN_SZAMSZ06 -0.9624298  0.0065937  -145.962  &lt; 2e-16 ***\nDESTIN_SZAMSZ07 -1.5060319  0.0097616  -154.281  &lt; 2e-16 ***\nDESTIN_SZAMSZ08 -0.4813202  0.0069794   -68.963  &lt; 2e-16 ***\nDESTIN_SZAMSZ09 -1.0220675  0.0066313  -154.129  &lt; 2e-16 ***\nDESTIN_SZAMSZ10  0.1235142  0.0047044    26.255  &lt; 2e-16 ***\nDESTIN_SZAMSZ11 -0.8917993  0.0088519  -100.746  &lt; 2e-16 ***\nDESTIN_SZAMSZ12  0.0195208  0.0051704     3.775 0.000160 ***\nDESTIN_SZBDSZ01  0.9736349  0.0042757   227.713  &lt; 2e-16 ***\nDESTIN_SZBDSZ02 -0.1969470  0.0055284   -35.625  &lt; 2e-16 ***\nDESTIN_SZBDSZ03  0.1266471  0.0050786    24.938  &lt; 2e-16 ***\nDESTIN_SZBDSZ04  1.1608485  0.0041956   276.684  &lt; 2e-16 ***\nDESTIN_SZBDSZ05  0.9293840  0.0044412   209.265  &lt; 2e-16 ***\nDESTIN_SZBDSZ06  0.4090567  0.0050300    81.323  &lt; 2e-16 ***\nDESTIN_SZBDSZ07 -0.8171478  0.0098945   -82.586  &lt; 2e-16 ***\nDESTIN_SZBDSZ08 -1.5895287  0.0111632  -142.391  &lt; 2e-16 ***\nDESTIN_SZBKSZ01 -1.3793311  0.0072145  -191.189  &lt; 2e-16 ***\nDESTIN_SZBKSZ02 -0.5253670  0.0061879   -84.903  &lt; 2e-16 ***\nDESTIN_SZBKSZ03 -1.0095362  0.0065426  -154.301  &lt; 2e-16 ***\nDESTIN_SZBKSZ04 -0.5662858  0.0056453  -100.311  &lt; 2e-16 ***\nDESTIN_SZBKSZ05 -0.9406607  0.0070597  -133.244  &lt; 2e-16 ***\nDESTIN_SZBKSZ06 -1.3129276  0.0067414  -194.755  &lt; 2e-16 ***\nDESTIN_SZBKSZ07  0.0120605  0.0049284     2.447 0.014400 *  \nDESTIN_SZBKSZ08 -1.3658471  0.0075109  -181.849  &lt; 2e-16 ***\nDESTIN_SZBKSZ09 -0.1771310  0.0055645   -31.832  &lt; 2e-16 ***\nDESTIN_SZBLSZ01 -0.8175223  0.0075645  -108.074  &lt; 2e-16 ***\nDESTIN_SZBLSZ02  0.1631280  0.0071753    22.735  &lt; 2e-16 ***\nDESTIN_SZBLSZ03  1.2598494  0.0081706   154.194  &lt; 2e-16 ***\nDESTIN_SZBLSZ04 -0.5642975  0.0137827   -40.943  &lt; 2e-16 ***\nDESTIN_SZBMSZ01  0.6921844  0.0054211   127.684  &lt; 2e-16 ***\nDESTIN_SZBMSZ02 -0.1209392  0.0055362   -21.845  &lt; 2e-16 ***\nDESTIN_SZBMSZ03 -0.2373881  0.0062427   -38.027  &lt; 2e-16 ***\nDESTIN_SZBMSZ04 -0.0407117  0.0058001    -7.019 2.23e-12 ***\nDESTIN_SZBMSZ05 -0.2363309  0.0075967   -31.110  &lt; 2e-16 ***\nDESTIN_SZBMSZ06 -1.1930710  0.0134761   -88.532  &lt; 2e-16 ***\nDESTIN_SZBMSZ07  0.4625103  0.0051864    89.178  &lt; 2e-16 ***\nDESTIN_SZBMSZ08 -0.8604731  0.0069899  -123.102  &lt; 2e-16 ***\nDESTIN_SZBMSZ09 -2.1290239  0.0154841  -137.498  &lt; 2e-16 ***\nDESTIN_SZBMSZ10 -1.4617153  0.0094014  -155.478  &lt; 2e-16 ***\nDESTIN_SZBMSZ11 -1.3234050  0.0085506  -154.773  &lt; 2e-16 ***\nDESTIN_SZBMSZ12 -0.8399230  0.0085361   -98.397  &lt; 2e-16 ***\nDESTIN_SZBMSZ13  0.1366529  0.0059697    22.891  &lt; 2e-16 ***\nDESTIN_SZBMSZ14 -1.0491968  0.0083021  -126.378  &lt; 2e-16 ***\nDESTIN_SZBMSZ15 -0.6726684  0.0076276   -88.189  &lt; 2e-16 ***\nDESTIN_SZBMSZ16 -1.4011734  0.0116569  -120.201  &lt; 2e-16 ***\nDESTIN_SZBMSZ17 -1.5682752  0.0167333   -93.722  &lt; 2e-16 ***\nDESTIN_SZBPSZ01 -1.1120017  0.0063197  -175.959  &lt; 2e-16 ***\nDESTIN_SZBPSZ02 -2.0833466  0.0091139  -228.590  &lt; 2e-16 ***\nDESTIN_SZBPSZ03 -1.6937265  0.0087437  -193.709  &lt; 2e-16 ***\nDESTIN_SZBPSZ04 -0.7964999  0.0066129  -120.447  &lt; 2e-16 ***\nDESTIN_SZBPSZ05  0.2109118  0.0048815    43.206  &lt; 2e-16 ***\nDESTIN_SZBPSZ06 -1.1808365  0.0083657  -141.152  &lt; 2e-16 ***\nDESTIN_SZBPSZ07 -0.2077428  0.0084543   -24.572  &lt; 2e-16 ***\nDESTIN_SZBSSZ01  0.3164175  0.0050682    62.431  &lt; 2e-16 ***\nDESTIN_SZBSSZ02 -0.4852688  0.0057001   -85.134  &lt; 2e-16 ***\nDESTIN_SZBSSZ03  0.4130432  0.0043061    95.921  &lt; 2e-16 ***\nDESTIN_SZBTSZ01  0.6215095  0.0048914   127.061  &lt; 2e-16 ***\nDESTIN_SZBTSZ02 -0.0145076  0.0071799    -2.021 0.043324 *  \nDESTIN_SZBTSZ03  0.4919981  0.0058498    84.105  &lt; 2e-16 ***\nDESTIN_SZBTSZ04 -0.6957555  0.0114078   -60.989  &lt; 2e-16 ***\nDESTIN_SZBTSZ05  0.3329814  0.0073568    45.262  &lt; 2e-16 ***\nDESTIN_SZBTSZ06 -0.1333295  0.0073965   -18.026  &lt; 2e-16 ***\nDESTIN_SZBTSZ07 -1.4449581  0.0113186  -127.663  &lt; 2e-16 ***\nDESTIN_SZBTSZ08 -0.7079056  0.0103797   -68.201  &lt; 2e-16 ***\nDESTIN_SZCBSZ01 -5.7344725  0.3162767   -18.131  &lt; 2e-16 ***\nDESTIN_SZCCSZ01 -0.0009541  0.0083381    -0.114 0.908900    \nDESTIN_SZCHSZ01 -0.2083016  0.0099054   -21.029  &lt; 2e-16 ***\nDESTIN_SZCHSZ02  0.5369606  0.0057531    93.334  &lt; 2e-16 ***\nDESTIN_SZCHSZ03  2.5530638  0.0043945   580.971  &lt; 2e-16 ***\nDESTIN_SZCKSZ01 -0.5725975  0.0056507  -101.333  &lt; 2e-16 ***\nDESTIN_SZCKSZ02 -1.1181852  0.0063287  -176.685  &lt; 2e-16 ***\nDESTIN_SZCKSZ03  0.1156680  0.0049440    23.396  &lt; 2e-16 ***\nDESTIN_SZCKSZ04 -0.8647725  0.0071003  -121.794  &lt; 2e-16 ***\nDESTIN_SZCKSZ05 -1.1641791  0.0076248  -152.684  &lt; 2e-16 ***\nDESTIN_SZCKSZ06 -0.4397612  0.0073040   -60.208  &lt; 2e-16 ***\nDESTIN_SZCLSZ01  0.1930552  0.0053752    35.916  &lt; 2e-16 ***\nDESTIN_SZCLSZ02 -2.0436501  0.0136039  -150.225  &lt; 2e-16 ***\nDESTIN_SZCLSZ03 -0.9338571  0.0082908  -112.638  &lt; 2e-16 ***\nDESTIN_SZCLSZ04  0.0532041  0.0053276     9.987  &lt; 2e-16 ***\nDESTIN_SZCLSZ05 -1.0782781  0.0088184  -122.276  &lt; 2e-16 ***\nDESTIN_SZCLSZ06  0.4068171  0.0049068    82.910  &lt; 2e-16 ***\nDESTIN_SZCLSZ07 -0.3579507  0.0060289   -59.373  &lt; 2e-16 ***\nDESTIN_SZCLSZ08 -0.2487993  0.0066588   -37.364  &lt; 2e-16 ***\nDESTIN_SZCLSZ09  0.1611080  0.0071178    22.635  &lt; 2e-16 ***\nDESTIN_SZDTSZ02 -1.7308348  0.0349466   -49.528  &lt; 2e-16 ***\nDESTIN_SZDTSZ03 -0.5994253  0.0146230   -40.992  &lt; 2e-16 ***\nDESTIN_SZDTSZ13 -1.3685031  0.0162803   -84.059  &lt; 2e-16 ***\nDESTIN_SZGLSZ01 -0.0910001  0.0055275   -16.463  &lt; 2e-16 ***\nDESTIN_SZGLSZ02 -0.0692224  0.0052840   -13.100  &lt; 2e-16 ***\nDESTIN_SZGLSZ03  0.6493421  0.0043446   149.459  &lt; 2e-16 ***\nDESTIN_SZGLSZ04  0.9327947  0.0043674   213.583  &lt; 2e-16 ***\nDESTIN_SZGLSZ05  0.8161728  0.0043625   187.087  &lt; 2e-16 ***\nDESTIN_SZHGSZ01  0.0658625  0.0042516    15.491  &lt; 2e-16 ***\nDESTIN_SZHGSZ02 -0.8134329  0.0056721  -143.409  &lt; 2e-16 ***\nDESTIN_SZHGSZ03 -1.3546132  0.0066257  -204.448  &lt; 2e-16 ***\nDESTIN_SZHGSZ04 -0.4500588  0.0048448   -92.895  &lt; 2e-16 ***\nDESTIN_SZHGSZ05 -0.5026431  0.0050996   -98.566  &lt; 2e-16 ***\nDESTIN_SZHGSZ06 -0.8673686  0.0059530  -145.704  &lt; 2e-16 ***\nDESTIN_SZHGSZ07  0.0560490  0.0047702    11.750  &lt; 2e-16 ***\nDESTIN_SZHGSZ08 -0.0443189  0.0052599    -8.426  &lt; 2e-16 ***\nDESTIN_SZHGSZ09 -0.0126355  0.0054966    -2.299 0.021518 *  \nDESTIN_SZHGSZ10 -3.5821793  0.0263281  -136.059  &lt; 2e-16 ***\nDESTIN_SZJESZ01 -0.3704281  0.0056684   -65.350  &lt; 2e-16 ***\nDESTIN_SZJESZ02 -0.7369159  0.0058686  -125.570  &lt; 2e-16 ***\nDESTIN_SZJESZ03 -0.8985484  0.0063627  -141.222  &lt; 2e-16 ***\nDESTIN_SZJESZ04 -1.0511995  0.0073996  -142.061  &lt; 2e-16 ***\nDESTIN_SZJESZ05 -1.5324974  0.0102612  -149.349  &lt; 2e-16 ***\nDESTIN_SZJESZ06  0.3105267  0.0048241    64.370  &lt; 2e-16 ***\nDESTIN_SZJESZ07 -1.3234483  0.0085497  -154.795  &lt; 2e-16 ***\nDESTIN_SZJESZ08 -0.6559742  0.0083174   -78.867  &lt; 2e-16 ***\nDESTIN_SZJESZ09  0.2663752  0.0063370    42.035  &lt; 2e-16 ***\nDESTIN_SZJESZ10  0.8529026  0.0076067   112.126  &lt; 2e-16 ***\nDESTIN_SZJESZ11  0.5559641  0.0074629    74.497  &lt; 2e-16 ***\nDESTIN_SZJWSZ01 -0.9790971  0.0071830  -136.308  &lt; 2e-16 ***\nDESTIN_SZJWSZ02 -0.8746590  0.0060179  -145.342  &lt; 2e-16 ***\nDESTIN_SZJWSZ03  0.5689062  0.0049105   115.855  &lt; 2e-16 ***\nDESTIN_SZJWSZ04  0.4520963  0.0050302    89.876  &lt; 2e-16 ***\nDESTIN_SZJWSZ05 -1.0249671  0.0067371  -152.137  &lt; 2e-16 ***\nDESTIN_SZJWSZ06 -0.7451483  0.0062189  -119.819  &lt; 2e-16 ***\nDESTIN_SZJWSZ07 -2.8453099  0.0287335   -99.024  &lt; 2e-16 ***\nDESTIN_SZJWSZ08 -0.3372309  0.0058003   -58.141  &lt; 2e-16 ***\nDESTIN_SZJWSZ09  1.0505330  0.0045908   228.832  &lt; 2e-16 ***\nDESTIN_SZKLSZ01 -0.2334836  0.0057970   -40.277  &lt; 2e-16 ***\nDESTIN_SZKLSZ02 -0.5416148  0.0061432   -88.164  &lt; 2e-16 ***\nDESTIN_SZKLSZ03 -0.8026495  0.0068745  -116.757  &lt; 2e-16 ***\nDESTIN_SZKLSZ04 -1.2918594  0.0090197  -143.227  &lt; 2e-16 ***\nDESTIN_SZKLSZ05 -0.4069101  0.0087812   -46.339  &lt; 2e-16 ***\nDESTIN_SZKLSZ06 -2.5333101  0.0363215   -69.747  &lt; 2e-16 ***\nDESTIN_SZKLSZ07 -0.6623343  0.0070761   -93.601  &lt; 2e-16 ***\nDESTIN_SZKLSZ08 -0.1408205  0.0054965   -25.620  &lt; 2e-16 ***\nDESTIN_SZLKSZ01 -1.2639235  0.0208254   -60.691  &lt; 2e-16 ***\nDESTIN_SZMDSZ01 -1.5655800  0.0202787   -77.203  &lt; 2e-16 ***\nDESTIN_SZMDSZ02 -0.9767682  0.0114687   -85.168  &lt; 2e-16 ***\nDESTIN_SZMDSZ03 -3.3328109  0.0254294  -131.061  &lt; 2e-16 ***\nDESTIN_SZMPSZ01 -0.4552859  0.0080666   -56.441  &lt; 2e-16 ***\nDESTIN_SZMPSZ02 -0.5386560  0.0064620   -83.358  &lt; 2e-16 ***\nDESTIN_SZMPSZ03  0.4952000  0.0052295    94.694  &lt; 2e-16 ***\nDESTIN_SZMUSZ02 -1.4434175  0.0202509   -71.277  &lt; 2e-16 ***\nDESTIN_SZNTSZ01 -2.9194067  0.0449654   -64.926  &lt; 2e-16 ***\nDESTIN_SZNTSZ02 -1.3780179  0.0112867  -122.092  &lt; 2e-16 ***\nDESTIN_SZNTSZ03 -0.5044699  0.0080449   -62.707  &lt; 2e-16 ***\nDESTIN_SZNTSZ05 -2.0017134  0.0258750   -77.361  &lt; 2e-16 ***\nDESTIN_SZNTSZ06 -3.8120537  0.0434271   -87.781  &lt; 2e-16 ***\nDESTIN_SZNVSZ01 -0.1071506  0.0051026   -20.999  &lt; 2e-16 ***\nDESTIN_SZNVSZ02 -0.0274710  0.0057611    -4.768 1.86e-06 ***\nDESTIN_SZNVSZ03  0.1076352  0.0057909    18.587  &lt; 2e-16 ***\nDESTIN_SZNVSZ04 -1.2087250  0.0110438  -109.448  &lt; 2e-16 ***\nDESTIN_SZNVSZ05 -1.0058290  0.0092167  -109.131  &lt; 2e-16 ***\nDESTIN_SZPGSZ01 -1.2029931  0.0163170   -73.726  &lt; 2e-16 ***\nDESTIN_SZPGSZ02 -1.2878671  0.0074139  -173.709  &lt; 2e-16 ***\nDESTIN_SZPGSZ03 -0.1520894  0.0048629   -31.275  &lt; 2e-16 ***\nDESTIN_SZPGSZ04 -0.1985959  0.0050374   -39.424  &lt; 2e-16 ***\nDESTIN_SZPGSZ05 -1.5290983  0.0082617  -185.083  &lt; 2e-16 ***\nDESTIN_SZPLSZ01 -0.3567934  0.0074298   -48.022  &lt; 2e-16 ***\nDESTIN_SZPLSZ02 -1.7114351  0.0134462  -127.280  &lt; 2e-16 ***\nDESTIN_SZPLSZ03 -0.3241427  0.0098895   -32.776  &lt; 2e-16 ***\nDESTIN_SZPLSZ04 -1.7117196  0.0119003  -143.838  &lt; 2e-16 ***\nDESTIN_SZPLSZ05 -0.5086379  0.0120051   -42.368  &lt; 2e-16 ***\nDESTIN_SZPNSZ01  0.2026781  0.0068977    29.383  &lt; 2e-16 ***\nDESTIN_SZPNSZ02  0.8313754  0.0078544   105.848  &lt; 2e-16 ***\nDESTIN_SZPNSZ03 -0.4041254  0.0086586   -46.673  &lt; 2e-16 ***\nDESTIN_SZPNSZ04  1.5814539  0.0093641   168.885  &lt; 2e-16 ***\nDESTIN_SZPNSZ05  1.1823430  0.0129843    91.059  &lt; 2e-16 ***\nDESTIN_SZPRSZ01 -1.1057553  0.0088197  -125.374  &lt; 2e-16 ***\nDESTIN_SZPRSZ02  0.0895099  0.0056308    15.897  &lt; 2e-16 ***\nDESTIN_SZPRSZ03  0.6921925  0.0043977   157.397  &lt; 2e-16 ***\nDESTIN_SZPRSZ04 -0.2848336  0.0084725   -33.619  &lt; 2e-16 ***\nDESTIN_SZPRSZ05  0.1744480  0.0053553    32.575  &lt; 2e-16 ***\nDESTIN_SZPRSZ06  0.4279206  0.0058735    72.856  &lt; 2e-16 ***\nDESTIN_SZPRSZ07 -1.5123108  0.0124303  -121.664  &lt; 2e-16 ***\nDESTIN_SZPRSZ08 -0.5650226  0.0068530   -82.449  &lt; 2e-16 ***\nDESTIN_SZQTSZ01 -0.5952360  0.0090505   -65.769  &lt; 2e-16 ***\nDESTIN_SZQTSZ02 -0.7728170  0.0078910   -97.937  &lt; 2e-16 ***\nDESTIN_SZQTSZ03 -0.5066812  0.0073996   -68.474  &lt; 2e-16 ***\nDESTIN_SZQTSZ04 -0.6398414  0.0075411   -84.847  &lt; 2e-16 ***\nDESTIN_SZQTSZ05 -0.4354527  0.0069345   -62.795  &lt; 2e-16 ***\nDESTIN_SZQTSZ06 -0.6597391  0.0071919   -91.733  &lt; 2e-16 ***\nDESTIN_SZQTSZ07 -0.9392696  0.0112518   -83.477  &lt; 2e-16 ***\nDESTIN_SZQTSZ08  0.4617774  0.0057011    80.998  &lt; 2e-16 ***\nDESTIN_SZQTSZ09 -0.3174497  0.0065890   -48.178  &lt; 2e-16 ***\nDESTIN_SZQTSZ10  0.1993449  0.0059923    33.267  &lt; 2e-16 ***\nDESTIN_SZQTSZ11  0.2551535  0.0061885    41.230  &lt; 2e-16 ***\nDESTIN_SZQTSZ12 -0.1662603  0.0086701   -19.176  &lt; 2e-16 ***\nDESTIN_SZQTSZ13  0.5500978  0.0063091    87.192  &lt; 2e-16 ***\nDESTIN_SZQTSZ14  0.5364435  0.0070157    76.463  &lt; 2e-16 ***\nDESTIN_SZQTSZ15  1.3611043  0.0081643   166.715  &lt; 2e-16 ***\nDESTIN_SZRCSZ01 -0.1034049  0.0076769   -13.470  &lt; 2e-16 ***\nDESTIN_SZRCSZ06 -1.0633902  0.0189846   -56.013  &lt; 2e-16 ***\nDESTIN_SZRVSZ01 -1.5486221  0.0165272   -93.701  &lt; 2e-16 ***\nDESTIN_SZRVSZ02 -2.4092611  0.0326906   -73.699  &lt; 2e-16 ***\nDESTIN_SZRVSZ03 -1.5172079  0.0139258  -108.950  &lt; 2e-16 ***\nDESTIN_SZRVSZ04 -1.1663615  0.0157430   -74.088  &lt; 2e-16 ***\nDESTIN_SZRVSZ05 -2.2404292  0.0281339   -79.634  &lt; 2e-16 ***\nDESTIN_SZSBSZ01 -1.3783780  0.0096022  -143.549  &lt; 2e-16 ***\nDESTIN_SZSBSZ02 -1.4445213  0.0081630  -176.959  &lt; 2e-16 ***\nDESTIN_SZSBSZ03  0.5149906  0.0051663    99.683  &lt; 2e-16 ***\nDESTIN_SZSBSZ04  0.2389086  0.0060765    39.317  &lt; 2e-16 ***\nDESTIN_SZSBSZ05 -1.2737442  0.0082818  -153.801  &lt; 2e-16 ***\nDESTIN_SZSBSZ06 -1.8683520  0.0227277   -82.206  &lt; 2e-16 ***\nDESTIN_SZSBSZ07 -0.5993154  0.0184895   -32.414  &lt; 2e-16 ***\nDESTIN_SZSBSZ08  0.8156302  0.0059840   136.302  &lt; 2e-16 ***\nDESTIN_SZSBSZ09  0.0900611  0.0057054    15.785  &lt; 2e-16 ***\nDESTIN_SZSESZ02 -0.6397704  0.0052491  -121.882  &lt; 2e-16 ***\nDESTIN_SZSESZ03  0.1714103  0.0042357    40.468  &lt; 2e-16 ***\nDESTIN_SZSESZ04 -1.0596175  0.0059865  -177.002  &lt; 2e-16 ***\nDESTIN_SZSESZ05 -0.8071891  0.0051229  -157.566  &lt; 2e-16 ***\nDESTIN_SZSESZ06 -0.5580934  0.0066216   -84.284  &lt; 2e-16 ***\nDESTIN_SZSESZ07 -3.1448863  0.0227788  -138.062  &lt; 2e-16 ***\nDESTIN_SZSGSZ01 -0.1795225  0.0060127   -29.857  &lt; 2e-16 ***\nDESTIN_SZSGSZ02 -0.2986570  0.0053561   -55.760  &lt; 2e-16 ***\nDESTIN_SZSGSZ03 -0.4074671  0.0050609   -80.513  &lt; 2e-16 ***\nDESTIN_SZSGSZ04 -0.1505164  0.0050931   -29.553  &lt; 2e-16 ***\nDESTIN_SZSGSZ05 -1.9908372  0.0101448  -196.242  &lt; 2e-16 ***\nDESTIN_SZSGSZ06  0.6715268  0.0041161   163.148  &lt; 2e-16 ***\nDESTIN_SZSGSZ07 -0.4494757  0.0055319   -81.252  &lt; 2e-16 ***\nDESTIN_SZSISZ01 -0.5517983  0.0261860   -21.072  &lt; 2e-16 ***\nDESTIN_SZSKSZ01 -0.4749154  0.0079257   -59.921  &lt; 2e-16 ***\nDESTIN_SZSKSZ02  0.9400302  0.0057218   164.290  &lt; 2e-16 ***\nDESTIN_SZSKSZ03 -0.2800377  0.0066081   -42.378  &lt; 2e-16 ***\nDESTIN_SZSKSZ04 -1.2570212  0.0145351   -86.482  &lt; 2e-16 ***\nDESTIN_SZSKSZ05 -0.2600474  0.0112800   -23.054  &lt; 2e-16 ***\nDESTIN_SZSLSZ01 -0.7775604  0.0085818   -90.606  &lt; 2e-16 ***\nDESTIN_SZSLSZ04 -0.8586515  0.0073142  -117.396  &lt; 2e-16 ***\nDESTIN_SZSRSZ01 -1.1370887  0.0142148   -79.993  &lt; 2e-16 ***\nDESTIN_SZTHSZ01 -4.3259988  0.0368554  -117.378  &lt; 2e-16 ***\nDESTIN_SZTHSZ03 -2.6632914  0.0252720  -105.385  &lt; 2e-16 ***\nDESTIN_SZTHSZ04 -3.1000906  0.0216372  -143.276  &lt; 2e-16 ***\nDESTIN_SZTHSZ06 -2.5952642  0.0156340  -166.001  &lt; 2e-16 ***\nDESTIN_SZTMSZ01 -0.2092828  0.0059257   -35.318  &lt; 2e-16 ***\nDESTIN_SZTMSZ02  1.8238139  0.0039155   465.798  &lt; 2e-16 ***\nDESTIN_SZTMSZ03  0.8518259  0.0043636   195.210  &lt; 2e-16 ***\nDESTIN_SZTMSZ04  1.0222812  0.0043466   235.191  &lt; 2e-16 ***\nDESTIN_SZTMSZ05  0.6323777  0.0060058   105.294  &lt; 2e-16 ***\nDESTIN_SZTNSZ01 -0.3336078  0.0074388   -44.847  &lt; 2e-16 ***\nDESTIN_SZTNSZ02 -1.0820469  0.0101689  -106.408  &lt; 2e-16 ***\nDESTIN_SZTNSZ03 -1.4186505  0.0119906  -118.313  &lt; 2e-16 ***\nDESTIN_SZTNSZ04 -0.3058199  0.0074743   -40.916  &lt; 2e-16 ***\nDESTIN_SZTPSZ01 -0.4872299  0.0061571   -79.133  &lt; 2e-16 ***\nDESTIN_SZTPSZ02  0.7158441  0.0041312   173.278  &lt; 2e-16 ***\nDESTIN_SZTPSZ03 -0.4314229  0.0059917   -72.004  &lt; 2e-16 ***\nDESTIN_SZTPSZ04 -1.5898245  0.0076083  -208.959  &lt; 2e-16 ***\nDESTIN_SZTPSZ05 -1.0445550  0.0062363  -167.497  &lt; 2e-16 ***\nDESTIN_SZTPSZ06 -0.4319582  0.0070100   -61.621  &lt; 2e-16 ***\nDESTIN_SZTPSZ07 -2.1602303  0.0120352  -179.493  &lt; 2e-16 ***\nDESTIN_SZTPSZ08 -1.1920493  0.0093083  -128.063  &lt; 2e-16 ***\nDESTIN_SZTPSZ09 -0.2022481  0.0071137   -28.431  &lt; 2e-16 ***\nDESTIN_SZTPSZ10 -1.2464793  0.0090124  -138.308  &lt; 2e-16 ***\nDESTIN_SZTPSZ11 -0.0808445  0.0056019   -14.432  &lt; 2e-16 ***\nDESTIN_SZTPSZ12 -0.6784376  0.0066340  -102.267  &lt; 2e-16 ***\nDESTIN_SZTSSZ01 -1.5845062  0.0222086   -71.346  &lt; 2e-16 ***\nDESTIN_SZTSSZ02 -0.1886010  0.0146338   -12.888  &lt; 2e-16 ***\nDESTIN_SZTSSZ03  0.6525526  0.0092450    70.585  &lt; 2e-16 ***\nDESTIN_SZTSSZ04  0.5285464  0.0100182    52.759  &lt; 2e-16 ***\nDESTIN_SZTSSZ05  1.4670106  0.0104357   140.577  &lt; 2e-16 ***\nDESTIN_SZTSSZ06  2.5043588  0.0167444   149.564  &lt; 2e-16 ***\nDESTIN_SZWCSZ01  1.9787931  0.0054306   364.375  &lt; 2e-16 ***\nDESTIN_SZWCSZ02 -2.2593108  0.0127916  -176.624  &lt; 2e-16 ***\nDESTIN_SZWCSZ03 -3.1897655  0.0326927   -97.568  &lt; 2e-16 ***\nDESTIN_SZWDSZ01  1.0476108  0.0044629   234.738  &lt; 2e-16 ***\nDESTIN_SZWDSZ02 -1.3176990  0.0065894  -199.973  &lt; 2e-16 ***\nDESTIN_SZWDSZ03  0.3432057  0.0052496    65.377  &lt; 2e-16 ***\nDESTIN_SZWDSZ04 -0.7895927  0.0073392  -107.586  &lt; 2e-16 ***\nDESTIN_SZWDSZ05 -0.8751665  0.0072946  -119.975  &lt; 2e-16 ***\nDESTIN_SZWDSZ06 -0.2106221  0.0053027   -39.720  &lt; 2e-16 ***\nDESTIN_SZWDSZ07 -1.6050834  0.0071754  -223.692  &lt; 2e-16 ***\nDESTIN_SZWDSZ08 -0.5124717  0.0069223   -74.032  &lt; 2e-16 ***\nDESTIN_SZWDSZ09  0.3813542  0.0054697    69.721  &lt; 2e-16 ***\nDESTIN_SZYSSZ01  0.0853753  0.0046572    18.332  &lt; 2e-16 ***\nDESTIN_SZYSSZ02 -0.3227172  0.0057351   -56.271  &lt; 2e-16 ***\nDESTIN_SZYSSZ03 -0.4151283  0.0066299   -62.615  &lt; 2e-16 ***\nDESTIN_SZYSSZ04 -0.4637327  0.0058206   -79.671  &lt; 2e-16 ***\nDESTIN_SZYSSZ05 -1.5888242  0.0111001  -143.136  &lt; 2e-16 ***\nDESTIN_SZYSSZ06 -1.4606209  0.0107759  -135.545  &lt; 2e-16 ***\nDESTIN_SZYSSZ07 -0.7839065  0.0144357   -54.304  &lt; 2e-16 ***\nDESTIN_SZYSSZ08  0.6265412  0.0045504   137.691  &lt; 2e-16 ***\nDESTIN_SZYSSZ09  0.1520067  0.0048092    31.607  &lt; 2e-16 ***\nlog(DIST)       -1.8468315  0.0004608 -4008.033  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 47094011  on 14470  degrees of freedom\nResidual deviance: 10420261  on 13912  degrees of freedom\nAIC: 10510518\n\nNumber of Fisher Scoring iterations: 7"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/data/geospatial/Business.html",
    "href": "Takehome_Ex/Takehome_Ex02/data/geospatial/Business.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/data/geospatial/Retails.html",
    "href": "Takehome_Ex/Takehome_Ex02/data/geospatial/Retails.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/data/geospatial/Liesure&Recreation.html",
    "href": "Takehome_Ex/Takehome_Ex02/data/geospatial/Liesure&Recreation.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/data/geospatial/F&B.html",
    "href": "Takehome_Ex/Takehome_Ex02/data/geospatial/F&B.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/data/geospatial/entertn.html",
    "href": "Takehome_Ex/Takehome_Ex02/data/geospatial/entertn.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/data/geospatial/FinServ.html",
    "href": "Takehome_Ex/Takehome_Ex02/data/geospatial/FinServ.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  },
  {
    "objectID": "Takehome_Ex/Takehome_Ex02/data/geospatial/Leisure&Recreation.html",
    "href": "Takehome_Ex/Takehome_Ex02/data/geospatial/Leisure&Recreation.html",
    "title": "ISSS624 Geospatial Analytics",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n       GEOGCRS[“WGS 84”,ENSEMBLE[“World Geodetic System 1984 ensemble”,MEMBER[“World Geodetic System 1984 (Transit)”],MEMBER[“World Geodetic System 1984 (G730)”],MEMBER[“World Geodetic System 1984 (G873)”],MEMBER[“World Geodetic System 1984 (G1150)”],MEMBER[“World Geodetic System 1984 (G1674)”],MEMBER[“World Geodetic System 1984 (G1762)”],MEMBER[“World Geodetic System 1984 (G2139)”],ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]],ENSEMBLEACCURACY[2.0]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433]],USAGE[SCOPE[“Horizontal component of 3D system.”],AREA[“World.”],BBOX[-90,-180,90,180]],ID[“EPSG”,4326]] +proj=longlat +datum=WGS84 +no_defs 3452 4326 EPSG:4326 WGS 84 longlat EPSG:7030 true"
  }
]